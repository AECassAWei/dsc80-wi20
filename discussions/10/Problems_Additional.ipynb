{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Review Problems\n",
    "\n",
    "### Not to be turned in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import disc10 as disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import sklearn\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping and Joining\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "*The datasets were taken from https://github.com/fivethirtyeight/data/tree/master/college-majors and altered for the purposes of this problem*\n",
    "<br><br>\n",
    "In this problem, there are two csv files:\n",
    "- `majors-data.csv` contains employment and salary data for college majors post-grad.\n",
    "- `majors-list.csv` contains major name and major category data.\n",
    "\n",
    "\n",
    "You want to explore which major categories are the ***best***.\n",
    "<br><br>\n",
    "First, create a function `merge_majors` that merges the two csv files together. Assume that parameter `df1` will be the `majors-list.csv` file and that parameter `df2` will be the `majors-data.csv` file.\n",
    "<br>\n",
    "\n",
    "*Note: in the resultant dataframe, keep the column `Major_code` but remove the column `FOD1P`. It should look something like this:*\n",
    "<img src=\"data/merged.png\"> \n",
    "<br>\n",
    "Then, create a function `best_majors` that takes in the merged dataframe and returns a list with the following values in the order given below:\n",
    "- `Major_Category` with the highest average employment **rate** (not number employed).\n",
    "- `Major_Category` with the highest median median salary.\n",
    "- `Major_Category` with the highest minimum P75th salary.\n",
    "- `Major_Category` with highest number of people employed year round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data/majors-list.csv')\n",
    "df2 = pd.read_csv('data/majors-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_majors(df1, df2):\n",
    "    '''\n",
    "    Merge the two input dataframes on major code number\n",
    "    >>> df1 = pd.read_csv('data/majors-list.csv')\n",
    "    >>> df2 = pd.read_csv('data/majors-data.csv')\n",
    "    >>> merged = merge_majors(df1, df2)\n",
    "    >>> len(merged) == len(df1)\n",
    "    True\n",
    "    >>> len(merged.columns)\n",
    "    10\n",
    "    >>> 'FOD1P' in merged.columns\n",
    "    False\n",
    "    '''\n",
    "    return pd.merge(df1, df2, left_on='FOD1P', right_on='Major_code', how='left').drop('FOD1P', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Major</th>\n",
       "      <th>Major_Category</th>\n",
       "      <th>Major_code</th>\n",
       "      <th>Total</th>\n",
       "      <th>Employed</th>\n",
       "      <th>Employed_full_time_year_round</th>\n",
       "      <th>Unemployed</th>\n",
       "      <th>Median</th>\n",
       "      <th>P25th</th>\n",
       "      <th>P75th</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GENERAL AGRICULTURE</td>\n",
       "      <td>Agriculture &amp; Natural Resources</td>\n",
       "      <td>1100</td>\n",
       "      <td>128148</td>\n",
       "      <td>90245</td>\n",
       "      <td>74078</td>\n",
       "      <td>2423</td>\n",
       "      <td>50000</td>\n",
       "      <td>34000</td>\n",
       "      <td>80000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGRICULTURE PRODUCTION AND MANAGEMENT</td>\n",
       "      <td>Agriculture &amp; Natural Resources</td>\n",
       "      <td>1101</td>\n",
       "      <td>95326</td>\n",
       "      <td>76865</td>\n",
       "      <td>64240</td>\n",
       "      <td>2266</td>\n",
       "      <td>54000</td>\n",
       "      <td>36000</td>\n",
       "      <td>80000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGRICULTURAL ECONOMICS</td>\n",
       "      <td>Agriculture &amp; Natural Resources</td>\n",
       "      <td>1102</td>\n",
       "      <td>33955</td>\n",
       "      <td>26321</td>\n",
       "      <td>22810</td>\n",
       "      <td>821</td>\n",
       "      <td>63000</td>\n",
       "      <td>40000</td>\n",
       "      <td>98000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANIMAL SCIENCES</td>\n",
       "      <td>Agriculture &amp; Natural Resources</td>\n",
       "      <td>1103</td>\n",
       "      <td>103549</td>\n",
       "      <td>81177</td>\n",
       "      <td>64937</td>\n",
       "      <td>3619</td>\n",
       "      <td>46000</td>\n",
       "      <td>30000</td>\n",
       "      <td>72000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FOOD SCIENCE</td>\n",
       "      <td>Agriculture &amp; Natural Resources</td>\n",
       "      <td>1104</td>\n",
       "      <td>24280</td>\n",
       "      <td>17281</td>\n",
       "      <td>12722</td>\n",
       "      <td>894</td>\n",
       "      <td>62000</td>\n",
       "      <td>38500</td>\n",
       "      <td>90000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Major                   Major_Category  \\\n",
       "0                    GENERAL AGRICULTURE  Agriculture & Natural Resources   \n",
       "1  AGRICULTURE PRODUCTION AND MANAGEMENT  Agriculture & Natural Resources   \n",
       "2                 AGRICULTURAL ECONOMICS  Agriculture & Natural Resources   \n",
       "3                        ANIMAL SCIENCES  Agriculture & Natural Resources   \n",
       "4                           FOOD SCIENCE  Agriculture & Natural Resources   \n",
       "\n",
       "   Major_code   Total  Employed  Employed_full_time_year_round  Unemployed  \\\n",
       "0        1100  128148     90245                          74078        2423   \n",
       "1        1101   95326     76865                          64240        2266   \n",
       "2        1102   33955     26321                          22810         821   \n",
       "3        1103  103549     81177                          64937        3619   \n",
       "4        1104   24280     17281                          12722         894   \n",
       "\n",
       "   Median  P25th    P75th  \n",
       "0   50000  34000  80000.0  \n",
       "1   54000  36000  80000.0  \n",
       "2   63000  40000  98000.0  \n",
       "3   46000  30000  72000.0  \n",
       "4   62000  38500  90000.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = merge_majors(df1, df2)\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_majors(df):\n",
    "    '''\n",
    "    Return a list of \"best\" majors\n",
    "    >>> df1 = pd.read_csv('data/majors-list.csv')\n",
    "    >>> df2 = pd.read_csv('data/majors-data.csv')\n",
    "    >>> merged = merge_majors(df1, df2)\n",
    "    >>> best = best_majors(merged)\n",
    "    >>> len(best)\n",
    "    4\n",
    "    >>> all(pd.Series(best).isin(merged.Major_Category.unique()))\n",
    "    True\n",
    "    '''\n",
    "    \n",
    "    cop = df.copy()\n",
    "    cop['Employment_rate'] = cop['Employed'] / cop['Total']\n",
    "    best_emply_rate = cop.groupby('Major_Category')['Employment_rate'].mean().idxmax()\n",
    "    best_median_sal = cop.groupby('Major_Category')['Median'].median().idxmax()\n",
    "    best_min_p75 = cop.groupby('Major_Category')['P75th'].min().idxmax()\n",
    "    year_round = cop.groupby('Major_Category')['Employed_full_time_year_round'].sum().idxmax()\n",
    "    \n",
    "    return [best_emply_rate, best_median_sal, best_min_p75, year_round]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Computers & Mathematics', 'Engineering', 'Engineering', 'Business']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_majors(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing and Permutation Testing\n",
    "\n",
    "* Given a *single* observed sample, make an assumption of how it came to be:\n",
    "    - This assumption is the *null hypothesis*.\n",
    "    - Generate data under this assumption (*probability model*).\n",
    "* Simulate data under the null hypothesis (*the null distribution*).\n",
    "* Ask \"is it likely the given observation arose from this assumption?\"\n",
    "\n",
    "\n",
    "* **Null Hypothesis**\n",
    "   - Nothing special happened / anything special you observed is purely by chance.\n",
    "   - A hypothesis associated with a contradiction to a theory one would like to prove (e.g. two means are the same).\n",
    "   \n",
    "* **Alternative Hypothesis**\n",
    "   - Something special happened.\n",
    "   - A hypothesis associated with a theory one would like to prove (e.g. two means are **NOT** the same).\n",
    "   \n",
    "### Finding the Appropriate Statistics\n",
    "\n",
    "* Statistics are used as a tool to compare two datasets. \n",
    "* Common statistics to use: TVD, ks-statistics, difference of means.\n",
    "\n",
    "**P-Value**\n",
    "* The probability that, if the null hypothesis is true, we observe a test statistic that is at least as extreme as what we've observed.\n",
    "* Must be decided before conducting a test.\n",
    "    * e.g. Calculating the proportion of simulated TVD that is greater than or equal to the observed TVD.\n",
    "\n",
    "**Permutaion Test**\n",
    "* Shuffle the group labels a number of times.\n",
    "    * **Null Hypothesis**: two distributions are the same.\n",
    "    * **Alternative Hypothesis**: two distributions are **NOT** the same.\n",
    "* Intuition: if two distributions are exactly the same, the pairing of label and data are completely random! One can therefore pair the labels and data randomly to mimic the situation under the null hypothesis. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "The dataset is taken from https://archive.ics.uci.edu/ml/datasets/adult and has been preprocessed for the purpose of this problem.\n",
    "\n",
    "In the dataset, for each bank customer with `ID`, there are the average payment amount `AVG_PAY_AMT` for the last six months, and `DEFAULT` which indicates whether the customer defaults the next payment or not (1: Yes, 0: No).\n",
    "\n",
    "You want to explore whether a customer's average payment is related to his/her defaulting the next payment.\n",
    "\n",
    "To do this, use a *permutation test* to assess whether customers who default the next payment have higher average payment in the last six months. \n",
    "\n",
    "First, create a function `null_and_statistic` that return a hard-coded list of answers to two questions below:\n",
    "\n",
    "* Which of the following is **NOT** a valid null hypothesis?\n",
    "    1. A customer's average payment is independent of his/her defaulting the next payment.\n",
    "    2. Average payment for a customer who defaults is the same as that for a customer who does not default.\n",
    "    3. A customer who defaults tend to have less average payment amount.\n",
    "* Which of the following is a better test statistic to use in this scenario?\n",
    "    1. Difference of Means\n",
    "    2. TVD\n",
    "\n",
    "Second, create a function `simulate_null` that takes in a dataframe like default, and returns one instance of the test-statistic that you chose above under the null hypothesis. \n",
    "\n",
    "Lastly, create a function `pval_default` that takes in a dataframe like default, and calculates the p-value for the permutation test using **1000** trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_and_statistic():\n",
    "    \"\"\"\n",
    "    answers to the two multiple-choice\n",
    "    questions listed above.\n",
    "\n",
    "    :Example:\n",
    "    >>> out = null_and_statistic()\n",
    "    >>> isinstance(out, list)\n",
    "    True\n",
    "    >>> out[0] in [1,2,3]\n",
    "    True\n",
    "    >>> out[1] in [1,2]\n",
    "    True\n",
    "    \"\"\"\n",
    "    \n",
    "    return [3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_null(data):\n",
    "    \"\"\"\n",
    "    simulate_null takes in a dataframe like default, \n",
    "    and returns one instance of the test-statistic \n",
    "    (difference of means) under the null hypothesis.\n",
    "\n",
    "    :Example:\n",
    "    >>> default_fp = os.path.join('data', 'default.csv')\n",
    "    >>> default = pd.read_csv(default_fp)\n",
    "    >>> out = simulate_null(default)\n",
    "    >>> isinstance(out, float)\n",
    "    True\n",
    "    >>> 0 <= out <= 1.0\n",
    "    True\n",
    "    \"\"\"\n",
    "    # shuffle the weights\n",
    "    shuffled_DEFAULT = (\n",
    "        data['DEFAULT']\n",
    "        .sample(replace=False, frac=1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # put them in a table\n",
    "    shuffled = (\n",
    "        data\n",
    "        .assign(**{'Shuffled DEFAULT': shuffled_DEFAULT})\n",
    "    )\n",
    "    \n",
    "    # compute the group differences (test statistic!)\n",
    "    group_means = (\n",
    "        shuffled\n",
    "        .groupby('Shuffled DEFAULT')\n",
    "        .mean()\n",
    "        .loc[:, 'AVG_PAY_AMT']\n",
    "    )\n",
    "    difference = group_means.diff().iloc[-1]\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pval_default(data):\n",
    "    \"\"\"\n",
    "    pval_default takes in a dataframe like default, \n",
    "    and calculates the p-value for the permutation \n",
    "    test using 1000 trials.\n",
    "    \n",
    "    :Example:\n",
    "    >>> default_fp = os.path.join('data', 'default.csv')\n",
    "    >>> default = pd.read_csv(default_fp)\n",
    "    >>> out = pval_default(default)\n",
    "    >>> isinstance(pval, float)\n",
    "    True\n",
    "    >>> 0 <= pval <= 0.1\n",
    "    True\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for _ in range(1000):\n",
    "        results.append(simulate_null(data))\n",
    "        \n",
    "    obs = (\n",
    "        data\n",
    "        .groupby('DEFAULT')['AVG_PAY_AMT']\n",
    "        .mean()\n",
    "        .diff()\n",
    "        .iloc[-1]\n",
    "    )\n",
    "    pval = (pd.Series(results) < obs).mean()\n",
    "    \n",
    "    return pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missingness Types\n",
    "\n",
    "**Useful review materials from class to check out:**\n",
    "- [Lecture 8: Missingness](https://github.com/ucsd-ets/dsc80-sp19/blob/master/lectures/08/Lecture%2008%20Missingness.ipynb)\n",
    "- [Lecture 9: Data Imputation](https://github.com/ucsd-ets/dsc80-sp19/blob/master/lectures/09/Lecture%2009%20Data%20Imputation.ipynb)\n",
    "- [Discussion 5](https://github.com/ucsd-ets/dsc80-sp19/blob/master/discussions/05/Discussion%2005.ipynb)\n",
    "- [Lab 4: Questions 7 and 8](https://github.com/ucsd-ets/dsc80-sp19/blob/master/labs/lab04/lab04.ipynb)\n",
    "- [Lab 5: Questions 1, 2, 3, 4](https://github.com/ucsd-ets/dsc80-sp19/blob/master/labs/lab05/lab05.ipynb)\n",
    "\n",
    "##### Missing by Design (MD)\n",
    "- The missing field is deliberately missing. The missing field is deliberately not collected or set to null (hence, \"missing by design\")\n",
    "- The missingness can be exactly predicted when a column will be null, with only knowledge of the other columns using a function of the rows of the dataset\n",
    "- **Example:** The executive board of a student organization is trying to choose 5 new board members for next year from a selection pool of 50. The selection process includes three rounds. After each round, each participant is ranked on a scale of 0 to 100 and only the top 50% of participants in each round are chosen to go to the next round. *In this scenario, only participants who scored highly in each round will have scores for the next round. Therefore, scores for rounds 2 and 3 are missing by design (MD) because only high scoring participants from the previous round will have these scores.*\n",
    "\n",
    "##### Missing Completely At Random (MCAR)\n",
    "- The missingness of missing value isn't related to the actual, unreported value itself, nor the values in any other fields. The missingness is not systematic.\n",
    "- The missingness is unconditionally uniform across rows. MCAR doesn't bias the observed data.\n",
    "- There is no relationship between the missing data and the any of the other data, observed or missing.\n",
    "- **Example:** A student organization is taking headshots for all of their members (the order of the students is completely random). About 2 hours into the photo shoot, the camera battery dies and some of the students are left without headshots. *In this scenario, the missing headshots (data) has nothing to do with the students or anything they did (it was completely random), it was just bad luck. Therefore, the missingness of the headshots is missing completely at random (MCAR) because there is no correlation between the missing headshots and any of the other variables regarding the organization members.*\n",
    "\n",
    "##### Missing At Random (MAR)\n",
    "- The missingness of the missing value has nothing to do with the value itself, but may be related to another field.\n",
    "- The missingness is uniform across rows, perhaps conditional on another column. MAR biases the observed data, but is fixable.\n",
    "- There is a systematic relationship between the missing values and the observed data (but not the missing values themselves).\n",
    "- Difference between MD and MAR: If you can *exactly/always* determine missingness on other columns, the missingness is MD. If there is just some sort of systematic relationship between the missing columns/values and other columns/values that may help us predict missingness, the missingness is MAR.\n",
    "- **Example:** A department (let's say it's the DSC department) has a dataframe of all the students in the DSC major, their grades in all of the DSC classes, and their grade level. Some of the students have NaN values for certain DSC courses and the department notices that there's a lot of missingness for first year students. *In this scenario, the missing grades (data) is related to the students' grade level because most first year students haven't had the opportunity to take certain courses such as DSC80 or DSC170 yet because they haven't finished their prerequsite classes. Therefore, the missingness of the grades is missing at random (MAR) because there is a relation between the missing grades and the grade levels of the students (observed data).*\n",
    "\n",
    "##### Not Missing At Random (NMAR)\n",
    "- The missingness of the missing value is related to the actual, unreported value.\n",
    "- NMAR biases the observed data in unobservable ways.\n",
    "- There is relationship between the propensity of a value to be missing and its value.\n",
    "- **Example:** A company is hiring students for an internship. On the online application, the GPA field is optional (while fields such as Name, Email, etc. are not). The company notices that some of the applications don't have a submitted GPA. *In this scenario, students with lower GPAs are less likely to self-report their GPA. Therefore, the missingness of the GPAs is not missing at random (NMAR) because there is a relation between the missing GPAs and the actual values of the missing GPAs.*\n",
    "\n",
    "\n",
    "## Imputation\n",
    "#####  Listwise deletion\n",
    "- Procedure: .dropna()\n",
    "- If MCAR, doesn't change statistics of the data\n",
    "- If MCAR and small, may have high variance\n",
    "\n",
    "##### Imputation with a single value: mean, median, mode  \n",
    "- Procedure: .fillna(dataframe[col].mean()) (or other statistic)\n",
    "- If MCAR, gives unbiased estimate of mean; variance is too low.\n",
    "- Analogue for categorical data: imputation with the mode.\n",
    "\n",
    "##### Imputation with a single value using a model: regression, kNN  \n",
    "- Procedure: for a column c1, conditional on a second column c2:  \n",
    "    `means = dataframe.groupby('c2').mean().to_dict()`  \n",
    "    `imputed = dataframe['c1'].apply(lambda x: means[x] if pd.isnull(x) else x)`\n",
    "- If MAR, gives unbiased estimate of mean; variance is too low.\n",
    "- Increases correlations between the columns.\n",
    "- If dependent on more than one column: use linear regression to predict missing value.\n",
    "\n",
    "##### Probabilistic imputation by drawing from a distribution\n",
    "- Procedure: draw from empirical distribution of observed data to fill missing values.\n",
    "- If MCAR, gives unbiased estimate of mean and variance.\n",
    "- Extending to MAR case: draw from conditional empirical distributions\n",
    "    - If conditional on a single categorical column c2:\n",
    "    - Apply MCAR procedure to the groups of `dataframe.groupby(c2)`\n",
    "  \n",
    "##### Multiple Imputation \n",
    "- Procedure: Apply probabilistic imputation multiple times, resulting in $N$ imputed datasets.\n",
    "- Do analyses separately on the $N$ imputed datasets (e.g. compute correlation coefficient).\n",
    "- Plot the distribution of the results of these analyses!\n",
    "- If a column is missing conditional on multiple columns, your \"multiple imputations\" should include probabilistic imputations for each!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missingness Identification\n",
    "\n",
    "**Question 3** \n",
    "\n",
    "In each of the following scenarios, choose the *best* answer. Return your answers in a function `identifications`.\n",
    "1. Professors are expected to turn in their final grades on June 17th so that the university can release final grades the next day. However, professors do have the option to turn in grades late. On the day grades are released, some of the grades are missing. Consider the data to have two pieces of information: the course and the professor.\n",
    "    - Is the missingness of the grades `MD, MCAR, MAR, or NMAR`?\n",
    "\n",
    "2. At the end of a new student orientation, as students are being picked up, the orientation leaders ask new students to fill out an optional survey with their name, intended major, and favorite color. When reviewing the dataframe of surveys, the OLs notice that some of the new students did not fill out the survey. \n",
    "    - Is the missingness of the surveys `MD, MCAR, MAR, or NMAR`?\n",
    "    \n",
    "3. When collecting data on new employees on an online form, a company has an optional question where an employee can submit their preferred name if they have one (alongside their legal name). When reviewing this information, the company notices that the preferred name column has many empty values.\n",
    "    - Is the missingness of preferred names `MD, MCAR, MAR, or NMAR`?\n",
    "\n",
    "4. A translating company creates a language translator to change words from one language to another. Their software compiles the translated text into a dataframe, with one row for each translated word. If there is no direct conversion of the word from language 1 to language 2 the software continues onto the next word. \n",
    "    - Is the missingness of words `MD, MCAR, MAR, or NMAR`?\n",
    "    \n",
    "5. The DSC department has compiled a dataframe of DSC professors, their hire date, the classes they've taught, and their average recommendation rate and releases it to the students. Some of the students notice that certain professors don't have an average recommendation rate.\n",
    "    - Is the missingness of the professors' average recommendation rates `MD, MCAR, MAR, or NMAR`?\n",
    "    \n",
    "**Be sure to address why you picked the answer you did!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifications():\n",
    "    \"\"\"\n",
    "    Multiple choice response for question X\n",
    "    >>> out = identifications()\n",
    "    >>> ans = ['MD', 'MCAR', 'MAR', 'NMAR']\n",
    "    >>> len(out) == 5\n",
    "    True\n",
    "    >>> set(out) <= set(ans)\n",
    "    True\n",
    "    \"\"\"\n",
    "    #MAR - based on professor\n",
    "    #MCAR - some students left already, not related to their name/major/color\n",
    "    #MAR - People with more complicated names more likely to have a nickname; MD is reasonable as well\n",
    "    #NMAR - if there is no direct translation, the software skips the word\n",
    "    #MAR - professors that were just hired/haven't taught won't have an avg rec rate\n",
    "    \n",
    "    return ['MAR', 'MCAR', 'MD', 'NMAR', 'MAR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation Questions\n",
    "To explore imputation, we will be using the `cars` dataframe to practice different imputation techniques. The `cars` dataframe has three columns: `car_make`, `car_color`, and `car_year`.\n",
    "\n",
    "First, we'll explore two different methods of conditional imputation: conditional median imputation and probabilistic imputation by drawing from a distribution.\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "Create a function `impute_years` that takes in a DataFrame like `cars` and imputes the missing values of `car_year` with the (single) median value of `car_year` conditional on `car_make` of the missing row. Set the dtype of `car_year` to *int* when you're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'cars.csv')\n",
    "cars = pd.read_csv(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_years(cars):\n",
    "    \"\"\"\n",
    "    impute_years takes in a DataFrame of car data\n",
    "    with missing values and imputes them using the scheme in\n",
    "    the question.\n",
    "    :Example:\n",
    "    >>> fp = os.path.join('data', 'cars.csv')\n",
    "    >>> df = pd.read_csv(fp)\n",
    "    >>> out = impute_years(df)\n",
    "    >>> out['car_year'].dtype == int\n",
    "    True\n",
    "    >>> out['car_year'].min() == df['car_year'].min()\n",
    "    True\n",
    "    \"\"\"\n",
    "    medians = cars.groupby('car_make')['car_year'].median()\n",
    "    medians = medians.round()\n",
    "    medians = medians.fillna(medians.median()) #In this dataset, the only McLaren car has no year\n",
    "\n",
    "    def impute(row):\n",
    "        if pd.isnull(row['car_year']):\n",
    "            row['car_year'] = medians[row['car_make']]\n",
    "        return row\n",
    "    \n",
    "    return cars.apply(impute, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Create a function `impute_colors` that takes in a DataFrame like `cars` and probabilistically imputes the missing values of `car_color` conditional on `car_make` using the distributions of `car_color`. To do this, you will need to sample from `car_color` distributions based on the `car_make` of the row. \n",
    "\n",
    "*Note*: This method of imputation should not radically change the distribution of `car_color` conditional on `car_make`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_colors(cars):\n",
    "    \"\"\"\n",
    "    impute_colors takes in a DataFrame of car data\n",
    "    with missing values and imputes them using the scheme in\n",
    "    the question.\n",
    "    :Example:\n",
    "    >>> fp = os.path.join('data', 'cars.csv')\n",
    "    >>> df = pd.read_csv(fp)\n",
    "    >>> out = impute_colors(df)\n",
    "    >>> out.loc[out['car_make'] == 'Toyota'].nunique() == 19\n",
    "    True\n",
    "    >>> 'Crimson' in out.loc[out['car_make'] == 'Austin']['car_color'].unique()\n",
    "    False\n",
    "    \"\"\"\n",
    "    no_color = cars.loc[cars['car_color'].isnull()]\n",
    "    color_dists = cars.groupby('car_make')['car_color'].value_counts()\n",
    "\n",
    "    def color_impute(row):\n",
    "        if pd.isnull(row['car_color']):\n",
    "            row['car_color'] = color_dists[row['car_make']]\\\n",
    "                .sample(weights = color_dists[row['car_make']].values).index[0]\n",
    "        return row\n",
    "\n",
    "    imputed = no_color.apply(color_impute, axis = 1)\n",
    "    cars[cars['car_color'].isnull()] = imputed\n",
    "    \n",
    "    return cars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection (HTTP, Networking/Services, APIs)\n",
    "\n",
    "### HyperText Transfer Protocol (HTTP)\n",
    "\n",
    "- `GET`\n",
    "  - Used to request data from a specified resource.\n",
    "  - One of the most common HTTP methods.\n",
    "- `POST`\n",
    "  - Used to send data to a server to create/update a resource.\n",
    "\n",
    "### Status Code\n",
    "  - **200** (OK): The request has succeeded, the information returned with the response is dependent on the method used in the request.\n",
    "  - **400** (Bad Request): The request could not be understood by the server due to malformed syntax. The client SHOULD NOT repeat the request without modifications.\n",
    "  - **404** (Not Found): The server has not found anything matching the Request-URL. No indication is given of whether the condition is temporary or permanent. \n",
    "      - `requests.get(\"https://httpstat.us/404\").status_code`\n",
    "\n",
    "### APIs\n",
    "  - Allows for authentication (and access to sensitive data).\n",
    "  - Usually has more reliable data that is easier to parse.\n",
    "  - Allows hosts to monitor usage and protect their website\n",
    "  - Why use APIs?\n",
    "      - Data is changing quickly.\n",
    "          - E.g: stock price data: don't want to scrape a page every few minutes.\n",
    "      - Want a small piece of a much larger set of data.\n",
    "          - E.g: just pull your own comments on Reddit? (all is too much)\n",
    "          - E.g: want your Google GPS history? (private)\n",
    "      - Usability and stability, not changing HTML requiring translation.\n",
    "          - Websites change all the time.\n",
    "\n",
    "### `robots.txt`\n",
    "  - Many sites have a published policy allowing or disallowing automatic access to their site.  \n",
    "  - This policy is in a text file `robots.txt`: learn more about it [here](https://moz.com/learn/seo/robotstxt).\n",
    "  - Remember the best practices above - just because you aren't prohibited by the robots policy doesn't mean you can scrape the site!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML\n",
    "\n",
    "### The Anatomy of HTML\n",
    "\n",
    "* **HTML Document**: the totality of markup that makes up a web-page.\n",
    "* **Document Object Model**: the internal representation of a HTML document as a *tree* structure.\n",
    "* **HTML Element**: an object in the DOM, such as a paragraph, header, title.\n",
    "* **HTML Tags**: markers that denote the *start* and *end* of an element (e.g. `<p>` and `</p>`).\n",
    "\n",
    "\n",
    "### Useful Elements/Tags:\n",
    "\n",
    "|Structure Elements|Description|\n",
    "|---|---|\n",
    "|`<html>`|the document|\n",
    "|`<head>`|the header|\n",
    "|`<body>`|the body|\n",
    "|`<div>` |a logical division of the document|\n",
    "|`<span>`|an *in-line* logical division|\n",
    "\n",
    "|Head/Body Elements|Description|\n",
    "|---|---|\n",
    "|`<p>`|the paragraph|\n",
    "|`<h1>, <h2>, ...`|header(s)|\n",
    "|`<img>`|images|\n",
    "|`<a>`| anchor (hyper-link)|\n",
    "|[MANY MORE](https://en.wikipedia.org/wiki/HTML_element)||\n",
    "\n",
    "\n",
    "### Example: Images and Hyperlinks\n",
    "\n",
    "* Image:\n",
    "```\n",
    "<img src=\"HumDum.png\" alt=\"Humbpty Dumpty\">\n",
    "```\n",
    "\n",
    "* Hyperlink: \n",
    "\n",
    "```\n",
    "<a href=\"https://ucsd.edu/\">Visit our page!</a>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have an HTML parsing problem for you, so let's walk through an example of how to grab a list of artist names from the web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# request data using the GET method\n",
    "page = requests.get('https://web.archive.org/web/20121007172955/https://www.nga.gov/collection/anZ1.htm')\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using html.parser, parse through the text on the web page\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab text from the 'BodyText' class\n",
    "artist_name_list = soup.find(class_='BodyText')\n",
    "artist_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all artist names are hyper-linked, so let's grab them\n",
    "artist_name_list_items = artist_name_list.find_all('a')\n",
    "\n",
    "for artist_name in artist_name_list_items:\n",
    "    print(artist_name.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above result doesn't look very nice, so let's grab the contents instead to find the names\n",
    "for artist_name in artist_name_list_items:\n",
    "    names = artist_name.contents[0]\n",
    "    print(names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions, Statistics on Text, and Text Features (NLP)\n",
    "\n",
    "Some links you might find useful:\n",
    "\n",
    "1. Read [Lecture 12 slides](https://github.com/ucsd-ets/dsc80-sp19/blob/master/lectures/12/Lecture%2012%20Text%20Data.ipynb) for general explanations and examples for regular expression we commonly use in this class. \n",
    "2. Utilize `cmd`+`F`/`ctrl`+`F` and [the official documentation of Regular expression operations](https://docs.python.org/3/library/re.html) for a quick lookup if you are unsure about your regular expression\n",
    "3. Try it out at https://regex101.com/ (Note, you should try to come up with the complete regular expression by hand first as regex101 won't be available during the Final.)\n",
    "\n",
    "\n",
    "In both of the questions below, refer to the starter code and doctests for a more detailed specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 7** \n",
    "\n",
    "Write a line of regular expression that checks whether you (a general user) are allowed to scrape the entire website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(robots):\n",
    "    \"\"\"\n",
    "    >>> robots1 = \"User-Agent: *\\\\nDisallow: /posts/\\\\nDisallow: /posts?\\\\nDisallow: /amzn/click/\\\\nDisallow: /questions/ask/\\\\nAllow: /\"\n",
    "    >>> match(robots1)\n",
    "    False\n",
    "    >>> robots2 = \"User-Agent: *\\\\nAllow: /\"\n",
    "    >>> match(robots2)\n",
    "    True\n",
    "    >>> robots3 = \"User-agent: Googlebot-Image\\\\nDisallow: /*/ivc/*\\\\nUser-Agent: *\\\\nAllow: /\"\n",
    "    >>> match(robots3)\n",
    "    True\n",
    "    \"\"\"\n",
    "    return re.search(r'\\bUser-Agent: \\*\\nAllow: /$',robots) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8**\n",
    "\n",
    "Write a function that extracts all phone numbers from given text and return the findings as a list of strings. Phone numbers might contain parentheses or hyphens. You don't need to clean it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(text):\n",
    "    \"\"\"\n",
    "    extracts all phone numbers from given \n",
    "    text and return the findings as a \n",
    "    list of strings\n",
    "    :Example:\n",
    "    >>> text1 = \"Contact us\\\\nFinancial Aid and Scholarships Office\\\\nPhone: (858)534-4480\\\\nFax: (858)534-5459\\\\nWebsite: fas.ucsd.edu\\\\nEmail: finaid@ucsd.edu\\\\nMailing address:\\\\n9500 Gilman Drive, Mail Code 0013\\\\nLa Jolla, CA 92093-0013\"\n",
    "    ['(858)534-4480','(858)534-5459']\n",
    "    >>> text2 = \"Contact us\\\\nPhone: 858-534-4480\\\\nFax: 858-534-5459\\\\nMailing address:\\\\n9500 Gilman Drive, Mail Code 0013\\\\nLa Jolla, CA 92093-00130\"\n",
    "    ['858-534-4480','858-534-5459']\n",
    "    \"\"\"\n",
    "    return re.findall('((?:\\(\\d{3}\\)|\\d{3})-?\\d{3}-?\\d{4})',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(858)534-4480', '(858)534-5459']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = \"Contact us\\\\nFinancial Aid and Scholarships Office\\\\nPhone: (858)534-4480\\\\nFax: (858)534-5459\\\\nWebsite: fas.ucsd.edu\\\\nEmail: finaid@ucsd.edu\\\\nMailing address:\\\\n9500 Gilman Drive, Mail Code 0013\\\\nLa Jolla, CA 92093-0013\"\n",
    "extract(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics on Text \n",
    "\n",
    "**Question 9**\n",
    "\n",
    "What is the tf-idf value of the word 'data' in the given corpus sentences below? Write a function `tfidf_data` that takes in a dataframe like sentences and calculates the tf-idf value of the word 'data' for each text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    In text processing, words of the text represen...\n",
       "1    How do we encode such data in a way which is r...\n",
       "2    The mapping from textual data to real valued v...\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pd.Series(['In text processing, words of the text represent discrete, categorical features ',\n",
    "                         'How do we encode such data in a way which is ready to be used by the algorithms',\n",
    "                         'The mapping from textual data to real valued vectors is called feature extraction'])\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_data(sentences):\n",
    "    \"\"\"\n",
    "    tf-idf of the word 'data' in a list of `sentences`.\n",
    "    \"\"\"\n",
    "    words = pd.Series(sentences.str.split().sum())\n",
    "\n",
    "    # tf = sentences.iloc[1].count('cow') / (sentences.iloc[1].count(' ') + 1)\n",
    "    tf = sentences.str.count(r'\\bdata\\b') / (sentences.str.count(' ') + 1)\n",
    "    idf = np.log(len(sentences) / sentences.str.contains(r'\\bdata\\b').sum())\n",
    "\n",
    "    tfidf = tf*idf\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.000000\n",
       "1    0.022526\n",
       "2    0.031190\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_data(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Features\n",
    "\n",
    "Suppose we're some millionaires who plan on investing in a new movie. Before we throw in our money, we would like to find out what type of movie is most popular and what type of movie makes the most. For simplicity, we are only considering the score of the movies and gross income of the movies when evaluating.\n",
    "\n",
    "`movie` is a collection of movie records. We'll be primarily working with the following columns in movie: `gross`, `genres`, `imdb_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gross</th>\n",
       "      <th>genres</th>\n",
       "      <th>imdb_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>760505847.0</td>\n",
       "      <td>Action|Adventure|Fantasy|Sci-Fi</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>309404152.0</td>\n",
       "      <td>Action|Adventure|Fantasy</td>\n",
       "      <td>7.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>200074175.0</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "      <td>6.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>448130642.0</td>\n",
       "      <td>Action|Thriller</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>73058679.0</td>\n",
       "      <td>Action|Adventure|Sci-Fi</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         gross                           genres  imdb_score\n",
       "0  760505847.0  Action|Adventure|Fantasy|Sci-Fi         7.9\n",
       "1  309404152.0         Action|Adventure|Fantasy         7.1\n",
       "2  200074175.0        Action|Adventure|Thriller         6.8\n",
       "3  448130642.0                  Action|Thriller         8.5\n",
       "5   73058679.0          Action|Adventure|Sci-Fi         6.6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loads the dataset for you to use, do not change anything here\n",
    "movie = pd.read_csv('data/movie_metadata.csv')\n",
    "movie = movie.loc[:,['gross', 'genres', 'imdb_score']].dropna()\n",
    "movie.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 10**\n",
    "\n",
    "We begin by extracting information from the `genres` column. Observe that genre of the movies are given as a string. Write a helper function to called `vectorize` that vectorizes the `genres` column. (Check out [Lecture 12](https://github.com/ucsd-ets/dsc80-sp19/blob/master/lectures/12/Lecture%2012%20Text%20Data.ipynb) if you get stuck! This is bag of words!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(df):\n",
    "    \"\"\"\n",
    "    Create a vector, indexed by the distinct words, with counts of the words in that entry.\n",
    "    \"\"\"\n",
    "    return pd.Series(df['genres'].split('|')).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models\n",
    "\n",
    "You're working with a team to create a security feature for a new web browser and search engine: *Noogle Brome*.  You want to classify domains as either malicious or benign so that your searches don't return any sites that could harm your users!\n",
    "\n",
    "Your team has already collected features from a bunch of sites whose maliciousness was already known.  Someone on your team looked through the scikit-learn documentation and built a model, but they don't know if it's any good.  That's where you come in.\n",
    "\n",
    "Read in the data, `malicious-sites.csv` and take a look at what you're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MALICIOUS</th>\n",
       "      <th>URL_LENGTH</th>\n",
       "      <th>NUMBER_SPECIAL_CHARACTERS</th>\n",
       "      <th>CHARSET</th>\n",
       "      <th>SERVER</th>\n",
       "      <th>WHOIS_COUNTRY</th>\n",
       "      <th>WHOIS_STATEPRO</th>\n",
       "      <th>WHOIS_AGE_DAYS</th>\n",
       "      <th>WHOIS_UPDATED_DAYS</th>\n",
       "      <th>TCP_CONVERSATION_EXCHANGE</th>\n",
       "      <th>DIST_REMOTE_TCP_PORT</th>\n",
       "      <th>REMOTE_IPS</th>\n",
       "      <th>APP_BYTES</th>\n",
       "      <th>SOURCE_APP_PACKETS</th>\n",
       "      <th>REMOTE_APP_PACKETS</th>\n",
       "      <th>SOURCE_APP_BYTES</th>\n",
       "      <th>REMOTE_APP_BYTES</th>\n",
       "      <th>APP_PACKETS</th>\n",
       "      <th>DNS_QUERY_TIMES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>ISO-8859-1</td>\n",
       "      <td>nginx</td>\n",
       "      <td>US</td>\n",
       "      <td>AK</td>\n",
       "      <td>7995</td>\n",
       "      <td>1999</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>3812</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "      <td>18784</td>\n",
       "      <td>4380</td>\n",
       "      <td>39</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>None</td>\n",
       "      <td>US</td>\n",
       "      <td>TX</td>\n",
       "      <td>8212</td>\n",
       "      <td>573</td>\n",
       "      <td>57</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4278</td>\n",
       "      <td>61</td>\n",
       "      <td>62</td>\n",
       "      <td>129889</td>\n",
       "      <td>4586</td>\n",
       "      <td>61</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>UTF-8</td>\n",
       "      <td>nginx</td>\n",
       "      <td>SC</td>\n",
       "      <td>Mahe</td>\n",
       "      <td>1179</td>\n",
       "      <td>1177</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>894</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>838</td>\n",
       "      <td>894</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>iso-8859-1</td>\n",
       "      <td>Apache/2</td>\n",
       "      <td>US</td>\n",
       "      <td>CO</td>\n",
       "      <td>6150</td>\n",
       "      <td>1240</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1189</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>8559</td>\n",
       "      <td>1327</td>\n",
       "      <td>14</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>us-ascii</td>\n",
       "      <td>Microsoft-HTTPAPI/2.0</td>\n",
       "      <td>US</td>\n",
       "      <td>FL</td>\n",
       "      <td>8109</td>\n",
       "      <td>803</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MALICIOUS  URL_LENGTH  NUMBER_SPECIAL_CHARACTERS     CHARSET  \\\n",
       "0          0          17                          6  ISO-8859-1   \n",
       "1          0          17                          6       UTF-8   \n",
       "2          0          18                          7       UTF-8   \n",
       "3          0          18                          6  iso-8859-1   \n",
       "4          0          19                          6    us-ascii   \n",
       "\n",
       "                  SERVER WHOIS_COUNTRY WHOIS_STATEPRO  WHOIS_AGE_DAYS  \\\n",
       "0                  nginx            US             AK            7995   \n",
       "1                   None            US             TX            8212   \n",
       "2                  nginx            SC           Mahe            1179   \n",
       "3               Apache/2            US             CO            6150   \n",
       "4  Microsoft-HTTPAPI/2.0            US             FL            8109   \n",
       "\n",
       "   WHOIS_UPDATED_DAYS  TCP_CONVERSATION_EXCHANGE  DIST_REMOTE_TCP_PORT  \\\n",
       "0                1999                         31                    22   \n",
       "1                 573                         57                     2   \n",
       "2                1177                         11                     6   \n",
       "3                1240                         12                     0   \n",
       "4                 803                          0                     0   \n",
       "\n",
       "   REMOTE_IPS  APP_BYTES  SOURCE_APP_PACKETS  REMOTE_APP_PACKETS  \\\n",
       "0           3       3812                  39                  37   \n",
       "1           5       4278                  61                  62   \n",
       "2           9        894                  11                  13   \n",
       "3           3       1189                  14                  13   \n",
       "4           0          0                   0                   0   \n",
       "\n",
       "   SOURCE_APP_BYTES  REMOTE_APP_BYTES  APP_PACKETS  DNS_QUERY_TIMES  \n",
       "0             18784              4380           39              8.0  \n",
       "1            129889              4586           61              4.0  \n",
       "2               838               894           11              0.0  \n",
       "3              8559              1327           14              2.0  \n",
       "4                 0                 0            0              0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/malicious-sites.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A teammate has already created a train-test split, and initialized a Random Forest classifier.\n",
    "\n",
    "*Note:* Do NOT modify this cell.  It is imperative for the Question 1 that these parameters and random states remain fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Do NOT modify this cell ###\n",
    "\n",
    "X, y = df.drop(\"MALICIOUS\", axis=1), df[\"MALICIOUS\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=50, max_depth=5, min_samples_split=2, class_weight={0:0.5},\n",
    "    random_state=123\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 11**\n",
    "\n",
    "Go ahead and finish the preprocessing.  We want all of our categorical columns to be one-hot encoded.  Return a list of the categorical columns in our dataset (any order is fine) in the function `categorical_columns`.\n",
    "\n",
    "Then, finish the preprocessing step of the pipeline below by using the return value of `categorical_columns` to help you one-hot encode the categorical features before arriving at the classifier.  Make sure to get this correct, as a correct pipeline is necessary for the rest of this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = (ColumnTransformer(transformers=[\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore'), [\"CHARSET\", \"SERVER\", \"WHOIS_COUNTRY\", \"WHOIS_STATEPRO\"]), # Preprocessing\n",
    "],\n",
    "                             remainder='passthrough'))\n",
    "\n",
    "pl = Pipeline([\n",
    "    \n",
    "    # Finish this step!\n",
    "    (\"pre\", preproc),\n",
    "    \n",
    "    (\"clf\", rf_classifier)\n",
    "    \n",
    "])\n",
    "\n",
    "pl.fit(X_train, y_train)\n",
    "y_pred = pl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 12**\n",
    "\n",
    "We're curious how \"good\" that model really is.  Calculate various metrics to get a sense of what our model is predicting, and how good it is, and then answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9420731707317073"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model has too many false positives, then we're \\_\\_\\_\\_, but if there are too many false negatives, then we're \\_\\_\\_\\_.  Return only the best choice in `false_consequences`.\n",
    "\n",
    "1. Hurting innocent domain holders; Hurting malicious domain holders\n",
    "2. Hurting malicious domain holders; Huring innocent domain holders\n",
    "3. Hurting innocent domain holders; Exposing users to malicious sites\n",
    "4. Exposing users to malicious sites; Hurting innocent domain holders\n",
    "\n",
    "What do we call the proportion of malicious websites that you manage to successfully block? Write a function, `blocked_malicious`, which returns all correct choices as well as the proportion that the model above manages to successfully block (go to three decimal places).\n",
    "\n",
    "1. Sensitivity\n",
    "2. True positive rate\n",
    "3. Recall\n",
    "4. <font color=\"blue\">+++++ OTHERS +++++\n",
    "\n",
    "If your boss wants you to claim that the model above is perfect (even though it isn't), what metric would you report?  In the function `fairness_claims`, return all correct choices as well as the value which will be reported.\n",
    "\n",
    "1. Specificity\n",
    "2. Precision\n",
    "3. <font color=\"blue\">+++++ OTHERS +++++\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_consequences():\n",
    "    \"\"\"\n",
    "    \n",
    "    >>> false_consequences() in range(1, 5)\n",
    "    True\n",
    "    \"\"\"\n",
    "    \n",
    "    return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocked_malicious():\n",
    "    \"\"\"\n",
    "    \n",
    "    >>> out = blocked_malicious()\n",
    "    >>> set(out[0]) <= set(range(5))\n",
    "    True\n",
    "    >>> 0 <= out[1] <= 1\n",
    "    True\n",
    "    \"\"\"\n",
    "    \n",
    "    return ([1, 2, 3], 0.345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fairness_claims():\n",
    "    \"\"\"\n",
    "    \n",
    "    >>> out = fairness_claims()\n",
    "    >>> set(out[0]) <= set(range(5))\n",
    "    True\n",
    "    >>> 0 <= out[1] <= 1\n",
    "    True\n",
    "    \"\"\"\n",
    "    \n",
    "    return ([1, 2], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 13:**  \n",
    "\n",
    "Now that we've taken a look at some of the different metrics conduct three parameter searchs on our RandomForestClassifier in order to maximize Recall, Precision, and F1 respectively.\n",
    "\n",
    "As you do so, think about the effect each of the resulting models have on our users and domain holders.  What is maximizing recall good for?  Also, take a look at the confusion matrix for each of the resulting \"best\" models.  Decide for yourself which model *you* would consider to be the fairest.\n",
    "\n",
    "You should check the following parameters:\n",
    "- n_estimators: 10, 100\n",
    "- max_depth: 10, 50\n",
    "- min_samples_split: 2, 4\n",
    "- class_weight: 50%, 20%, 10% weight on class zero\n",
    "\n",
    "Write a function, `parameters` which returns a dictionary of parameters over which the pipeline will search. The keys of your dictionary should be `<step name>__<param name>`, using two underscores (see [sklearn documentation](https://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py)).\n",
    "\n",
    "Write a function, `parameter_search` which takes in X, y, and a pipeline like the one defined in Question 0 (you may assume that there is a step named \"pre\" and a step named \"clf\") and conducts three parameter searches.\n",
    "\n",
    "For each parameter search, return the best pipeline which was able to maximize the score for Recall, Precision, and F1 respectively, using 3-Fold cross validation.\n",
    "\n",
    "*Note:* Our classifier exists inside of a Pipeline.  You will need to figure out how to specify this classifier in order to conduct the parameter search.\n",
    "\n",
    "*Note:* A warning may occur for some of the parameter searches.  Don't be alarmed!  Can you figure out what is causing the warning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters():\n",
    "    \n",
    "    params = {\n",
    "        \"clf__n_estimators\": [10, 100],\n",
    "        \"clf__max_depth\": [10, 50],\n",
    "        \"clf__min_samples_split\": [2, 4],\n",
    "        \"clf__class_weight\": [{0:0.5}, {0:0.2}, {0:0.1}]\n",
    "    }\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_search(X, y, pl):\n",
    "    \n",
    "    params = parameters()\n",
    "\n",
    "    recl_grid = GridSearchCV(pl, params, scoring=\"recall\", cv=3)\n",
    "    prec_grid = GridSearchCV(pl, params, scoring=\"precision\", cv=3)\n",
    "    f1_grid = GridSearchCV(pl, params, scoring=\"f1\", cv=3)\n",
    "    \n",
    "    pl_recl = recl_grid.fit(X, y).best_estimator_\n",
    "    pl_prec = prec_grid.fit(X, y).best_estimator_\n",
    "    pl_f1 = f1_grid.fit(X, y).best_estimator_\n",
    "    \n",
    "    return [pl_recl, pl_prec, pl_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 14:**\n",
    "\n",
    "Finally, let's take a look at parity measures.  You believe that the majority of your users will visit sites that have been established for around one or two decades, so you want to make sure that sites of different ages are being treated with equal fairness.\n",
    "\n",
    "Write a function `age_parity` which takes in X, y, a pipeline, a scoring function, and a value k, and computes parity measures for different age brackets.  Your function should return a Series indexed by age bracket with values as the score of predictions in that age bracket.\n",
    "\n",
    "The domain ages should be bracketed into bins with a width of k years, represented by their upper bound.  For example if we were to use $k=4$, then a domain that is $372$ days old would fall into bin $4$, and a domain that is $6925$ days old would fall into bin $20$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_pairity(X, y, pl, scoring, k):\n",
    "    \n",
    "    def age_brackets(ages, k):\n",
    "        return ages.apply(lambda x: k * (x // (k*365.25) + 1))\n",
    "    \n",
    "    results = X.assign(\n",
    "        malicious=y,\n",
    "        predicted=pl.predict(X),\n",
    "        age_bracket=age_brackets(X.WHOIS_AGE_DAYS, k)\n",
    "    )\n",
    "    \n",
    "    return results.groupby(\"age_bracket\").apply(lambda x: scoring(x.malicious, x.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_bracket\n",
       "4.0     0.916667\n",
       "8.0     0.740741\n",
       "12.0    0.925110\n",
       "16.0    0.888889\n",
       "20.0    0.945824\n",
       "24.0    0.991848\n",
       "28.0    1.000000\n",
       "32.0    1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "age_pairity(X, y, pl, accuracy_score, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
