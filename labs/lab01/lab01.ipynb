{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Lab 01\n",
    "\n",
    "### Due Date: Tuesday January 14, Midnight (11:59 PM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `lab01.py` file, that will be imported into the current notebook.\n",
    "\n",
    "Labs and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *lab assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the lab! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `lab01.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing code from `lab**.py`\n",
    "\n",
    "* We import our `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab**.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab**.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab**.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab**` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lab01 as lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 0 (EXAMPLE):**\n",
    "\n",
    "Write a function that takes in a possibly empty list of integers and:\n",
    "* Returns `True` if there exist two adjacent list elements that are consecutive integers.\n",
    "* Otherwise, returns `False`.\n",
    "\n",
    "For example, because `9` is next to `8`:\n",
    "```\n",
    ">>> lab.consecutive_ints([5,3,6,4,9,8])\n",
    "True\n",
    "```\n",
    "Whereas:\n",
    "```\n",
    ">>> lab.consecutive_ints([1,3,5,7,9])\n",
    "False\n",
    "```\n",
    "\n",
    "*Note*: This question is done for you, to demonstrate a completed homework problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Develop your code here (or in an IDE) if you'd like.\n",
    "# Though only code in lab01.py will be graded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add more cells if you'd like!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code in two ways:\n",
    "1. Run the cell below to test your code. You should also copy the cell and change the input to test further (i.e. write your own doctests)! Does it work for corner cases? Real-world data is **very messy** and you should expect your data processing code to break without thorough testing!\n",
    "2. Run doctests on `lab01.py` by running the following command on the commandline:\n",
    "```\n",
    "python -m doctest lab01.py\n",
    "```\n",
    "If the doctests pass, then there should be *no* output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your code!\n",
    "lab.consecutive_ints([1,3,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.consecutive_ints([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.consecutive_ints([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 1 (median):**\n",
    "\n",
    "Write a function called *median* that takes a non-empty list of numbers, returning the median element of the list. If the list has even length, it should return the mean of the two elements in the middle. Do not use any imported libraries for this question; you may use any built-in function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.median([6, 5, 4, 3, 2]) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.median([50, 20, 15, 40]) == 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.median([1, 2, 3, 4]) == 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.median([0]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try this\n",
    "lab.median([0, -1, 1, 100]) == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 2 (List Distances):**\n",
    "\n",
    "Similar to Question 0, write a function that takes in a possibly empty list of integers and:\n",
    "* Returns `True` if there exist two list elements $i$ places apart, whose distance as integers is also $i$.\n",
    "* Otherwise, returns `False`.\n",
    "\n",
    "Assume your inputs tend to satisfy the condition, and the pair(s) saitifying the condition tend to be close together; design your function to run faster for this case. (Optimizing your code for an assumed distribution of incoming data is very common in data science).\n",
    "\n",
    "For example, because `3` and (the second) `5` are two places apart, and $|3-5| = 2$:\n",
    "```\n",
    ">>> lab.same_diff_ints([5,3,1,5,9,8])\n",
    "True\n",
    "```\n",
    "Whereas:\n",
    "```\n",
    ">>> lab.same_diff_ints([1,3,5,7,9])\n",
    "False\n",
    "```\n",
    "\n",
    "*Note*: Make sure to define some extreme test cases. Use the `%time` command to time your function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lab.same_diff_ints([1,3,5,7,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lab.same_diff_ints([5,3,1,5,9,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lab.same_diff_ints([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lab.same_diff_ints([9,4,1,1,-4,-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Strings and Files\n",
    "\n",
    "The following questions will help you (re)learn the basics of working with strings and reading data from files (which are read in as strings, by default)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 3 (Prefixes):**\n",
    "\n",
    "Write a function `prefixes` that takes a string and returns a string of every consecutive prefix of the input string. For example, `prefixes('Data!')` should return `'DDaDatDataData!'`.  (See the doctests for more examples).\n",
    "\n",
    "Recall that [strings may be sliced](https://docs.python.org/3/tutorial/introduction.html#strings), like lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DDaDatDataData!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.prefixes('Data!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MMaMarMariMarinMarina'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.prefixes('Marina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaaaaraaroaaron'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.prefixes('aaron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Question 4 (Evens reversed):**\n",
    "\n",
    "Write a function `evens_reversed` that takes in a non-negative integer $N$ and returns a string containing all even integers from $1$ to $N$ (inclusive) in reversed order, separated by spaces. Additionally, [zero pad](https://www.tutorialspoint.com/python/string_zfill.htm) each integer, so that each has the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6 4 2'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.evens_reversed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10 08 06 04 02'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.evens_reversed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lab.evens_reversed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'100 098 096 094 092 090 088 086 084 082 080 078 076 074 072 070 068 066 064 062 060 058 056 054 052 050 048 046 044 042 040 038 036 034 032 030 028 026 024 022 020 018 016 014 012 010 008 006 004 002'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time lab.evens_reversed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[Recall](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files) that the built-in function `open` takes in a file path and returns *a file object* (sometimes called a *file handle*). Below are a few properties of file objects:\n",
    "\n",
    "* `open(path)` opens the file at location `path` for reading.\n",
    "* `open(path)` is an *iterable*, which contains successive lines of the file.\n",
    "* Once a file object is opened, after use it should be closed to avoid memory leaks. To ensure a file is closed once done, you should use a *context manager* as follows:\n",
    "```\n",
    "with open(path) as fh:\n",
    "    for line in fh:\n",
    "        process_line(line)\n",
    "```\n",
    "* To read the entire file into a string, use the read method:\n",
    "```\n",
    "with open(path) as fh:\n",
    "    s = fh.read()\n",
    "```\n",
    "However, you should be careful when reading an entire file into memory that the file isn't too big! *You should avoid this whenever possible!*\n",
    "\n",
    "**Question 5 (Reading Files):**\n",
    "\n",
    "Create a function `last_chars` that takes a file object and returns a string consisting of the last character of the line.\n",
    "\n",
    "*Remark:* A newline is the \"delimiter\" of the lines of a file, and doesn't count as part of the line (as the tests imply). Every other character is part of the line. For more info on this, see [the interpretation](https://en.wikipedia.org/wiki/Newline#Interpretation) of files as a 'newline delimited variables' file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hrg'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('data', 'chars.txt')\n",
    "lab.last_chars(open(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_fp = os.path.join('data', 'empty.txt')\n",
    "lab.last_chars(open(empty_fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s,t?.sen'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_fp = os.path.join('data', 'multipleNewLines.txt')\n",
    "lab.last_chars(open(multi_fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `numpy` exercises\n",
    "\n",
    "For an introduction to arrays and `numpy` recall the relevant section of [DSC 10](https://www.inferentialthinking.com/chapters/05/1/Arrays.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6 (Basic Arrays):**\n",
    "\n",
    "Create the following functions using `numpy` methods satisfying the requirements given in each part. Your solutions should **not** contain any loops or list comprehensions.\n",
    "\n",
    "* A function `arr_1` that takes in a `numpy` array and adds to each element the square-root of the index of each element.\n",
    "\n",
    "* A function `arr_2` that takes in a `numpy` array of integers and returns a boolean array (i.e. an array of booleans) whose `ith` element is `True` if and only if the `ith` element of the input array is divisble by 16.\n",
    "\n",
    "* A function `arr_3` that takes in a `numpy` array of [stock prices](https://en.wikipedia.org/wiki/Stock) per share on successive days in USD and returns an array of growth rates. That is, the `ith` number of the output array should contain the rate of growth in stock price between the $i^{th}$ day to the $(i+1)^{th}$ day. The growth rate should be a proportion, rounded to the nearest hundredth.\n",
    "\n",
    "* Suppose:\n",
    "    - `A` is a `numpy` array of [stock prices](https://en.wikipedia.org/wiki/Stock) per share for a company on successive days in USD \n",
    "    - you start with \\\\$20, and put aside \\\\$20 at the end of each day to buy as much stock as possible the following day. \n",
    "    - Any money left-over after a given day is saved for possibly buying stock on a future day. \n",
    "    - Create a function `arr_4` that takes in `A` and returns the day on which you can buy at least one share from 'left-over' money. If this never happens, return `-1`. The first stock purchase occurs on day 0. *Note: you cannot buy fractions of a share of stock*.\n",
    "    \n",
    "*Example:* If the stock price is \\\\$3 every day, then the answer is 'day 1':\n",
    "* day 0: buy 6 shares; \\\\$2 left-over; \\\\$22 at end of day.\n",
    "* day 1: buy 7 shares; \\\\$1 left-over; \\\\$21 at end of day.\n",
    "This is more than the 6 shares that \\\\$20 can buy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  4,  9, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr1 = np.array([1, 2, 3, 4])\n",
    "lab.arr_1(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False,  True, False])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr2 = np.array([1, 2, 16, 17, 32, 33])\n",
    "lab.arr_2(arr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.  ,  0.01, -0.01,  0.  ,  0.  , -0.  ,  0.02,  0.01,  0.02,\n",
       "        0.01,  0.01,  0.  ,  0.  ,  0.01, -0.  , -0.01, -0.  ,  0.  ,\n",
       "       -0.01,  0.01,  0.  , -0.  ,  0.02,  0.  ,  0.01, -0.  ,  0.  ,\n",
       "        0.01, -0.02,  0.01,  0.  , -0.01,  0.01,  0.  , -0.  , -0.01,\n",
       "        0.01,  0.03, -0.01, -0.  ,  0.01,  0.01,  0.  ,  0.  , -0.  ,\n",
       "        0.01,  0.01, -0.  ,  0.  ,  0.02, -0.01, -0.01,  0.01, -0.01,\n",
       "        0.01,  0.02, -0.01, -0.01, -0.  ,  0.01,  0.  , -0.  ,  0.  ,\n",
       "        0.01, -0.  ,  0.01, -0.  ,  0.01,  0.01,  0.01, -0.  , -0.  ,\n",
       "        0.01,  0.  ,  0.  ,  0.02, -0.02, -0.  ,  0.01,  0.  ,  0.01,\n",
       "        0.01, -0.  , -0.02, -0.01, -0.01, -0.01, -0.01,  0.  ,  0.01,\n",
       "        0.01,  0.01, -0.  ,  0.  , -0.  , -0.01,  0.01,  0.01,  0.  ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('data', 'stocks.csv')\n",
    "stocks = np.array([float(x) for x in open(fp)])\n",
    "lab.arr_3(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks4 = np.array([3, 3, 3, 3])\n",
    "lab.arr_4(stocks4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stocks5 = np.array([1, 3, 5, 7])\n",
    "lab.arr_4(stocks5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.arr_4(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Getting Started with Pandas\n",
    "\n",
    "The following questions will help you get comfortable with Pandas. These questions are similar to questions on tables in DSC 10; review the [textbook](https://www.inferentialthinking.com) as necessary. As always for Pandas questions:\n",
    "1. Avoid writing loops through the rows of the dataset to do the problem, and\n",
    "2. Test the output/correctness of your code with the help of the dataset given, but be sure your code will also run on data \"like\" the dataset given (sampling rows using the `.sample` method is useful for this!).\n",
    "\n",
    "**Question 7 (Pandas basics):**\n",
    "\n",
    "Read in the file `movies_by_year.csv` in the `data` directory and understand the dataset by answering the following questions. To do this, create a function `movie_stats` that takes in a dataframe like `movies` and returns a series containing the following statistics:\n",
    "* The number of years covered by the dataset (`num_years`).\n",
    "* The total number of movies made over all years in the dataset (`tot_movies`).\n",
    "* The year with the fewest number of movies made; a tie should return the earliest year (`yr_fewest_movies`).\n",
    "* The average amount of money grossed over all the years in the dataset (`avg_gross`).\n",
    "* The year with the highest gross *per movie* (`highest_per_movie`).\n",
    "* The name of the top movie during the second-lowest (total) grossing year (`second_lowest`).\n",
    "* The average number of movies made the year *after* a Harry Potter movie was the #1 movie (`avg_after_harry`).\n",
    "\n",
    "The index of the output series are given in parenthesis above.\n",
    "\n",
    "*Note*: Your function should work on a dataset of the same format that contains information from other years. You may assume that none of the answers involving ranking returns a tie.\n",
    "\n",
    "*Note*: To make sure your function still runs, in the event that one of the 7 parts throws an exception (e.g. due to a very incorrect answer), use `Try... Except...` structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_fp = os.path.join('data', 'movies_by_year.csv')\n",
    "movies = pd.read_csv(movie_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of years\n",
    "num_years = len(movies['Year'])\n",
    "num_years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17834"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of movies\n",
    "tot_movies = np.sum(movies['Number of Movies'])\n",
    "tot_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1990"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Earliest year with fewest movies\n",
    "yr_fewest_movies = np.min(movies[movies['Number of Movies'] == np.min(movies['Number of Movies'])]['Year'])\n",
    "yr_fewest_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7226.914705882354"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average gross\n",
    "avg_gross = np.mean(movies['Total Gross'])\n",
    "avg_gross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2009"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Year with highest gross per movie\n",
    "highest_per_movie = movies.loc[(movies['Total Gross'] / movies['Number of Movies']).idxmax()]['Year']\n",
    "highest_per_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Back to the Future'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name of top movie during the second-lowest total gross year\n",
    "second_lowest = movies[movies['Total Gross'] == movies['Total Gross'].nsmallest().iloc[1]]['#1 Movie'].iloc[0]\n",
    "second_lowest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "573.0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average number of movies made the year after a Harry Potter movie was the top movie\n",
    "avg_after_harry = np.mean(movies[movies['Year'].isin(movies[movies['#1 Movie'].str.contains(pat='Harry Potter')]['Year'].values + 1)]['Number of Movies'])\n",
    "avg_after_harry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_years                            34\n",
       "tot_movies                        17834\n",
       "yr_fewest_movies                   1990\n",
       "avg_gross                       7226.91\n",
       "highest_per_movie                  2009\n",
       "second_lowest        Back to the Future\n",
       "avg_after_harry                     573\n",
       "dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = {}\n",
    "# movies = movies.sort_values('Year')\n",
    "try: # Total number of years\n",
    "    num_years = len(movies['Year'])\n",
    "    dic.update({'num_years': num_years})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try: # Total number of movies\n",
    "    tot_movies = np.sum(movies['Number of Movies'])\n",
    "    dic.update({'tot_movies': tot_movies})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try: # Earliest year with fewest movies\n",
    "    yr_fewest_movies = np.min(movies[movies['Number of Movies'] == np.min(movies['Number of Movies'])]['Year'])\n",
    "    dic.update({'yr_fewest_movies': yr_fewest_movies})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try: # Average gross\n",
    "    avg_gross = np.mean(movies['Total Gross'])\n",
    "    dic.update({'avg_gross': avg_gross})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try: # Year with highest gross per movie\n",
    "    highest_per_movie = movies.loc[(movies['Total Gross'] / movies['Number of Movies']).idxmax()]['Year']\n",
    "    dic.update({'highest_per_movie': highest_per_movie})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try: # Name of top movie during the second-lowest total gross year\n",
    "    second_lowest = movies[movies['Total Gross'] == movies['Total Gross'].nsmallest().iloc[1]]['#1 Movie'].iloc[0]\n",
    "    dic.update({'second_lowest': second_lowest})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try: # Average number of movies made the year after a Harry Potter movie was the top movie\n",
    "    avg_after_harry = np.mean(movies[movies['Year'].isin(movies[movies['#1 Movie'].str.contains(pat='Harry Potter')]['Year'].values + 1)]['Number of Movies'])\n",
    "    dic.update({'avg_after_harry': avg_after_harry})\n",
    "except:\n",
    "    pass\n",
    "\n",
    "series = pd.Series(dic)\n",
    "series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "num_years                            34\n",
       "tot_movies                        17834\n",
       "yr_fewest_movies                   1990\n",
       "avg_gross                       7226.91\n",
       "highest_per_movie                  2009\n",
       "second_lowest        Back to the Future\n",
       "avg_after_harry                     573\n",
       "dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.movie_stats(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CSV Files\n",
    "\n",
    "**Question 8 (Reading malformed csv files):**\n",
    "\n",
    "`malformed.csv` contains a file of comma-separated values, containing the following fields:\n",
    "\n",
    "\n",
    "|column name|description|type|\n",
    "|---|---|---|\n",
    "|first|first name of person|str|\n",
    "|last|last name of person|str|\n",
    "|weight|weight of person (lbs)|float|\n",
    "|height|height of person (in)|float|\n",
    "|geo|location of person; comma-separated latitude/longitude|str|\n",
    "\n",
    "Unfortunately, the entries contains errors that cause the Pandas `read_csv` function to fail parsing the file with the default settings. Instead, you must read in the file manually using Python's built-in `open` function.\n",
    "\n",
    "Clean the csv file into a Pandas DataFrame with columns as described in the table above, by creating a function called `parse_malformed` that takes in a file path and returns a parsed, properly-typed dataframe. The dataframe should contain columns as described in the table above (with the specified types); it should agree with `pd.read_csv` when the lines are not malformed.\n",
    "\n",
    "\n",
    "*Note:* Assume that the given csv file is a sample of a larger file; you will be graded against a **different** sample of the larger file that has the same type of parsing errors. That is, you should **not** hard-code your cleaning of the data to specific errors on specific lines in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 33: expected 5 fields, saw 6\\nSkipping line 50: expected 5 fields, saw 6\\nSkipping line 57: expected 5 fields, saw 9\\nSkipping line 78: expected 5 fields, saw 8\\nSkipping line 84: expected 5 fields, saw 6\\nSkipping line 89: expected 5 fields, saw 6\\nSkipping line 90: expected 5 fields, saw 6\\nSkipping line 92: expected 5 fields, saw 6\\nSkipping line 94: expected 5 fields, saw 6\\n'\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/malformed.csv', error_bad_lines = False) # line 33, 50, 57, 78, 84, 89, 90, 92, 94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['Julia', 'Wagner', 142.0, 86.0, '39.8,15.4'] True\n",
      "1 ['Angelica', 'Rija', 155.0, 56.0, '38.2,-71.7'] True\n",
      "2 ['Tyler', 'Micajah', 116.0, 73.0, '38.0,6.9'] True\n",
      "3 ['Kathleen', 'Nakea', 163.0, 69.0, '36.3,-86.8'] True\n",
      "4 ['Axel', 'Ronit', 95.0, 74.0, '36.8,128.2'] True\n",
      "5 ['Amiya', 'Kyona', 130.0, 72.0, '36.3,114.5'] True\n",
      "6 ['Torrey', 'Joshuacaleb', 105.0, 79.0, '38.3,145.1'] True\n",
      "7 ['Mariah', 'Alese', 149.0, 68.0, '36.1,45.7'] True\n",
      "8 ['Grayson', 'Daimen', 140.0, 80.0, '38.1,-72.6'] True\n",
      "9 ['Yvette', 'Trayce', 179.0, 67.0, '36.9,-8.3'] True\n",
      "10 ['Cody', 'Hatim', 150.0, 63.0, '38.0,-7.3'] True\n",
      "11 ['Marissa', 'Daud', 135.0, 58.0, '37.3,11.0'] True\n",
      "12 ['Logan', 'Cristel', 133.0, 67.0, '35.5,-110.2'] True\n",
      "13 ['Kaiyah', 'Brinden', 187.0, 82.0, '34.8,83.2'] True\n",
      "14 ['Ivan', 'Devyne', 193.0, 54.0, '36.6,262.0'] True\n",
      "15 ['Shamaria', 'Aldrick', 139.0, 73.0, '38.5,-94.6'] True\n",
      "16 ['Travis', 'Anavictoria', 117.0, 62.0, '36.3,69.5'] True\n",
      "17 ['Kennedy', 'Dalynn', 171.0, 77.0, '37.3,-27.5'] True\n",
      "18 ['Alina', 'Danniell', 105.0, 55.0, '37.4,314.7'] True\n",
      "19 ['Cameron', 'Angelica', 139.0, 56.0, '38.8,-79.3'] True\n",
      "20 ['Madison', 'Barkley', 120.0, 69.0, '38.2,86.1'] True\n",
      "21 ['Jackson', 'Taylr', 113.0, 78.0, '36.7,56.7'] True\n",
      "22 ['Agustin', 'Stephanye', 91.0, 62.0, '36.4,54.5'] True\n",
      "23 ['Janesha', 'Jhayla', 143.0, 64.0, '35.9,-70.5'] True\n",
      "24 ['Nickolas', 'Karenna', 159.0, 75.0, '35.9,-73.9'] True\n",
      "25 ['Stacy', 'Meaghen', 149.0, 68.0, '36.6,-27.7'] True\n",
      "26 ['Matthew', 'Kalis', 166.0, 66.0, '37.8,0.2'] True\n",
      "27 ['Rayona', 'Treniece', 151.0, 59.0, '37.2,140.1'] True\n",
      "28 ['Jack', 'Hanai', 164.0, 70.0, '38.6,-63.6'] True\n",
      "29 ['Hayley', 'Zsazsa', 146.0, 62.0, '36.2,-2.6'] True\n",
      "30 ['Jacqueline', 'Treyten', 164.0, 73.0, '37.1,-56.1'] True\n",
      "31 ['Anthony', 'Janaysia', 127.0, 77.0, '39.1,93.6'] True\n",
      "32 ['Saige', 'Kalieb', 144.0, 61.0, '38.2,16.2'] True\n",
      "33 ['Tiffany', 'Jamire', 160.0, 87.0, '37.6,155.0'] True\n",
      "34 ['Nichole', 'Derryk', 134.0, 66.0, '36.9,-52.0'] True\n",
      "35 ['Litzi', 'Keairah', 137.0, 69.0, '36.8,182.4'] True\n",
      "36 ['Timothy', 'Montrice', 151.0, 65.0, '36.7,69.2'] True\n",
      "37 ['Alicia', 'Lilijana', 161.0, 77.0, '37.2,56.9'] True\n",
      "38 ['Margaret', 'Ruthie', 143.0, 65.0, '36.7,69.2'] True\n",
      "39 ['Javier', 'Allycia', 121.0, 71.0, '37.3,79.6'] True\n",
      "40 ['Melanie', 'Vincent', 112.0, 60.0, '37.3,123.5'] True\n",
      "41 ['Jesus', 'Brennden', 145.0, 75.0, '38.6,-43.4'] True\n",
      "42 ['John', 'Zyon', 115.0, 77.0, '38.1,-106.3'] True\n",
      "43 ['David', 'Aubrea', 122.0, 62.0, '38.0,-21.9'] True\n",
      "44 ['Wesley', 'Laniece', 165.0, 68.0, '37.2,71.2'] True\n",
      "45 ['Lindsey', 'Alima', 79.0, 66.0, '36.2,-61.4'] True\n",
      "46 ['Kenneth', 'Kimo', 124.0, 54.0, '37.0,49.6'] True\n",
      "47 ['Lauryn', 'Milia', 133.0, 62.0, '37.7,113.6'] True\n",
      "48 ['Ryan', 'Elleanor', 149.0, 68.0, '37.0,-32.3'] True\n",
      "49 ['Keyara', 'Rozlyn', 148.0, 70.0, '37.5,74.6'] True\n",
      "50 ['Tyree', 'Windy', 128.0, 80.0, '36.9,83.6'] True\n",
      "51 ['Tori', 'Adithi', 187.0, 68.0, '35.5,-12.4'] True\n",
      "52 ['Carolina', 'Cecille', 154.0, 71.0, '38.6,12.4'] True\n",
      "53 ['Emmanuel', 'Leidi', 112.0, 61.0, '37.6,64.6'] True\n",
      "54 ['Andrew', 'Vikrant', 90.0, 61.0, '36.6,36.9'] True\n",
      "55 ['Donovan', 'Randa', 120.0, 73.0, '36.5,28.2'] True\n",
      "56 ['Rachel', 'Alexiz', 145.0, 67.0, '38.0,36.4'] True\n",
      "57 ['Dylan', 'Aryon', 104.0, 65.0, '35.0,18.0'] True\n",
      "58 ['Jeevan', 'Ardit', 148.0, 71.0, '36.9,-58.9'] True\n",
      "59 ['Kaitlin', 'Isrrael', 108.0, 78.0, '36.2,16.0'] True\n",
      "60 ['Vanessa', 'Guiselle', 96.0, 77.0, '38.0,86.0'] True\n",
      "61 ['Alyssa', 'Gillian', 88.0, 69.0, '38.2,252.0'] True\n",
      "62 ['Christina', 'Jerard', 178.0, 77.0, '38.2,-57.5'] True\n",
      "63 ['Hayli', 'Marton', 125.0, 73.0, '37.7,-93.7'] True\n",
      "64 ['Isabella', 'Laverne', 117.0, 76.0, '36.4,88.1'] True\n",
      "65 ['Parker', 'Kerly', 160.0, 59.0, '36.7,139.3'] True\n",
      "66 ['Madeleine', 'Amritpal', 166.0, 58.0, '36.2,51.4'] True\n",
      "67 ['Keaton', 'Kinzy', 119.0, 64.0, '36.4,20.5'] True\n",
      "68 ['Mercedes', 'Tarika', 129.0, 70.0, '37.0,-56.8'] True\n",
      "69 ['Dallen', 'Keshae', 127.0, 75.0, '37.1,-113.1'] True\n",
      "70 ['Shay', 'Marcea', 103.0, 65.0, '36.0,-33.1'] True\n",
      "71 ['Adrianna', 'Karsten', 157.0, 81.0, '34.8,36.1'] True\n",
      "72 ['Max', 'Nairi', 113.0, 67.0, '36.9,12.2'] True\n",
      "73 ['Suzanne', 'Dearion', 129.0, 70.0, '36.9,-70.4'] True\n",
      "74 ['Camryn', 'Tasnim', 126.0, 63.0, '36.5,19.8'] True\n",
      "75 ['Cielo', 'Saylah', 103.0, 65.0, '36.1,83.1'] True\n",
      "76 ['Annastasia', 'Averi', 91.0, 67.0, '37.1,-43.7'] True\n",
      "77 ['Hannah', 'Jerit', 142.0, 53.0, '36.6,10.3'] True\n",
      "78 ['Jovita', 'Marquaveon', 150.0, 70.0, '38.2,-32.6'] True\n",
      "79 ['Jasmine', 'Jahmal', 107.0, 72.0, '38.5,9.1'] True\n",
      "80 ['Cheyenne', 'Gerrid', 55.0, 71.0, '36.6,68.2'] True\n",
      "81 ['Karma', 'Nameer', 131.0, 65.0, '37.5,69.8'] True\n",
      "82 ['Madeline', 'Cira', 128.0, 65.0, '37.4,149.6'] True\n",
      "83 ['Connor', 'Sheridyn', 141.0, 73.0, '36.8,-80.7'] True\n",
      "84 ['Sarah', 'Sunday', 180.0, 54.0, '36.7,-5.6'] True\n",
      "85 ['Victoria', 'Maliik', 164.0, 60.0, '36.3,151.2'] True\n",
      "86 ['George', 'Tyshonna', 105.0, 74.0, '37.5,-91.5'] True\n",
      "87 ['Marquis', 'Kylar', 137.0, 51.0, '38.2,228.4'] True\n",
      "88 ['Marlene', 'Fatuma', 117.0, 64.0, '36.4,-9.5'] True\n",
      "89 ['Kathryn', 'Diondra', 149.0, 65.0, '38.3,68.0'] True\n",
      "90 ['Jonathan', 'Jersey', 107.0, 77.0, '36.0,11.8'] True\n",
      "91 ['Joshua', 'Caileen', 181.0, 67.0, '37.3,79.0'] True\n",
      "92 ['Emily', 'Leonid', 146.0, 57.0, '37.8,-68.7'] True\n",
      "93 ['Jacqualyn', 'Angelea', 116.0, 63.0, '36.4,-31.5'] True\n",
      "94 ['Gionna', 'Pamala', 180.0, 79.0, '38.6,-28.6'] True\n",
      "95 ['Yasmeen', 'Jahron', 135.0, 84.0, '38.3,-127.3'] True\n",
      "96 ['Meghan', 'Carlyann', 101.0, 66.0, '36.6,80.5'] True\n",
      "97 ['Tess', 'Shree', 146.0, 68.0, '38.8,64.9'] True\n",
      "98 ['Maria', 'Kalvyn', 115.0, 51.0, '37.1,-90.4'] True\n",
      "99 ['Lexie', 'Cheyenna', 151.0, 71.0, '35.9,86.1'] True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>first</th>\n",
       "      <th>last</th>\n",
       "      <th>weight</th>\n",
       "      <th>height</th>\n",
       "      <th>geo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Julia</td>\n",
       "      <td>Wagner</td>\n",
       "      <td>142.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>39.8,15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Angelica</td>\n",
       "      <td>Rija</td>\n",
       "      <td>155.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>38.2,-71.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tyler</td>\n",
       "      <td>Micajah</td>\n",
       "      <td>116.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>38.0,6.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kathleen</td>\n",
       "      <td>Nakea</td>\n",
       "      <td>163.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>36.3,-86.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Axel</td>\n",
       "      <td>Ronit</td>\n",
       "      <td>95.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>36.8,128.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amiya</td>\n",
       "      <td>Kyona</td>\n",
       "      <td>130.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>36.3,114.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Torrey</td>\n",
       "      <td>Joshuacaleb</td>\n",
       "      <td>105.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>38.3,145.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mariah</td>\n",
       "      <td>Alese</td>\n",
       "      <td>149.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>36.1,45.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Grayson</td>\n",
       "      <td>Daimen</td>\n",
       "      <td>140.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>38.1,-72.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Yvette</td>\n",
       "      <td>Trayce</td>\n",
       "      <td>179.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>36.9,-8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Cody</td>\n",
       "      <td>Hatim</td>\n",
       "      <td>150.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>38.0,-7.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Marissa</td>\n",
       "      <td>Daud</td>\n",
       "      <td>135.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>37.3,11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Logan</td>\n",
       "      <td>Cristel</td>\n",
       "      <td>133.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>35.5,-110.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Kaiyah</td>\n",
       "      <td>Brinden</td>\n",
       "      <td>187.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>34.8,83.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ivan</td>\n",
       "      <td>Devyne</td>\n",
       "      <td>193.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>36.6,262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Shamaria</td>\n",
       "      <td>Aldrick</td>\n",
       "      <td>139.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>38.5,-94.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Travis</td>\n",
       "      <td>Anavictoria</td>\n",
       "      <td>117.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.3,69.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Kennedy</td>\n",
       "      <td>Dalynn</td>\n",
       "      <td>171.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>37.3,-27.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Alina</td>\n",
       "      <td>Danniell</td>\n",
       "      <td>105.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>37.4,314.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Cameron</td>\n",
       "      <td>Angelica</td>\n",
       "      <td>139.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>38.8,-79.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Madison</td>\n",
       "      <td>Barkley</td>\n",
       "      <td>120.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>38.2,86.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Jackson</td>\n",
       "      <td>Taylr</td>\n",
       "      <td>113.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>36.7,56.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Agustin</td>\n",
       "      <td>Stephanye</td>\n",
       "      <td>91.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.4,54.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Janesha</td>\n",
       "      <td>Jhayla</td>\n",
       "      <td>143.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>35.9,-70.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Nickolas</td>\n",
       "      <td>Karenna</td>\n",
       "      <td>159.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>35.9,-73.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Stacy</td>\n",
       "      <td>Meaghen</td>\n",
       "      <td>149.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>36.6,-27.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Matthew</td>\n",
       "      <td>Kalis</td>\n",
       "      <td>166.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>37.8,0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Rayona</td>\n",
       "      <td>Treniece</td>\n",
       "      <td>151.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>37.2,140.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Jack</td>\n",
       "      <td>Hanai</td>\n",
       "      <td>164.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>38.6,-63.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Hayley</td>\n",
       "      <td>Zsazsa</td>\n",
       "      <td>146.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>36.2,-2.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Shay</td>\n",
       "      <td>Marcea</td>\n",
       "      <td>103.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>36.0,-33.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Adrianna</td>\n",
       "      <td>Karsten</td>\n",
       "      <td>157.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>34.8,36.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Max</td>\n",
       "      <td>Nairi</td>\n",
       "      <td>113.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>36.9,12.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Suzanne</td>\n",
       "      <td>Dearion</td>\n",
       "      <td>129.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>36.9,-70.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Camryn</td>\n",
       "      <td>Tasnim</td>\n",
       "      <td>126.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>36.5,19.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Cielo</td>\n",
       "      <td>Saylah</td>\n",
       "      <td>103.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>36.1,83.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Annastasia</td>\n",
       "      <td>Averi</td>\n",
       "      <td>91.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>37.1,-43.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Hannah</td>\n",
       "      <td>Jerit</td>\n",
       "      <td>142.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>36.6,10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Jovita</td>\n",
       "      <td>Marquaveon</td>\n",
       "      <td>150.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>38.2,-32.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Jasmine</td>\n",
       "      <td>Jahmal</td>\n",
       "      <td>107.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>38.5,9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Cheyenne</td>\n",
       "      <td>Gerrid</td>\n",
       "      <td>55.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>36.6,68.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>Karma</td>\n",
       "      <td>Nameer</td>\n",
       "      <td>131.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>37.5,69.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>Madeline</td>\n",
       "      <td>Cira</td>\n",
       "      <td>128.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>37.4,149.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>Connor</td>\n",
       "      <td>Sheridyn</td>\n",
       "      <td>141.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>36.8,-80.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>Sarah</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>180.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>36.7,-5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>Victoria</td>\n",
       "      <td>Maliik</td>\n",
       "      <td>164.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>36.3,151.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>George</td>\n",
       "      <td>Tyshonna</td>\n",
       "      <td>105.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>37.5,-91.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Marquis</td>\n",
       "      <td>Kylar</td>\n",
       "      <td>137.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>38.2,228.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Marlene</td>\n",
       "      <td>Fatuma</td>\n",
       "      <td>117.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>36.4,-9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Kathryn</td>\n",
       "      <td>Diondra</td>\n",
       "      <td>149.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>38.3,68.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Jonathan</td>\n",
       "      <td>Jersey</td>\n",
       "      <td>107.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>36.0,11.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Joshua</td>\n",
       "      <td>Caileen</td>\n",
       "      <td>181.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>37.3,79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Emily</td>\n",
       "      <td>Leonid</td>\n",
       "      <td>146.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>37.8,-68.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Jacqualyn</td>\n",
       "      <td>Angelea</td>\n",
       "      <td>116.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>36.4,-31.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Gionna</td>\n",
       "      <td>Pamala</td>\n",
       "      <td>180.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>38.6,-28.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Yasmeen</td>\n",
       "      <td>Jahron</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>38.3,-127.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Meghan</td>\n",
       "      <td>Carlyann</td>\n",
       "      <td>101.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>36.6,80.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Tess</td>\n",
       "      <td>Shree</td>\n",
       "      <td>146.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>38.8,64.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Maria</td>\n",
       "      <td>Kalvyn</td>\n",
       "      <td>115.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>37.1,-90.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Lexie</td>\n",
       "      <td>Cheyenna</td>\n",
       "      <td>151.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>35.9,86.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         first         last  weight  height          geo\n",
       "0        Julia       Wagner   142.0    86.0    39.8,15.4\n",
       "1     Angelica         Rija   155.0    56.0   38.2,-71.7\n",
       "2        Tyler      Micajah   116.0    73.0     38.0,6.9\n",
       "3     Kathleen        Nakea   163.0    69.0   36.3,-86.8\n",
       "4         Axel        Ronit    95.0    74.0   36.8,128.2\n",
       "5        Amiya        Kyona   130.0    72.0   36.3,114.5\n",
       "6       Torrey  Joshuacaleb   105.0    79.0   38.3,145.1\n",
       "7       Mariah        Alese   149.0    68.0    36.1,45.7\n",
       "8      Grayson       Daimen   140.0    80.0   38.1,-72.6\n",
       "9       Yvette       Trayce   179.0    67.0    36.9,-8.3\n",
       "10        Cody        Hatim   150.0    63.0    38.0,-7.3\n",
       "11     Marissa         Daud   135.0    58.0    37.3,11.0\n",
       "12       Logan      Cristel   133.0    67.0  35.5,-110.2\n",
       "13      Kaiyah      Brinden   187.0    82.0    34.8,83.2\n",
       "14        Ivan       Devyne   193.0    54.0   36.6,262.0\n",
       "15    Shamaria      Aldrick   139.0    73.0   38.5,-94.6\n",
       "16      Travis  Anavictoria   117.0    62.0    36.3,69.5\n",
       "17     Kennedy       Dalynn   171.0    77.0   37.3,-27.5\n",
       "18       Alina     Danniell   105.0    55.0   37.4,314.7\n",
       "19     Cameron     Angelica   139.0    56.0   38.8,-79.3\n",
       "20     Madison      Barkley   120.0    69.0    38.2,86.1\n",
       "21     Jackson        Taylr   113.0    78.0    36.7,56.7\n",
       "22     Agustin    Stephanye    91.0    62.0    36.4,54.5\n",
       "23     Janesha       Jhayla   143.0    64.0   35.9,-70.5\n",
       "24    Nickolas      Karenna   159.0    75.0   35.9,-73.9\n",
       "25       Stacy      Meaghen   149.0    68.0   36.6,-27.7\n",
       "26     Matthew        Kalis   166.0    66.0     37.8,0.2\n",
       "27      Rayona     Treniece   151.0    59.0   37.2,140.1\n",
       "28        Jack        Hanai   164.0    70.0   38.6,-63.6\n",
       "29      Hayley       Zsazsa   146.0    62.0    36.2,-2.6\n",
       "..         ...          ...     ...     ...          ...\n",
       "70        Shay       Marcea   103.0    65.0   36.0,-33.1\n",
       "71    Adrianna      Karsten   157.0    81.0    34.8,36.1\n",
       "72         Max        Nairi   113.0    67.0    36.9,12.2\n",
       "73     Suzanne      Dearion   129.0    70.0   36.9,-70.4\n",
       "74      Camryn       Tasnim   126.0    63.0    36.5,19.8\n",
       "75       Cielo       Saylah   103.0    65.0    36.1,83.1\n",
       "76  Annastasia        Averi    91.0    67.0   37.1,-43.7\n",
       "77      Hannah        Jerit   142.0    53.0    36.6,10.3\n",
       "78      Jovita   Marquaveon   150.0    70.0   38.2,-32.6\n",
       "79     Jasmine       Jahmal   107.0    72.0     38.5,9.1\n",
       "80    Cheyenne       Gerrid    55.0    71.0    36.6,68.2\n",
       "81       Karma       Nameer   131.0    65.0    37.5,69.8\n",
       "82    Madeline         Cira   128.0    65.0   37.4,149.6\n",
       "83      Connor     Sheridyn   141.0    73.0   36.8,-80.7\n",
       "84       Sarah       Sunday   180.0    54.0    36.7,-5.6\n",
       "85    Victoria       Maliik   164.0    60.0   36.3,151.2\n",
       "86      George     Tyshonna   105.0    74.0   37.5,-91.5\n",
       "87     Marquis        Kylar   137.0    51.0   38.2,228.4\n",
       "88     Marlene       Fatuma   117.0    64.0    36.4,-9.5\n",
       "89     Kathryn      Diondra   149.0    65.0    38.3,68.0\n",
       "90    Jonathan       Jersey   107.0    77.0    36.0,11.8\n",
       "91      Joshua      Caileen   181.0    67.0    37.3,79.0\n",
       "92       Emily       Leonid   146.0    57.0   37.8,-68.7\n",
       "93   Jacqualyn      Angelea   116.0    63.0   36.4,-31.5\n",
       "94      Gionna       Pamala   180.0    79.0   38.6,-28.6\n",
       "95     Yasmeen       Jahron   135.0    84.0  38.3,-127.3\n",
       "96      Meghan     Carlyann   101.0    66.0    36.6,80.5\n",
       "97        Tess        Shree   146.0    68.0    38.8,64.9\n",
       "98       Maria       Kalvyn   115.0    51.0   37.1,-90.4\n",
       "99       Lexie     Cheyenna   151.0    71.0    35.9,86.1\n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['first', 'last', 'weight', 'height', 'geo'])\n",
    "fp = os.path.join('data', 'malformed.csv')\n",
    "with open(fp) as file:\n",
    "    file.readline() # Get rid of title\n",
    "    for i, line in enumerate(file):\n",
    "        content = line.rstrip().split(',') # Strip new lines\n",
    "        content = list(filter(None, content)) # Remove empty strings\n",
    "        content[-2:] = [','.join(content[-2:])] # Combine geo elements\n",
    "        content = list(map(lambda it: it.strip('\\\"'), content)) # Strip elements of \"\"\n",
    "        content[2] = float(content[2]) # Cast weight as float\n",
    "        content[3] = float(content[3]) # Cast height as float\n",
    "        print(i, content, len(content) == 5)\n",
    "        df = df.append(pd.Series(content, index=['first', 'last', 'weight', 'height', 'geo']), ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done!\n",
    "\n",
    "* Submit the lab on Gradescope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
