{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Lab 03\n",
    "\n",
    "### Due Date: Tuesday January 28th, Midnight - 11:59 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `lab**.py` file, that will be imported into the current notebook.\n",
    "\n",
    "Labs and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *lab assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the lab! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `lab**.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing code from `lab**.py`\n",
    "\n",
    "* We import our `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab**.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab**.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab**.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab**` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lab03 as lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Hypothetically speaking...\n",
    "\n",
    "In this section we'll develop an intuition for the terms and structure of hypothesis testing -- it's nothing to be afraid of!\n",
    "\n",
    "The first step is always to define what you're looking at, create your hypotheses, and set a level of significance.  Once you've done that, you can find a p-value which is related to your test statistic.\n",
    "\n",
    "If all of these words are scary: look at the lecture notebook, the textbook references, and don't forget to think about the real-world meaning of these terms!  The following example describes a real-world scenario, so you can think of it in a normal lens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1: Faulty tires**\n",
    "\n",
    "A tire manufacturer tests whether a set of tires meets the company's performance standards by checking:\n",
    "\n",
    "> In 60 out of 100 tests, if a Honda CRV can come to a complete stop from 60 mph in fewer than 108 feet.\n",
    "\n",
    "That is, 60% of the time, the stopping distance of the car should be above average for the car (outfitted with generic tires). The factory is wondering if a current run of tires is up to standard, so they choose a random set of tires from the production line to test their performance, and bring the car to a complete stop from 60 mph a total of 100 times. Then they ask:\n",
    "\n",
    "> Are these tires faulty? Or are they safe?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following are valid null hypotheses that address the question we are trying to answer, using the data we are given?  Which are valid alternative hypotheses?\n",
    "\n",
    "Outfitted with that set of tires, the car:\n",
    "1. has a 60 mph stopping distance under 108 feet, at least 60% of the time.\n",
    "1. has a 60 mph stopping distance under 108 feet, at most 60% of the time.\n",
    "1. has a 60 mph stopping distance under 108 feet, equal to 60% of the time.\n",
    "1. has at least as short stopping distance to the same car with generic tires, at least 60% of the time.\n",
    "1. has at least as short stopping distance to the same car with generic tires, at most 60% of the time.\n",
    "1. has at least as short stopping distance to the same car with generic tires, roughly 60% of the time.\n",
    "1. is as safe as the car with generic tires.\n",
    "1. causes the car to stop in a shorter distance.\n",
    "\n",
    "\n",
    "Write a function `car_null_hypoth` which takes zero arguments and returns a list of the valid null hypotheses.  \n",
    "Write a function `car_alt_hypoth` which takes zero arguments and returns a list of the valid alternative hypotheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 7]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Null Hypothesis # ASK!!!!!!!!!!!!!!!!!!!!!\n",
    "[3, 6, 7] # [3, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 8]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative Hypothesis # ASK!!!!!!!!!!!!!!!!!!!!!\n",
    "[1, 4, 8] # We can test both tail right? # [2, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the following are valid test statistics for our question?\n",
    "\n",
    "1. The average number of feet the car took to come to a complete stop in 100 attempts.\n",
    "1. The number of times the car stopped in under 108 feet in 100 attempts.\n",
    "1. The number of attempts it took before the car stopped in under 108 feet.\n",
    "1. The proportion of attempts the car successfully stopped in under 108 feet.\n",
    "\n",
    "Write a function `car_test_stat` which takes zero arguments and returns a list of valid test statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test statistic # ASK!!!!!!!!!!!!!!!!!!!!!\n",
    "[2, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is calculated as how likely it is to find something as extreme or more extreme than our observed test statistic.  To do this, we assume the null hypothesis is true, and then define \"extremeness\" based on the alternative hypothesis.\n",
    "\n",
    "Why don't we just look at the probability of finding our observed test statistic?\n",
    "\n",
    "1. Because our observed test statistic isn't extreme.\n",
    "2. Because the probability of finding our observed test statistic equals the probability of finding something more extreme.\n",
    "3. Because the probability of finding our observed test statistic is essentially zero.\n",
    "4. Because our null hypothesis isn't suggesting equality.\n",
    "5. Because our alternative hypothesis isn't suggesting equality.\n",
    "\n",
    "Write a function `car_p_value` which takes zero arguments and returns the correct reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p-val # ASK!!!!!!!!!!!!!!!!!!!!! What's wrong just looking at the prop in observaton\n",
    "5 # Correct is 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping: Google Play Store\n",
    "\n",
    "The questions below analyze a dataset of Google Play Store apps. The dataset has been preprocessed slightly for your convenience.\n",
    "\n",
    "Columns:\n",
    "* `App`: App Name\n",
    "* `Category`: App Category\n",
    "* `Rating`: Average App Rating\n",
    "* `Reviews`: Number of Reviews\n",
    "* `Size`: Size of App\n",
    "* `Installs`: Binned Number of Installs\n",
    "* `Type`: Paid or Free\n",
    "* `Price`: Price of App\n",
    "* `Content Rating`: Age group the app is targeted at\n",
    "* `Last Updated`: Last Updated Date\n",
    "\n",
    "\n",
    "Link: https://www.kaggle.com/lava18/google-play-store-apps\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "First, we'd like to do some basic cleaning to this dataset to better analyze it.\n",
    "In the function `clean_apps`, which takes the Play Store dataset as input, clean as follows and return the cleaned df:\n",
    "* Convert `Reviews` to type int.\n",
    "* Strip all letters from the ends of `Size`, convert all units to unit kilobyte, and convert the column to type float (Hint: all Sizes end in either M (megabyte) or k (kilobyte); a helper function may be useful here).\n",
    "* Strip the '+' from the ends of `Installs`, remove the commas, and convert it to type int.\n",
    "* Since `Type` is binary, change all the 'Free's to 1 and the 'Paid's to 0.\n",
    "* Strip dollar mark in `Price` and convert it to correct numeric data type.\n",
    "* Strip all but the year (e.g. 2018) from `Last Updated` and convert it to type int.\n",
    "\n",
    "Please return a *copy* of the original dataframe; don't alter the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Category</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Size</th>\n",
       "      <th>Installs</th>\n",
       "      <th>Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>Content Rating</th>\n",
       "      <th>Last Updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5324</th>\n",
       "      <td>BSPlayer ARMv7 VFP CPU support</td>\n",
       "      <td>VIDEO_PLAYERS</td>\n",
       "      <td>4.3</td>\n",
       "      <td>9966</td>\n",
       "      <td>5.5M</td>\n",
       "      <td>1,000,000+</td>\n",
       "      <td>Free</td>\n",
       "      <td>0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>March 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8887</th>\n",
       "      <td>FL SW Fishing Regulations</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>4.6</td>\n",
       "      <td>60</td>\n",
       "      <td>24M</td>\n",
       "      <td>1,000+</td>\n",
       "      <td>Paid</td>\n",
       "      <td>$1.99</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>March 7, 2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3440</th>\n",
       "      <td>Ludo - Don't get angry</td>\n",
       "      <td>GAME</td>\n",
       "      <td>4.2</td>\n",
       "      <td>131</td>\n",
       "      <td>23M</td>\n",
       "      <td>1,000+</td>\n",
       "      <td>Paid</td>\n",
       "      <td>$1.61</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>February 24, 2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Weight Loss Running by Verv</td>\n",
       "      <td>HEALTH_AND_FITNESS</td>\n",
       "      <td>4.5</td>\n",
       "      <td>27393</td>\n",
       "      <td>59M</td>\n",
       "      <td>1,000,000+</td>\n",
       "      <td>Free</td>\n",
       "      <td>0</td>\n",
       "      <td>Mature 17+</td>\n",
       "      <td>July 16, 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5282</th>\n",
       "      <td>BR</td>\n",
       "      <td>LIFESTYLE</td>\n",
       "      <td>3.0</td>\n",
       "      <td>859</td>\n",
       "      <td>3.5M</td>\n",
       "      <td>100,000+</td>\n",
       "      <td>Free</td>\n",
       "      <td>0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>July 19, 2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 App            Category  Rating  Reviews  \\\n",
       "5324  BSPlayer ARMv7 VFP CPU support       VIDEO_PLAYERS     4.3     9966   \n",
       "8887       FL SW Fishing Regulations              SPORTS     4.6       60   \n",
       "3440          Ludo - Don't get angry                GAME     4.2      131   \n",
       "954      Weight Loss Running by Verv  HEALTH_AND_FITNESS     4.5    27393   \n",
       "5282                              BR           LIFESTYLE     3.0      859   \n",
       "\n",
       "      Size    Installs  Type  Price Content Rating       Last Updated  \n",
       "5324  5.5M  1,000,000+  Free      0       Everyone     March 31, 2017  \n",
       "8887   24M      1,000+  Paid  $1.99       Everyone      March 7, 2014  \n",
       "3440   23M      1,000+  Paid  $1.61       Everyone  February 24, 2016  \n",
       "954    59M  1,000,000+  Free      0     Mature 17+      July 16, 2018  \n",
       "5282  3.5M    100,000+  Free      0       Everyone      July 19, 2018  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play_fp = os.path.join('data', 'googleplaystore.csv')\n",
    "play = pd.read_csv(play_fp)\n",
    "play.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to convert mega to kilo\n",
    "def size_to_kilo(size):\n",
    "    \"\"\"\n",
    "    Convert Megabyte and Kilobyte to Kilobytes,\n",
    "    and strip string off M or K.\n",
    "    \n",
    "    :param size: string to convert\n",
    "    :return: float, the converted number\n",
    "    \"\"\"\n",
    "\n",
    "    if size == np.nan: # No size info\n",
    "        return 0\n",
    "    \n",
    "    prev, last = size[:-1], size[-1]\n",
    "    if last == 'M': # Megabyte\n",
    "        return float(prev) * 1000\n",
    "    else: # Kilobyte\n",
    "        return float(prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answers\n",
    "cleaned = play.copy() # Deep copy of dataframe\n",
    "cleaned['Reviews'] = cleaned['Reviews'].astype(int) # Cast Reviews to int\n",
    "cleaned['Size'] = cleaned['Size'].apply(size_to_kilo).astype(float) # Convert size to kilobyte\n",
    "cleaned['Installs'] = cleaned['Installs'].str.replace(',', '').str.replace('+', '').astype(int) # Strip , +\n",
    "cleaned['Type'] = cleaned['Type'].apply(lambda tp: 1 if tp == 'Free' else 0).astype(int) # Binary format of Type\n",
    "cleaned['Price'] = cleaned['Price'].str.replace('$', '').astype(float) # Strip $ and convert to float\n",
    "cleaned['Last Updated'] = cleaned['Last Updated'].str[-4:].astype(int) # Strip everthing but the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Category</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Size</th>\n",
       "      <th>Installs</th>\n",
       "      <th>Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>Content Rating</th>\n",
       "      <th>Last Updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Photo Editor &amp; Candy Camera &amp; Grid &amp; ScrapBook</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "      <td>4.1</td>\n",
       "      <td>159</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Coloring book moana</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "      <td>3.9</td>\n",
       "      <td>967</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>500000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U Launcher Lite – FREE Live Cool Themes, Hide ...</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "      <td>4.7</td>\n",
       "      <td>87510</td>\n",
       "      <td>8700.0</td>\n",
       "      <td>5000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sketch - Draw &amp; Paint</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "      <td>4.5</td>\n",
       "      <td>215644</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>50000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Teen</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pixel Draw - Number Art Coloring Book</td>\n",
       "      <td>ART_AND_DESIGN</td>\n",
       "      <td>4.3</td>\n",
       "      <td>967</td>\n",
       "      <td>2800.0</td>\n",
       "      <td>100000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 App        Category  Rating  \\\n",
       "0     Photo Editor & Candy Camera & Grid & ScrapBook  ART_AND_DESIGN     4.1   \n",
       "1                                Coloring book moana  ART_AND_DESIGN     3.9   \n",
       "2  U Launcher Lite – FREE Live Cool Themes, Hide ...  ART_AND_DESIGN     4.7   \n",
       "3                              Sketch - Draw & Paint  ART_AND_DESIGN     4.5   \n",
       "4              Pixel Draw - Number Art Coloring Book  ART_AND_DESIGN     4.3   \n",
       "\n",
       "   Reviews     Size  Installs  Type  Price Content Rating  Last Updated  \n",
       "0      159  19000.0     10000     1    0.0       Everyone          2018  \n",
       "1      967  14000.0    500000     1    0.0       Everyone          2018  \n",
       "2    87510   8700.0   5000000     1    0.0       Everyone          2018  \n",
       "3   215644  25000.0  50000000     1    0.0           Teen          2018  \n",
       "4      967   2800.0    100000     1    0.0       Everyone          2018  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9145 entries, 0 to 9144\n",
      "Data columns (total 10 columns):\n",
      "App               9145 non-null object\n",
      "Category          9145 non-null object\n",
      "Rating            7729 non-null float64\n",
      "Reviews           9145 non-null int32\n",
      "Size              9145 non-null float64\n",
      "Installs          9145 non-null int32\n",
      "Type              9145 non-null int32\n",
      "Price             9145 non-null float64\n",
      "Content Rating    9145 non-null object\n",
      "Last Updated      9145 non-null int32\n",
      "dtypes: float64(3), int32(4), object(3)\n",
      "memory usage: 571.6+ KB\n"
     ]
    }
   ],
   "source": [
    "cleaned.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (Continued)**\n",
    "\n",
    "Now, we can do some basic exploration.\n",
    "\n",
    "In the function `store_info`, find the following using the **cleaned** dataframe:\n",
    "* Find the year with the highest median `Installs`, among all years with at least 100 apps.\n",
    "* Find the `Content Rating` with the highest minimum `Rating`.\n",
    "* Find the `Category` has the highest average price.\n",
    "* Find the `Category` with lowest average rating, among apps that have at least 1000 reviews.\n",
    "\n",
    "and return these values in a list.\n",
    "\n",
    "*Remark:* Note that the last question is asking you to compute the *average of averages* (the 'Rating' column contains the average rating of an app) -- such analyses are prone to occurrences of Simpson's Paradox. Considering apps with at least 1000 reviews helps limit the effect of such [ecological fallacies](https://afraenkel.github.io/practical-data-science/05/understanding-aggregations.html#reversing-aggregations-ecological-fallacies).\n",
    "* You can assume there is no ties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2018, 'Adults only 18+', 'FINANCE', 'DATING']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "# Q2.1\n",
    "df_install = cleaned.groupby('Last Updated').aggregate({'App': 'count', 'Installs': 'median'}) # Aggregate count over App, median over Installs\n",
    "year = df_install[df_install['App'] >= 100]['Installs'].idxmax() # Year with App >= 100, Installs median max\n",
    "\n",
    "# Q2.2\n",
    "df_rating = cleaned.groupby('Content Rating')['Rating'].min() # Group by Content Rating to find min Rating\n",
    "cont_rate = df_rating.idxmax() # Content Rating with max min Rating\n",
    "\n",
    "# Q2.3 # ASK!!!!!!!!!!!!!!!!!!!!! include free/$0 app? Result for this df does not matter\n",
    "df_categh = cleaned.groupby('Category')['Price'].mean() # Group by Category to find average Price\n",
    "# df_categ = cleaned[cleaned['Type'] == 0].groupby('Category')['Price'].mean() # Not include Free App\n",
    "categ_h = df_categh.idxmax() # Cateogry with max average Pirce\n",
    "\n",
    "# Q2.4\n",
    "df_categl = cleaned[cleaned['Reviews'] >= 1000].groupby('Category')['Rating'].mean() # Group by Category to find mean Rating\n",
    "categ_l = df_categl.idxmin() # Category with min mean Rating\n",
    "\n",
    "[year, cont_rate, categ_h, categ_l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.store_info(cleaned) == ([year, cont_rate, categ_h, categ_l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming Apps review count by App category\n",
    "\n",
    "A reasonable question that we may ask after cleaning the apps dataset is that how popular each app is. One way of measuring popularity of apps is by studying its review count within their respective category. \n",
    "\n",
    "**Question 3**\n",
    "* Create a function `std_reviews_by_app_cat` that takes in a **cleaned** dataframe and outputs a dataframe with \n",
    "    - the same rows as the input,\n",
    "    - two columns given by `['Category', 'Reviews']`,\n",
    "    - where the `Reviews` columns are *standardized by app category* -- that is, the number of reviews for every app is put into the standard units for the category it belongs to. For a review of standard units, see the [DSC 10 Textbook](https://www.inferentialthinking.com/chapters/15/1/Correlation)\n",
    "    - *Hint*: use the methoc `groupby` and `transform`.\n",
    "* Lastly, create a function `su_and_spread` that returns a list of two items (hard-code your answers):\n",
    "    - Consider the following scenario: half of the apps in the category 'FAMILY' receives ratings of 0 stars while the other\n",
    "    half has rating of 5 stars. Similarly, the ‘MEDICAL' category has half 1-star and half 4-star apps.\n",
    "    Which app would have a higher rating after standarization? The five stars in the family category or the four stars in the\n",
    "    medical one. Answer with the name of the corresponding category ('FAMILY'/'MEDICAL') or use 'equal' if you think both\n",
    "    rating would be the same after standarization. (Don't worry about the uppercase but do be careful with the spelling). \n",
    "    - Which category type has the biggest \"spread\" of review count?\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = lab.clean_apps(play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate standard units\n",
    "def standard_units(nums):\n",
    "    \"\"\"\n",
    "    Convert any array/Series of numbers to standard units.\n",
    "    \n",
    "    :param nums: an array of number\n",
    "    :return: standardized array/Series of nums\n",
    "    \"\"\"\n",
    "    \n",
    "    return (nums - np.mean(nums))/np.std(nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3.1\n",
    "# cleaned.groupby(['Category'])['Reviews'].aggregate([np.mean, np.std]) # mean and s.d. of each category\n",
    "review_by_categ = cleaned[['Category', 'Reviews']].copy() # Deep copy of cleaned 'Category' & 'Reviews'\n",
    "review_by_categ['Reviews'] = review_by_categ.groupby('Category')['Reviews'].transform(standard_units) # Transform into standard units\n",
    "# review_by_categ # Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>App</th>\n",
       "      <th>Category</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "      <th>Size</th>\n",
       "      <th>Installs</th>\n",
       "      <th>Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>Content Rating</th>\n",
       "      <th>Last Updated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>954</th>\n",
       "      <td>Weight Loss Running by Verv</td>\n",
       "      <td>HEALTH_AND_FITNESS</td>\n",
       "      <td>4.5</td>\n",
       "      <td>27393</td>\n",
       "      <td>59000.0</td>\n",
       "      <td>1000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Mature 17+</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             App            Category  Rating  Reviews  \\\n",
       "954  Weight Loss Running by Verv  HEALTH_AND_FITNESS     4.5    27393   \n",
       "\n",
       "        Size  Installs  Type  Price Content Rating  Last Updated  \n",
       "954  59000.0   1000000     1    0.0     Mature 17+          2018  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 1.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3.2 Part 1\n",
    "fam = [0] * 71 + [5] * 71\n",
    "med = [1] * 46 + [4] * 46\n",
    "fam_mean, fam_std = np.mean(fam), np.std(fam)\n",
    "med_mean, med_std = np.mean(med), np.std(med)\n",
    "fam_z = (5 - fam_mean) / fam_std\n",
    "med_z = (4 - med_mean) / med_std\n",
    "fam_z, med_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GAME', 'GAME')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3.2 Part 2 # ASK!!!!!!!!!!!!!!!!!!!!!!! How to define spread? Range? Or standard deviation? # Simulation!!!\n",
    "max_spread = cleaned.groupby(['Category'])['Reviews'].aggregate([np.mean, np.std, np.max, np.min]) # mean and s.d. of each category\n",
    "max_spread['range'] = max_spread['amax'] - max_spread['amin']\n",
    "max_spread['range'].idxmax(), max_spread['std'].idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['equal', 'GAME']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "['equal', 'GAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook Friends\n",
    "\n",
    "**Question 4**\n",
    "\n",
    "A group of students decided to send out a survey to their Facebook friends. Each student asks 1000 of their friends for their first and last name, the company they currently work at, their job title, their email, and the university they attended. Combine all the data contained in the files `survey*.csv` (within the `responses` folder within the data folder) into a single dataframe. The number of files and the number of rows in each file may vary, so don't hardcode your answers!\n",
    "\n",
    "Create a function `read_survey` which takes in a directory path (containing files `survey*.csv`), and outputs a dataframe with six columns titled: `first name`, `last name`, `current company`, `job title`, `email`, `university` (in that order). \n",
    "\n",
    "*Hint*: You can list the files in a directory using `os.listdir`.\n",
    "\n",
    "*Remark: You may have to do some cleaning to make this possible!*\n",
    "\n",
    "Create a function `com_stats` that takes in in a dataframe and returns a list containing: the most common first name, job held, university attended, and current company (in that order) for people with emails that end in \".com\". If there are ties for the most common value, give the value with the \"largest size\" (as defined by the python string ordering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4.1\n",
    "dirname = os.path.join('data', 'responses')\n",
    "# lab.read_survey(dirname)\n",
    "surveys = pd.DataFrame(columns=['first name', 'last name', 'current company', 'job title', 'email', 'university'])\n",
    "# print(surveys)\n",
    "for fp in os.listdir(dirname): # Read through file names in dir\n",
    "    if fp[:6] != 'survey' or fp[-4:] != '.csv': # ASK !!!!!!!!!!!!!!!!!!!!!!!!!!!!! Do we need to consider if there are other files included in directory?\n",
    "        continue\n",
    "    \n",
    "    # np.array(os.listdir(dirname))[pd.Series(os.listdir(dirname)).str.contains('^survey[0-9]+.csv$')] # A more complicated process to match the pattern\n",
    "    \n",
    "    df = pd.read_csv(os.path.join(dirname, fp)) # Read from directory path\n",
    "    df.columns = df.columns.str.lower().str.replace('_', ' ') # Standardize column names\n",
    "    surveys = pd.concat([surveys, df], ignore_index=True, sort=False) # Append new df to surveys\n",
    "# surveys # Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.read_survey(dirname).equals(surveys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS~\n",
    "# df1 = pd.read_csv(os.path.join(dirname, 'survey1.csv'))\n",
    "# df2 = pd.read_csv(os.path.join(dirname, 'survey2.csv'))\n",
    "# df3 = pd.read_csv(os.path.join(dirname, 'survey3.csv'))\n",
    "# df4 = pd.read_csv(os.path.join(dirname, 'survey4.csv'))\n",
    "# df5 = pd.read_csv(os.path.join(dirname, 'survey5.csv'))\n",
    "# df1.columns = df1.columns.str.lower().str.replace('_', ' ') # Standardize column names\n",
    "# df2.columns = df2.columns.str.lower().str.replace('_', ' ') # Standardize column names\n",
    "# df3.columns = df3.columns.str.lower().str.replace('_', ' ') # Standardize column names\n",
    "# df4.columns = df4.columns.str.lower().str.replace('_', ' ') # Standardize column names\n",
    "# df5.columns = df5.columns.str.lower().str.replace('_', ' ') # Standardize column names\n",
    "\n",
    "# df_arr = [df1, df2, df3, df4, df5]\n",
    "# temp = pd.concat(df_arr, ignore_index = True, sort=False)[['first name', 'last name', 'current company', 'job title', 'email', 'university']]\n",
    "# temp.equals(surveys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Q4.2 # ASK!!!!!!!!!!!!!!!!!!! 1) Does not have company? What to do? 2) Easier solution\n",
    "df = surveys.copy()\n",
    "df_c = df.copy().fillna('') # Deep copy, fill NaN with empty string\n",
    "name = df_c[~(df_c['first name'] == '')]['first name'].value_counts()#.sort_index(ascending=False)#.sort_values() #.sort_index(ascending=False)#.idxmax() # Johannah\n",
    "name_ind = name[name == name.max()].sort_index(ascending=False).idxmax()\n",
    "\n",
    "job = df_c[~(df_c['job title'] == '')]['job title'].value_counts() # Chemical Engineer\n",
    "job_ind = job[job == job.max()].sort_index(ascending=False).idxmax()\n",
    "\n",
    "univer = df_c[~(df_c['university'] == '')]['university'].value_counts() # Southwest University\n",
    "univer_ind = univer[univer == univer.max()].sort_index(ascending=False).idxmax()\n",
    "\n",
    "comp = df_c[(df_c['email'].str.contains('.com$')) & ~(df_c['current company'] == '')]['current company'].value_counts() # Tillman LLC\n",
    "# temp2 = df[(df['email'].str.contains('.com$')) & ~(df['email'].isnull())]['current company'].value_counts() # Tillman LLC\n",
    "comp_ind = comp[comp == comp.max()].sort_index(ascending=False).idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Johannah', 'Chemical Engineer', 'Southwest University', 'Tillman LLC']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer\n",
    "[name_ind, job_ind, univer_ind, comp_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Data\n",
    "**Question 5**\n",
    "\n",
    "Every week, a professor sends out an extra credit survey asking for students' favorite things (animals, movies, etc). \n",
    "- Each student who has completed at least 75% of the surveys receives 5 points of extra credit.\n",
    "- If at least 90% of the class answers at least one of the questions (ex. favorite animal), *everyone* in the class receives 1 point of extra credit. This overall class extra credit only applies once (ex. If 95% of students answer favorite color and 91% answer favorite animal, the entire class still only receives 1 extra point as a class).\n",
    "\n",
    "Create a function `combine_surveys` which takes in a directory path (containing files `favorite*.csv`) and combines all of the survey data into one DataFrame, indexed by student ID (a value 1 - 1000).\n",
    "\n",
    "Create a function `check_credit` which takes in a DataFrame with the combined survey data and outputs a DataFrame of the names of students and how many extra credit points they would receive, indexed by their ID (a value 1-1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5.1 ASK!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Can we use glob in this case? \n",
    "dirname = os.path.join('data', 'extra-credit-surveys')\n",
    "# df = lab.combine_surveys(dirname)\n",
    "dataframes = [] # List to hold all dataframes\n",
    "for fp in os.listdir(dirname): # Read through file names in dir\n",
    "    if fp[:8] != 'favorite' or fp[-4:] != '.csv': # ASK !!!!!!!!!!!!!!!!!!!!!!!!!!!!! Do we need to consider if there are other files included in directory?\n",
    "        continue\n",
    "    df = pd.read_csv(os.path.join(dirname, fp)) # Read from directory path\n",
    "    dataframes.append(df.set_index('id')) # Append dataframe\n",
    "favorites = pd.concat(dataframes, axis=1, sort=False)\n",
    "# favorites # Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab.combine_surveys(dirname).equals(favorites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTS~\n",
    "# df1 = pd.read_csv(os.path.join(dirname, 'favorite1.csv')).set_index('id')\n",
    "# df2 = pd.read_csv(os.path.join(dirname, 'favorite2.csv')).set_index('id')\n",
    "# df3 = pd.read_csv(os.path.join(dirname, 'favorite3.csv')).set_index('id')\n",
    "# df4 = pd.read_csv(os.path.join(dirname, 'favorite4.csv')).set_index('id')\n",
    "# df5 = pd.read_csv(os.path.join(dirname, 'favorite5.csv')).set_index('id')\n",
    "# df6 = pd.read_csv(os.path.join(dirname, 'favorite6.csv')).set_index('id')\n",
    "\n",
    "# df_arr = [df1, df2, df3, df4, df5, df6]\n",
    "# pd.concat(df_arr, axis=1, sort=False).equals(favorites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5.2 ASK!!!!!!!!!!!!!!!!!!!!!!! 1) Movie No Strings Attached lol, only () counts? 2) name/id as one question? 3) name of extra credit columns?\n",
    "df = favorites.copy()\n",
    "df_c =  df.copy().replace('\\(no ', np.nan, regex=True) # Deep copy & Data Cleaning\n",
    "# df['movie'].unique()\n",
    "# df[df['movie'].str.lower().str.contains('no')]\n",
    "# df['genre'].unique()\n",
    "# df[df['genre'].str.lower().str.contains('\\(no ')]\n",
    "# df['animal'].unique()\n",
    "# df[df['animal'].str.lower().str.contains('no ')]\n",
    "# df['plant'].unique()\n",
    "# df[df['plant'].str.lower().str.contains('no ')]\n",
    "# df['color'].unique()\n",
    "# df[df['color'].str.lower().str.contains('no ')]\n",
    "\n",
    "cols = np.array(df_c.columns)[np.array(df_c.columns) != 'name'] # Array of cols without 'name'\n",
    "\n",
    "# Helper function to check proportion of completion\n",
    "def prop_complete(lst):\n",
    "    \"\"\"\n",
    "    Given a row/col, check prop of not empty string.\n",
    "    \n",
    "    :param lst: col/row to check\n",
    "    :return: prop of completion\n",
    "    \"\"\"\n",
    "    return np.count_nonzero(~lst.isna()) / len(lst)\n",
    "\n",
    "ind_prop = df_c[cols].apply(prop_complete, axis=1) # Individual EC, apply prop to rows\n",
    "ind_extra = (ind_prop >= 0.75).replace(True, 5).rename('extra credit') # Individual EC Series\n",
    "class_prop = df_c[cols].apply(prop_complete, axis=0) # Class EC, apply prop to col\n",
    "class_extra = np.any(class_prop > 0.90) # Any question 90% completion\n",
    "if class_extra: # If 90% completion for any question, class extra credit\n",
    "    ind_extra = ind_extra + 1\n",
    "df_extra = pd.concat([df['name'], ind_extra], axis=1)\n",
    "# df_extra # Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining pets and owners\n",
    "\n",
    "**Question 6**\n",
    "\n",
    "You are analyzing data from a veterinarian clinic. The datasets contain several types of information from the clinic, including its customers (pet owners), pets, and available procedures and history. The column names are self-explanatory. These dataframes are provided to you:\n",
    "-  `owners` stores the customer information, where every `OwnerID` is unique (verify yourself).\n",
    "-  `pets` stores the pet information. Each pet belongs to a customer in `owners`.\n",
    "-  `procedure_detail` contains a catalog of procedures that are offered by the clinic.\n",
    "-  `procedure_history` has procedure records. Each procedure is given to a pet in `pets`.\n",
    "\n",
    "You want to answer the following questions:\n",
    "\n",
    "1. How many pets in the dataset have had at least one procedure? Note that some pets are registered but haven't had any procedure performed. Create a function `at_least_once` that takes in `pets`, `procedure_history` and returns the number of pets as an integer.\n",
    "\n",
    "2. What is the name of each customer's pet(s)? Create a function `pet_name_by_owner` that takes in `owners`, `pets` and returns a Series that holds the pet name (as a string) indexed by owner's (first) name. If an owner has multiple pets, the corresponding value should be a list of names as strings.\n",
    "\n",
    "3. How much does each customer spend in total on procedures? Create a function `total_cost_per_owner` that returns a Series that contains the sum of money that an owner has spent on their pets' procedures, indexed by `OwnerID`. Hint: think of what makes a procedure unique in the context of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "owners_fp = os.path.join('data', 'pets', 'Owners.csv')\n",
    "pets_fp = os.path.join('data', 'pets', 'Pets.csv')\n",
    "procedure_detail_fp = os.path.join('data', 'pets', 'ProceduresDetails.csv')\n",
    "procedure_history_fp = os.path.join('data', 'pets', 'ProceduresHistory.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "owners = pd.read_csv(owners_fp)\n",
    "pets = pd.read_csv(pets_fp)\n",
    "procedure_detail = pd.read_csv(procedure_detail_fp)\n",
    "procedure_history = pd.read_csv(procedure_history_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6.1\n",
    "proc_hist = procedure_history.groupby('PetID', as_index=False).count()[['PetID', 'Date']].rename(columns={'Date': 'Count'}) # Pet procedure counts\n",
    "pet_proc = pd.merge(pets, proc_hist, how = \"left\", on='PetID') # Pet procedure dataframe\n",
    "np.count_nonzero(~pet_proc['Count'].isna()) # Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "First Name\n",
       "Jessica                      Biscuit\n",
       "Rosa                           Stowe\n",
       "Susan                           Enyo\n",
       "Benjamin          [Danger, Collette]\n",
       "Charles                        Rumba\n",
       "Joe                       Heisenberg\n",
       "Jason                       Crockett\n",
       "Joseph                       Blackie\n",
       "Carolyn                       Cookie\n",
       "Doris                          Scout\n",
       "Jeffrey                       Bandit\n",
       "Christopher                    Rumba\n",
       "William                       Goethe\n",
       "Robert                           Taz\n",
       "Luisa                           Lily\n",
       "Wm                             Simba\n",
       "John                           Kashi\n",
       "Anne                         Natacha\n",
       "Bruce                          Bruce\n",
       "John                         Biscuit\n",
       "Travis                       Houdini\n",
       "Paul                           Tiger\n",
       "Ed                             Simba\n",
       "Lee            [Bright, Angel, Jake]\n",
       "Susan                          Daisy\n",
       "Connie                       Biscuit\n",
       "Marion                       Cuddles\n",
       "Dorothy                        Daisy\n",
       "Bruce                         Danger\n",
       "Lena                         Biscuit\n",
       "                       ...          \n",
       "John                           Rumba\n",
       "Tony                          Dexter\n",
       "Debra                         Cookie\n",
       "Bobbie                         Priya\n",
       "Deborah                       Sarosh\n",
       "Tom                            Tiger\n",
       "Mario                  [Pip, Dexter]\n",
       "Ricardo                      Cuddles\n",
       "Florence                     Vuitton\n",
       "Edna                          Cookie\n",
       "Julia                       Antigone\n",
       "Gustavo                        Tiger\n",
       "Elvia                  [Scout, Lily]\n",
       "Paula                         Keller\n",
       "Stacey         [Taz, Candy, Humbert]\n",
       "Jackie                      Thatcher\n",
       "Max                        Bonaparte\n",
       "Daniel                         Bruce\n",
       "Robert                        Draper\n",
       "Elizabeth                  Sojourner\n",
       "Roger                         Danger\n",
       "Enrique                         Jake\n",
       "Bruce                        Swiffer\n",
       "Joyce                        Lakshmi\n",
       "Robert                           Taz\n",
       "William                       Bandit\n",
       "Carmen                       Maripol\n",
       "Steven                         Rumba\n",
       "Gary                [Scooter, Daisy]\n",
       "Marie                           Dior\n",
       "Name: Name, Length: 89, dtype: object"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6.2\n",
    "# Helper function to concatenate pet names\n",
    "def concat_pets(strs):\n",
    "    \"\"\"\n",
    "    Concatenate pet names.\n",
    "    \n",
    "    :param strs: strings to parse in\n",
    "    :return: string if one pet name, list if more\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(strs) ==  1: # If only one string\n",
    "        return np.sum(strs)\n",
    "    else: # If more\n",
    "        return list(strs)\n",
    "        \n",
    "pets_owners = pd.merge(pets, owners.rename(columns={'Name':'First Name'}), on='OwnerID')\n",
    "owned = pets_owners.groupby(['OwnerID', 'First Name']).aggregate({'Name':concat_pets}).reset_index('OwnerID', drop=True)#.loc['Lee']\n",
    "owned['Name'] # Answer\n",
    "# owned[owned.index.duplicated()] # Check duplicated index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OwnerID\n",
       "1070     25.0\n",
       "1132      0.0\n",
       "1202      0.0\n",
       "1306      0.0\n",
       "1312      0.0\n",
       "1319      0.0\n",
       "1334      0.0\n",
       "1546     10.0\n",
       "1653     10.0\n",
       "1766     10.0\n",
       "1899      0.0\n",
       "1915    305.0\n",
       "2063      0.0\n",
       "2103      0.0\n",
       "2419     10.0\n",
       "2700      0.0\n",
       "2722     10.0\n",
       "2755      0.0\n",
       "2809     20.0\n",
       "2863      0.0\n",
       "2967      0.0\n",
       "3034      0.0\n",
       "3086     10.0\n",
       "3089     40.0\n",
       "3245     10.0\n",
       "3518      0.0\n",
       "3615      0.0\n",
       "3644      0.0\n",
       "3661      0.0\n",
       "3663      0.0\n",
       "        ...  \n",
       "7219      0.0\n",
       "7261    125.0\n",
       "7340      0.0\n",
       "7343      0.0\n",
       "7359      0.0\n",
       "7393      0.0\n",
       "7484      0.0\n",
       "7579    325.0\n",
       "7581    185.0\n",
       "7606     10.0\n",
       "7663     10.0\n",
       "7772      0.0\n",
       "7846     10.0\n",
       "7908      0.0\n",
       "8133     10.0\n",
       "8143     40.0\n",
       "8215     30.0\n",
       "8316    450.0\n",
       "8619      0.0\n",
       "8830      0.0\n",
       "9037      0.0\n",
       "9039      0.0\n",
       "9365     10.0\n",
       "9385      0.0\n",
       "9427      0.0\n",
       "9604     10.0\n",
       "9614      0.0\n",
       "9828     10.0\n",
       "9850      0.0\n",
       "9900     10.0\n",
       "Name: Price, Length: 89, dtype: float64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6.3 # ASK!!!!!!!!!!!!!!!!!!!!!!!!!! procedure merge actually does not matter? As they would actually just merge based on two columns?\n",
    "# Remember to process 'owned' dataframe using functions\n",
    "proc_full = pd.merge(procedure_detail, procedure_history, on=['ProcedureType', 'ProcedureSubCode']) # Procedure costs\n",
    "pet_proc_full = pd.merge(pets, proc_full, how = \"left\", on='PetID') # Pets procedures\n",
    "pet_owner_all = pd.merge(pet_proc_full, owners.rename(columns={'Name':'First Name'}), on='OwnerID') # Every information\n",
    "pet_owner_all.groupby('OwnerID')['Price'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProcedureType</th>\n",
       "      <th>ProcedureSubCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>GENERAL SURGERIES</td>\n",
       "      <td>17</td>\n",
       "      <td>Radical Mastectomy</td>\n",
       "      <td>450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ProcedureType  ProcedureSubCode         Description  Price\n",
       "38  GENERAL SURGERIES                17  Radical Mastectomy    450"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# owners[owners['OwnerID'] == 8316]\n",
    "# pets[pets['OwnerID'] == 8316] # PetID: J1-6366\n",
    "# procedure_history[procedure_history['PetID'] == 'J1-6366'] # ProcedureType: GENERAL SURGERIES, ProcedureSubCode: 17\n",
    "procedure_detail[(procedure_detail['ProcedureType'] == 'GENERAL SURGERIES') & (procedure_detail['ProcedureSubCode'] == 17)] # 450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProcedureType</th>\n",
       "      <th>ProcedureSubCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>GENERAL SURGERIES</td>\n",
       "      <td>8</td>\n",
       "      <td>Umbilical</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ProcedureType  ProcedureSubCode Description  Price\n",
       "29  GENERAL SURGERIES                 8   Umbilical    175"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# owners[owners['OwnerID'] == 7581]\n",
    "# pets[pets['OwnerID'] == 7581] # PetID: X0-8765\n",
    "# procedure_history[procedure_history['PetID'] == 'X0-8765'] # ProcedureType: VACCINATIONS & GENERAL SURGERIES, ProcedureSubCode: 5 & 8\n",
    "procedure_detail[(procedure_detail['ProcedureType'] == 'GENERAL SURGERIES') & (procedure_detail['ProcedureSubCode'] == 8)] # 175\n",
    "# procedure_detail[(procedure_detail['ProcedureType'] == 'VACCINATIONS') & (procedure_detail['ProcedureSubCode'] == 5)] # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done!\n",
    "\n",
    "* Submit the lab on Gradescope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
