{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80: Lab 07\n",
    "\n",
    "### Due Date: Tuesday, February 25 11:59PM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "Much like in DSC 10, this Jupyter Notebook contains the statements of the problems and provides code and markdown cells to display your answers to the problems. Unlike DSC 10, the notebook is *only* for displaying a readable version of your final answers. The coding work will be developed in an accompanying `lab*.py` file, that will be imported into the current notebook.\n",
    "\n",
    "Labs and programming assignments will be graded in (at most) two ways:\n",
    "1. The functions and classes in the accompanying python file will be tested (a la DSC 20),\n",
    "2. The notebook will be graded (for graphs and free response questions).\n",
    "\n",
    "**Do not change the function names in the `*.py` file**\n",
    "- The functions in the `*.py` file are how your assignment is graded, and they are graded by their name. The dictionary at the end of the file (`GRADED FUNCTIONS`) contains the \"grading list\". The final function in the file allows your doctests to check that all the necessary functions exist.\n",
    "- If you changed something you weren't supposed to, just use git to revert!\n",
    "\n",
    "**Tips for working in the Notebook**:\n",
    "- The notebooks serve to present you the questions and give you a place to present your results for later review.\n",
    "- The notebook on *lab assignments* are not graded (only the `.py` file).\n",
    "- Notebooks for PAs will serve as a final report for the assignment, and contain conclusions and answers to open ended questions that are graded.\n",
    "- The notebook serves as a nice environment for 'pre-development' and experimentation before designing your function in your `.py` file.\n",
    "\n",
    "**Tips for developing in the .py file**:\n",
    "- Do not change the function names in the starter code; grading is done using these function names.\n",
    "- Do not change the docstrings in the functions. These are there to tell you if your work is on the right track!\n",
    "- You are encouraged to write your own additional functions to solve the lab! \n",
    "    - Developing in python usually consists of larger files, with many short functions.\n",
    "    - You may write your other functions in an additional `.py` file that you import in `lab**.py` (much like we do in the notebook).\n",
    "- Always document your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing code from `lab**.py`\n",
    "\n",
    "* We import our `.py` file that's contained in the same directory as this notebook.\n",
    "* We use the `autoreload` notebook extension to make changes to our `lab**.py` file immediately available in our notebook. Without this extension, we would need to restart the notebook kernel to see any changes to `lab**.py` in the notebook.\n",
    "    - `autoreload` is necessary because, upon import, `lab**.py` is compiled to bytecode (in the directory `__pycache__`). Subsequent imports of `lab**` merely import the existing compiled python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lab07 as lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice with regular expressions (Regex)\n",
    "\n",
    "**Question 1**\n",
    "\n",
    "You start with some basic regular expression exercises to get some practice using them. You will find function stubs and related doctests in the starter code. \n",
    "\n",
    "**Exercise 1:** A string that has a `[` as the third character and `]` as the sixth character.\n",
    "\n",
    "**Exercise 2:** Phone numbers that start with '(858)' and follow the format '(xxx) xxx-xxxx' (x represents a digit).\n",
    "\n",
    "*Notice: There is a space between (xxx) and xxx-xxxx*\n",
    "\n",
    "**Exercise 3:** A string whose length is between 6 to 10 and contains only word characters, white spaces and `?`. This string must have `?` as its last character.\n",
    "\n",
    "**Exercise 4:** A string that begins with '\\\\$' and with another '\\\\$' within, where:\n",
    "   - Characters between the two '\\\\$' can be anything (including nothing) except the letters 'a', 'b', 'c' (lower case).\n",
    "   - Characters after the second '\\\\$' can only have any number of the letters 'a', 'b', 'c' (upper or lower case), with every 'a' before every 'b', and every 'b' before every 'c'.\n",
    "       - E.g. 'AaBbbC' works, 'ACB' doesn't.\n",
    "\n",
    "**Exercise 5:** A string that represents a valid Python file name including the extension. \n",
    "\n",
    "*Notice*: For simplicity, assume that the file name contains only letters, numbers and an underscore `_`.\n",
    "\n",
    "**Exercise 6:** Find patterns of lowercase letters joined with an underscore.\n",
    "\n",
    "**Exercise 7:** Find patterns that start with and end with a `_`.\n",
    "\n",
    "**Exercise 8:**  Apple registration numbers and Apple hardware product serial numbers might have the number '0' (zero), but never the letter 'O'. Serial numbers don't have the number '1' (one) or the letter 'i'. Write a line of regex expression that checks if the given Serial number belongs to a genuine Apple product.\n",
    "\n",
    "**Exercise 9:** Check if a given ID number is from Los Angeles (LAX), San Diego(SAN) or the state of New York (NY). ID numbers have the following format `SC-NN-CCC-NNNN`. \n",
    "   - SC represents state code in uppercase \n",
    "   - NN represents a number with 2 digits \n",
    "   - CCC represents a three letter city code in uppercase\n",
    "   - NNNN represents a number with 4 digits\n",
    "\n",
    "**Exercise 10:**  Given an input string, cast it to lower case, remove spaces/punctuation, and return a list of every 3-character substring that satisfy the following:\n",
    "   - The first character doesn't start with 'a' or 'A'\n",
    "   - The last substring (and only the last substring) can be shorter than 3 characters, depending on the length of the input string.\n",
    "   - The substrings cannot overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1\n",
    "def match_1(string):\n",
    "    \"\"\"\n",
    "    A string that has a [ as the third character and ] as the sixth character.\n",
    "    >>> match_1(\"abcde]\")\n",
    "    False\n",
    "    >>> match_1(\"ab[cde\")\n",
    "    False\n",
    "    >>> match_1(\"a[cd]\")\n",
    "    False\n",
    "    >>> match_1(\"ab[cd]\")\n",
    "    True\n",
    "    >>> match_1(\"1ab[cd]\")\n",
    "    False\n",
    "    >>> match_1(\"ab[cd]ef\")\n",
    "    True\n",
    "    >>> match_1(\"1b[#d] _\")\n",
    "    True\n",
    "    \"\"\"\n",
    "    #Your Code Here\n",
    "    pattern = '^.{2}\\[.{2}\\]'\n",
    "\n",
    "    #Do not edit following code\n",
    "    prog = re.compile(pattern)\n",
    "    return prog.search(string) is not None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_1(\"abcde]\") == False\n",
    "# match_1(\"ab[cde\") == False\n",
    "# match_1(\"a[cd]\") == False\n",
    "# match_1(\"ab[cd]\") == True\n",
    "# match_1(\"1ab[cd]\") == False\n",
    "# match_1(\"ab[cd]ef\") == True\n",
    "# match_1(\"1b[#d] _\") == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2\n",
    "def match_2(string):\n",
    "    \"\"\"\n",
    "    Phone numbers that start with '(858)' and\n",
    "    follow the format '(xxx) xxx-xxxx' (x represents a digit)\n",
    "    Notice: There is a space between (xxx) and xxx-xxxx\n",
    "\n",
    "    >>> match_2(\"(123) 456-7890\")\n",
    "    False\n",
    "    >>> match_2(\"858-456-7890\")\n",
    "    False\n",
    "    >>> match_2(\"(858)45-7890\")\n",
    "    False\n",
    "    >>> match_2(\"(858) 456-7890\")\n",
    "    True\n",
    "    >>> match_2(\"(858)456-789\")\n",
    "    False\n",
    "    >>> match_2(\"(858)456-7890\")\n",
    "    False\n",
    "    >>> match_2(\"a(858) 456-7890\")\n",
    "    False\n",
    "    >>> match_2(\"(858) 456-7890b\")\n",
    "    False\n",
    "    \"\"\"\n",
    "    #Your Code Here\n",
    "    pattern = '^\\(858\\) \\d{3}-\\d{4}$'\n",
    "\n",
    "    #Do not edit following code\n",
    "    prog = re.compile(pattern)\n",
    "    return prog.search(string) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_2(\"(123) 456-7890\") == False\n",
    "# match_2(\"858-456-7890\") == False\n",
    "# match_2(\"(858)45-7890\") == False\n",
    "# match_2(\"(858) 456-7890\") == True\n",
    "# match_2(\"(858)456-789\") == False\n",
    "# match_2(\"(858)456-7890\") == False\n",
    "# match_2(\"a(858) 456-7890\") == False\n",
    "# match_2(\"(858) 456-7890b\") == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3\n",
    "def match_3(string):\n",
    "    \"\"\"\n",
    "    Find a pattern whose length is between 6 to 10\n",
    "    and contains only word character, white space and ?.\n",
    "    This string must have ? as its last character.\n",
    "\n",
    "    >>> match_3(\"qwertsd?\")\n",
    "    True\n",
    "    >>> match_3(\"qw?ertsd?\")\n",
    "    True\n",
    "    >>> match_3(\"ab c?\")\n",
    "    False\n",
    "    >>> match_3(\"ab   c ?\")\n",
    "    True\n",
    "    >>> match_3(\" asdfqwes ?\")\n",
    "    False\n",
    "    >>> match_3(\" adfqwes ?\")\n",
    "    True\n",
    "    >>> match_3(\" adf!qes ?\")\n",
    "    False\n",
    "    >>> match_3(\" adf!qe? \")\n",
    "    False\n",
    "    \"\"\"\n",
    "    #Your Code Here\n",
    "\n",
    "    pattern = '^[\\w ?]{5,9}\\?$'\n",
    "\n",
    "    #Do not edit following code\n",
    "    prog = re.compile(pattern)\n",
    "    return prog.search(string) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_3(\"qwertsd?\") == True\n",
    "# match_3(\"qw?ertsd?\") == True\n",
    "# match_3(\"ab c?\") == False\n",
    "# match_3(\"ab   c ?\") == True\n",
    "# match_3(\" asdfqwes ?\") == False\n",
    "# match_3(\" adfqwes ?\") == True\n",
    "# match_3(\" adf!qes ?\") == False\n",
    "# match_3(\" adf!qe? \") == False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4\n",
    "def match_4(string):\n",
    "    \"\"\"\n",
    "    A string that begins with '$' and with another '$' within, where:\n",
    "        - Characters between the two '$' can be anything except the \n",
    "        letters 'a', 'b', 'c' (lower case).\n",
    "        - Characters after the second '$' can only have any number \n",
    "        of the letters 'a', 'b', 'c' (upper or lower case), with every \n",
    "        'a' before every 'b', and every 'b' before every 'c'.\n",
    "            - E.g. 'AaBbbC' works, 'ACB' doesn't.\n",
    "\n",
    "    >>> match_4(\"$$AaaaaBbbbc\")\n",
    "    True\n",
    "    >>> match_4(\"$!@#$aABc\")\n",
    "    True\n",
    "    >>> match_4(\"$a$aABc\")\n",
    "    False\n",
    "\n",
    "    >>> match_4(\"$iiuABc\")\n",
    "    False\n",
    "    >>> match_4(\"123$Abc\")\n",
    "    False\n",
    "    >>> match_4(\"$$Abc\")\n",
    "    True\n",
    "    >>> match_4(\"$qw345t$AAAc\")\n",
    "    False\n",
    "    >>> match_4(\"$s$Bca\")\n",
    "    False\n",
    "    \"\"\"\n",
    "    #Your Code Here\n",
    "    pattern = '^\\$[^abc]*\\$[Aa]+[Bb]+[Cc]+'\n",
    "\n",
    "    #Do not edit following code\n",
    "    prog = re.compile(pattern)\n",
    "    return prog.search(string) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_4(\"$$AaaaaBbbbc\") == True\n",
    "# match_4(\"$!@#$aABc\") == True\n",
    "# match_4(\"$a$aABc\") == False\n",
    "# match_4(\"$iiuABc\") == False\n",
    "# match_4(\"123$Abc\") == False\n",
    "# match_4(\"$$Abc\") == True\n",
    "# match_4(\"$qw345t$AAAc\") == False\n",
    "# match_4(\"$s$Bca\") == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5\n",
    "def match_5(string):\n",
    "    \"\"\"\n",
    "    A string that represents a valid Python file name including the extension.\n",
    "    *Notice*: For simplicity, assume that the file name contains only letters, numbers and an underscore `_`.\n",
    "\n",
    "    >>> match_5(\"dsc80.py\")\n",
    "    True\n",
    "    >>> match_5(\"dsc80py\")\n",
    "    False\n",
    "    >>> match_5(\"dsc80..py\")\n",
    "    False\n",
    "    >>> match_5(\"dsc80+.py\")\n",
    "    False\n",
    "    \"\"\"\n",
    "\n",
    "    #Your Code Here\n",
    "    pattern = '^[\\w]+\\.py$'\n",
    "\n",
    "    #Do not edit following code\n",
    "    prog = re.compile(pattern)\n",
    "    return prog.search(string) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_5(\"dsc80.py\") == True\n",
    "# match_5(\"dsc80py\") == False\n",
    "# match_5(\"dsc80..py\") == False\n",
    "# match_5(\"dsc80+.py\") == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6\n",
    "def match_6(string):\n",
    "    \"\"\"\n",
    "    Find patterns of lowercase letters joined with an underscore.\n",
    "    >>> match_6(\"aab_cbb_bc\")\n",
    "    False\n",
    "    >>> match_6(\"aab_cbbbc\")\n",
    "    True\n",
    "    >>> match_6(\"aab_Abbbc\")\n",
    "    False\n",
    "    >>> match_6(\"abcdef\")\n",
    "    False\n",
    "    >>> match_6(\"ABCDEF_ABCD\")\n",
    "    False\n",
    "    \"\"\"\n",
    "\n",
    "    #Your Code Here\n",
    "    pattern = '^[a-z]+\\_[a-z]+$'\n",
    "\n",
    "    #Do not edit following code\n",
    "    prog = re.compile(pattern)\n",
    "    return prog.search(string) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_6(\"aab_cbb_bc\") == False\n",
    "# match_6(\"aab_cbbbc\") == True\n",
    "# match_6(\"aab_Abbbc\") == False\n",
    "# match_6(\"abcdef\") == False\n",
    "# match_6(\"ABCDEF_ABCD\") == False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7\n",
    "def match_7(string):\n",
    "    \"\"\"\n",
    "    Find patterns that start with and end with a _\n",
    "    >>> match_7(\"_abc_\")\n",
    "    True\n",
    "    >>> match_7(\"abd\")\n",
    "    False\n",
    "    >>> match_7(\"bcd\")\n",
    "    False\n",
    "    >>> match_7(\"_ncde\")\n",
    "    False\n",
    "    \"\"\"\n",
    "    \n",
    "    #Your Code Here\n",
    "    pattern = '^\\_.*\\_$'\n",
    "\n",
    "    #Do not edit following code\n",
    "    prog = re.compile(pattern)\n",
    "    return prog.search(string) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_7(\"_abc_\") == True\n",
    "# match_7(\"abd\") == False\n",
    "# match_7(\"bcd\") == False\n",
    "# match_7(\"_ncde\") == False\n",
    "# match_7(\"__\") == True\n",
    "# match_7(\"_\") == False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8\n",
    "def match_8(string):\n",
    "    \"\"\"\n",
    "    Apple registration numbers and Apple hardware product serial numbers\n",
    "    might have the number \"0\" (zero), but never the letter \"O\".\n",
    "    Serial numbers don't have the number \"1\" (one) or the letter \"i\".\n",
    "\n",
    "    Write a line of regex expression that checks\n",
    "    if the given Serial number belongs to a genuine Apple product.\n",
    "\n",
    "    >>> match_8(\"ASJDKLFK10ASDO\")\n",
    "    False\n",
    "    >>> match_8(\"ASJDKLFK0ASDo\")\n",
    "    True\n",
    "    >>> match_8(\"JKLSDNM01IDKSL\")\n",
    "    False\n",
    "    >>> match_8(\"ASDKJLdsi0SKLl\")\n",
    "    False\n",
    "    >>> match_8(\"ASDJKL9380JKAL\")\n",
    "    True\n",
    "    \"\"\"\n",
    "\n",
    "    #Your Code Here\n",
    "    pattern = '^[^(O1i)]*$'\n",
    "\n",
    "    #Do not edit following code\n",
    "    prog = re.compile(pattern)\n",
    "    return prog.search(string) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_8(\"ASJDKLFK10ASDO\") == False\n",
    "# match_8(\"ASJDKLFK0ASDo\") == True\n",
    "# match_8(\"JKLSDNM01IDKSL\") == False\n",
    "# match_8(\"ASDKJLdsi0SKLl\") == False\n",
    "# match_8(\"ASDJKL9380JKAL\") == True\n",
    "# match_8(\"ASDJKL9380JKALIIIo000\") == True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9\n",
    "def match_9(string):\n",
    "    \"\"\"\n",
    "    Check if a given ID number is from Los Angeles (LAX), San Diego(SAN) or\n",
    "    the state of New York (NY). ID numbers have the following format SC-NN-CCC-NNNN.\n",
    "        - SC represents state code in uppercase\n",
    "        - NN represents a number with 2 digits\n",
    "        - CCC represents a three letter city code in uppercase\n",
    "        - NNNN represents a number with 4 digits\n",
    "    \n",
    "    >>> match_9('NY-32-NYC-1232')\n",
    "    True\n",
    "    >>> match_9('ca-23-SAN-1231')\n",
    "    False\n",
    "    >>> match_9('MA-36-BOS-5465')\n",
    "    False\n",
    "    >>> match_9('CA-56-LAX-7895')\n",
    "    True\n",
    "    \"\"\"\n",
    "\n",
    "    #Your Code Here\n",
    "    pattern = '(^CA-[0-9]{2}-((LAX)|(SAN))-[0-9]{4}$)|(^NY-[0-9]{2}-(NYC)-[0-9]{4}$)'\n",
    "\n",
    "    #Do not edit following code\n",
    "    prog = re.compile(pattern)\n",
    "    return prog.search(string) is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_9('NY-32-NYC-1232') == True\n",
    "# match_9('ca-23-SAN-1231') == False\n",
    "# match_9('MA-36-BOS-5465') == False# \n",
    "# match_9('CA-56-LAX-7895') == True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 10\n",
    "def match_10(string):\n",
    "    \"\"\"\n",
    "    Given an input string, cast it to lower case, remove spaces/punctuations, \n",
    "    and return a list of every 3-character substring that satisfy the following:\n",
    "        - The first character doesn't start with 'a' or 'A'\n",
    "        - The last substring (and only the last substring) can be shorter than \n",
    "        3 characters, depending on the length of the input string.\n",
    "        - The substrings cannot overlap\n",
    "    \n",
    "    >>> match_10('ABCdef')\n",
    "    ['def']\n",
    "    >>> match_10(' DEFaabc !g ')\n",
    "    ['def', 'cg']\n",
    "    >>> match_10('Come ti chiami?')\n",
    "    ['com', 'eti', 'chi']\n",
    "    >>> match_10('and')\n",
    "    []\n",
    "    >>> match_10( \"Ab..DEF\")\n",
    "    ['def']\n",
    "    \"\"\"\n",
    "    no_space = string.replace(' ', '') # Remove space\n",
    "    lower_case = no_space.lower() # Cast to lower space\n",
    "    \n",
    "    # If the length of the string is 10, are we disregarding the last character or not?\n",
    "    # Example: string = 'ade$ffr% *', are we ignoring * in this first step?\n",
    "    # sequence = re.findall('..?.?', lower_case) # Find all three-character sequence\n",
    "    \n",
    "    # not_a = []\n",
    "    # for se in sequence: # Remove the ones start with a\n",
    "        # if se[0] != 'a':\n",
    "            # not_a.append(se)\n",
    "    \n",
    "    # complete = ''.join(not_a) # Join back into string\n",
    "    # punc = r'[\\!\\\"\\#\\$\\%\\&\\\\\\'\\(\\)\\*\\+\\,\\-\\.\\/\\:\\;\\<\\=\\>\\?\\@\\[\\]\\^\\_\\`\\{\\|\\}\\~]' # Punctuations\n",
    "    complete = re.sub(r'a.{2}', '', lower_case)\n",
    "    no_punc = re.sub(r'[^A-Za-z0-9 ]', '', complete)\n",
    "    com_sequence = re.findall(r'..?.?', no_punc) # Find complete three-character sequence\n",
    "    return com_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_10('ABCdef') == ['def']\n",
    "# match_10(' DEFaabc !g ') == ['def', 'cg']\n",
    "# match_10('Come ti chiami?') == ['com', 'eti', 'chi']\n",
    "# match_10('and') == []\n",
    "# match_10( \"Ab..DEF\") == ['def']\n",
    "# match_10( \"Ab..AEF\")  == [] # Does it mean it would return this?????????? ASK!!!!!!!!!!!!\n",
    "# Check!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex groups: extracting personal information from messy data\n",
    "\n",
    "**Question 2**\n",
    "\n",
    "The file in `data/messy.txt` contains personal information from a fictional website that a user scraped from webserver logs. Within this dataset, there are four fields that interest you:\n",
    "1. Email Addresses (assume they are alphanumeric user-names and domain-names),\n",
    "2. [Social Security Numbers](https://en.wikipedia.org/wiki/Social_Security_number#Structure)\n",
    "3. Bitcoin Addresses (alpha-numeric strings of long length)\n",
    "4. Street Addresses\n",
    "\n",
    "Create a function `extract_personal` that takes in a string like `open('data/messy.txt').read()` and returns a tuple of four separate lists containing values of the 4 pieces of information listed above (in the order given). Do **not** keep empty values.\n",
    "\n",
    "*Hint*: There are multiple \"delimiters\" in use in the file; there are few enough of them that you can safely determine what they are.\n",
    "\n",
    "*Note:* Since this data is messy/corrupted, your function will be allowed to miss ~5% of the records in each list. Good spot checking using certain useful substrings (e.g. `@` for emails) should help assure correctness! Your function will be tested on a sample of the file `messy.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'messy.txt')\n",
    "s = open(fp, encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\t4/12/2018\\tLorem ipsum dolor sit amet, consectetuer adipiscing elit. Proin risus. Praesent lectus.\\n\\nVestibulum quam sapien| varius ut, blandit non, interdum in, ante. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Duis faucibus accumsan odio. Curabitur convallis.|dottewell0@gnu.org\\toR1mOq,!@#$%^&*(),[{bitcoin:18A8rBU3wvbLTSxMjqrPNc9mvonpA4XMiv\\tIP:192.232.9.210\\tccn:3563354617955160|ssn:380-09-9403}]|05-6609813,814 Monterey Court\\n2\\t12/18/2018\\tSuspendisse potenti. In eleifend quam a odio. In hac habitasse platea dictumst.\\n\\nMaecenas ut massa quis augue luctus tincidunt. Nulla mollis molestie lorem. Quisque ut erat.,bassiter1@sphinn.com\\tc5KvmarHX3o,test\\u2060test\\u202b,[{bitcoin:1EB7kYpnfJSqS7kUFpinsmPF3uiH9sfRf1,IP:20.73.13.197|ccn:3542723823957010\\tssn:118-12-8276}#{bitcoin:1E5fev4boabWZmXvHGVkHcNJZ2tLnpM6Zv*IP:238.206.212.148\\tccn:337941898369615,ssn:427-22-9352}#{bitcoin:1DqG3WcmGw74PjptjzcAmxGFuQdvWL7RCC,IP:171.241.15.98\\tccn:3574672962323693,ssn:649-16-224'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = s[:1000]\n",
    "test\n",
    "# test = '#kdumphreyh@hc360.com|dtr04K,(╯°□°）╯︵ ┻━┻)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to actually split????????????????? ASK!!!!!!!!!!!!!!!!\n",
    "# re.split(r'\\t|\\n', test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are some valid domain names? Would upper case be involved???? .com???.edu??? ASK!!!!!!!!!!!!!!!!!\n",
    "email_pat = r'([A-Za-z0-9]+@[A-Za-z0-9]+(\\.[A-Za-z0-9]+)+)'\n",
    "# Exception emails like egristonr7@pagesperso-orange.fr, jpitcaithleyre@t-online.de\n",
    "\n",
    "# ssn_pat = '(?!(000)|(666))[0-8][0-9]{2}-(?!(00))[0-9]{2}-(?!(0000))[0-9]{4}' # 423-00-9575 not match this, 00 for second group??????????????? ASK!!!!!!!!!\n",
    "ssn_pat = '((?!(000)|(666))[0-8][0-9]{2}-[0-9]{2}-(?!(0000))[0-9]{4})'\n",
    "\n",
    "# Online, something about the length of bitcoin, and not including O,I,l?????????? Need to consider this?\n",
    "bit_pat = '[13][A-Za-z0-9]{26,33}' \n",
    "\n",
    "add_pat = '(\\d+( [A-z]+)+)' # Too simple? What about St. Amerbse street?? Or things like this\n",
    "# Some weird address such as 0 Veith Drive, 04 Westport Lane, 04232 Monterey Circle?????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# email_group = re.findall(email_pat, s)\n",
    "# emails = [group[0] for group in email_group]\n",
    "# len(emails) # 938 results\n",
    "# Search for @ only, has 963 results, within 5%\n",
    "\n",
    "# ssn_group = re.findall('[0-9]{3}-[0-9]{2}-[0-9]{4}', s)\n",
    "# ssn_group = re.findall(ssn_pat, s) # No difference???? Why bother??? hhh\n",
    "# ssns = [group[0] for group in ssn_group]\n",
    "# len(ssns) # 2571\n",
    "# Search for ssn, has 2858 results, null results has 286, n = 2572, within 5%\n",
    "\n",
    "# bits = re.findall(bit_pat, s)\n",
    "# len(bits) # 2781\n",
    "# Search for bitcoin, has 2857 results, null results has 76, n = 2781 (exact)\n",
    "\n",
    "# adds = re.findall(add_pat, s)\n",
    "# len(adds) # 2781\n",
    "# [group[0] for group in adds]\n",
    "# Seems to make sense?? But when bitcoin is null, address does not necessary has to be null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.findall('\\d+ [A-z]+ [A-z]+', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_personal(s):\n",
    "    \"\"\"\n",
    "    :Example:\n",
    "    >>> fp = os.path.join('data', 'messy.test.txt')\n",
    "    >>> s = open(fp, encoding='utf8').read()\n",
    "    >>> emails, ssn, bitcoin, addresses = extract_personal(s)\n",
    "    >>> emails[0] == 'test@test.com'\n",
    "    True\n",
    "    >>> ssn[0] == '423-00-9575'\n",
    "    True\n",
    "    >>> bitcoin[0] == '1BvBMSEYstWetqTFn5Au4m4GFg7xJaNVN2'\n",
    "    True\n",
    "    >>> addresses[0] == '530 High Street'\n",
    "    True\n",
    "    \"\"\"\n",
    "    # Pattern of each category\n",
    "    email_pat = r'([A-Za-z0-9]+@[A-Za-z0-9]+(\\.[A-Za-z0-9]+)+)'\n",
    "    ssn_pat = r'[0-9]{3}-[0-9]{2}-[0-9]{4}'\n",
    "    bit_pat = r'[13][A-Za-z0-9]{26,33}' \n",
    "    add_pat = r'(\\d+( [A-z]+)+)' \n",
    "\n",
    "    # Get email address\n",
    "    email_group = re.findall(email_pat, s)\n",
    "    emails = [group[0] for group in email_group]\n",
    "\n",
    "    # Get social security number\n",
    "    ssns = re.findall(ssn_pat, s) \n",
    "    # ssns = [group[0] for group in ssn_group]\n",
    "\n",
    "    # Get bitcoin address\n",
    "    bits = re.findall(bit_pat, s)\n",
    "\n",
    "    # Get address\n",
    "    adds_group = re.findall(add_pat, s)\n",
    "    adds = [group[0] for group in adds_group]\n",
    "    return (emails, ssns, bits, adds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails, ssns, bits, adds = extract_personal(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(938, 2571, 2781, 942)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emails), len(ssns), len(bits), len(adds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emails\n",
    "# ssns\n",
    "# bits[0]\n",
    "# adds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content in Amazon review data\n",
    "\n",
    "**Question 3**\n",
    "\n",
    "The dataset `reviews.txt` contains [Amazon reviews](http://jmcauley.ucsd.edu/data/amazon/) for ~200k phones and phone accessories. This dataset has been \"cleaned\" for you. The goal of this section is to create a function that takes in the review dataset and a review and returns the word that \"best summarizes the review\" using TF-IDF.'\n",
    "\n",
    "1. Create a function `tfidf_data(review, reviews)` that takes a review as well as the review data and returns a dataframe:\n",
    "    - indexed by the words in `review`,\n",
    "    - with columns given by (a) the number of times each word is found in the review (`cnt`), (b) the term frequency for each word (`tf`), (c) the inverse document frequency for each word (`idf`), and (d) the TF-IDF for each word (`tfidf`).\n",
    "    \n",
    "2. Create a function `relevant_word(tfidf_data)` which takes in a dataframe as above and returns the word that \"best summarizes the review\" described by `tfidf_data`.\n",
    "\n",
    "\n",
    "*Note:* Use this function to \"cluster\" review types -- run it on a sample of reviews and see which words come up most. Unfortunately, you will likely have to change your code from your answer above to run it on the entire dataset (to do this, you should compute as many of the frequencies \"ahead of time\" and look them up when needed; you should also likely filter out words that occur \"rarely\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASK! (not so sure what this is doing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small dataset testing\n",
    "fp = os.path.join('data', 'tests.txt')\n",
    "reviews = pd.read_csv(fp, header=None, squeeze=True)\n",
    "review = open(os.path.join('data', 'review.txt'), encoding='utf8').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_pat = '\\\\b%s\\\\b' % 'this'\n",
    "review.count(re_pat)\n",
    "#'\\\\b%s\\\\b' % 'this'\n",
    "temp = 'this hahathisthaha thirtthisth thisthit'\n",
    "temp.count('this ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer to check answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = pd.Series(review.split()).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    return tfDict\n",
    "\n",
    "def computeIDF(documents):\n",
    "    import math\n",
    "    N = len(documents)\n",
    "    \n",
    "    idfDict = dict.fromkeys(documents[0].keys(), 0)\n",
    "    for document in documents:\n",
    "        for word, val in document.items():\n",
    "            if word in idfDict.keys():\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'this': 3,\n",
       "  'is': 3,\n",
       "  'a': 4,\n",
       "  'great': 1,\n",
       "  'new': 1,\n",
       "  'case': 3,\n",
       "  'design': 1,\n",
       "  'that': 2,\n",
       "  'i': 1,\n",
       "  'have': 1,\n",
       "  'not': 1,\n",
       "  'seen': 1,\n",
       "  'before': 1,\n",
       "  'it': 3,\n",
       "  'has': 1,\n",
       "  'slim': 1,\n",
       "  'silicone': 1,\n",
       "  'skin': 1,\n",
       "  'really': 1,\n",
       "  'locks': 1,\n",
       "  'in': 1,\n",
       "  'the': 3,\n",
       "  'phone': 2,\n",
       "  'to': 3,\n",
       "  'cover': 2,\n",
       "  'and': 5,\n",
       "  'protect': 1,\n",
       "  'your': 1,\n",
       "  'from': 1,\n",
       "  'spills': 1,\n",
       "  'such': 1,\n",
       "  'also': 2,\n",
       "  'hard': 1,\n",
       "  'polycarbonate': 1,\n",
       "  'outside': 1,\n",
       "  'shell': 1,\n",
       "  'guard': 1,\n",
       "  'against': 1,\n",
       "  'damage': 1,\n",
       "  'comes': 1,\n",
       "  'with': 1,\n",
       "  'different': 2,\n",
       "  'interchangeable': 1,\n",
       "  'skins': 1,\n",
       "  'covers': 1,\n",
       "  'create': 1,\n",
       "  'multiple': 1,\n",
       "  'color': 1,\n",
       "  'combinations': 1,\n",
       "  'kind': 1,\n",
       "  'of': 2,\n",
       "  'than': 1,\n",
       "  'usual': 1,\n",
       "  'chunk': 1,\n",
       "  'plastic': 1,\n",
       "  'innovative': 1,\n",
       "  'suits': 1,\n",
       "  'iphone': 1,\n",
       "  '5': 1,\n",
       "  'perfectly': 1},\n",
       " {'i': 2,\n",
       "  'brought': 1,\n",
       "  'this': 1,\n",
       "  'for': 2,\n",
       "  'my': 1,\n",
       "  'sister': 1,\n",
       "  'who': 1,\n",
       "  'has': 1,\n",
       "  'a': 1,\n",
       "  'g2': 1,\n",
       "  'but': 2,\n",
       "  'not': 1,\n",
       "  'sure': 1,\n",
       "  'how': 1,\n",
       "  'it': 3,\n",
       "  'works': 1,\n",
       "  'yet': 1,\n",
       "  'will': 1,\n",
       "  'let': 1,\n",
       "  'you': 2,\n",
       "  'know': 1,\n",
       "  'as': 2,\n",
       "  'soon': 1,\n",
       "  'update': 1,\n",
       "  'me': 1,\n",
       "  'updateit': 1,\n",
       "  'was': 1,\n",
       "  'too': 1,\n",
       "  'small': 1,\n",
       "  'her': 2,\n",
       "  'phone': 1,\n",
       "  'per': 1,\n",
       "  'the': 1,\n",
       "  'description': 1,\n",
       "  'should': 1,\n",
       "  'fit': 1,\n",
       "  't': 1,\n",
       "  'mobile': 1,\n",
       "  'galaxy': 1,\n",
       "  \"didn't\": 2,\n",
       "  'bother': 1,\n",
       "  'to': 1,\n",
       "  'mail': 1,\n",
       "  'back': 1},\n",
       " {'i': 20,\n",
       "  'am': 3,\n",
       "  'both': 4,\n",
       "  'delighted': 1,\n",
       "  'and': 27,\n",
       "  'disappointed': 1,\n",
       "  'with': 13,\n",
       "  'dell': 6,\n",
       "  'venue': 10,\n",
       "  'pro': 11,\n",
       "  'windows': 11,\n",
       "  'phone': 32,\n",
       "  'is': 60,\n",
       "  'the': 93,\n",
       "  'operating': 1,\n",
       "  'system': 1,\n",
       "  'of': 45,\n",
       "  'my': 7,\n",
       "  'choice': 1,\n",
       "  'primarily': 1,\n",
       "  'due': 2,\n",
       "  'to': 41,\n",
       "  'its': 4,\n",
       "  'productivity': 3,\n",
       "  'very': 13,\n",
       "  'much': 6,\n",
       "  'businesslike': 1,\n",
       "  'precisely': 2,\n",
       "  'type': 2,\n",
       "  'mobile': 12,\n",
       "  'want': 2,\n",
       "  'however': 2,\n",
       "  'terrible': 3,\n",
       "  'power': 5,\n",
       "  'management': 5,\n",
       "  'makes': 2,\n",
       "  'it': 36,\n",
       "  'virtually': 1,\n",
       "  'unbearable': 1,\n",
       "  'me': 5,\n",
       "  'let': 1,\n",
       "  'give': 1,\n",
       "  'ratings': 1,\n",
       "  'below': 1,\n",
       "  'first': 1,\n",
       "  'on': 13,\n",
       "  'a': 59,\n",
       "  'scale': 1,\n",
       "  '10': 1,\n",
       "  'numbers': 1,\n",
       "  'in': 16,\n",
       "  'parenthesis': 1,\n",
       "  'for': 14,\n",
       "  'iphone': 9,\n",
       "  '4': 5,\n",
       "  'as': 10,\n",
       "  'reference': 1,\n",
       "  'thermal': 3,\n",
       "  'performance': 3,\n",
       "  '2': 5,\n",
       "  '8': 3,\n",
       "  'battery': 5,\n",
       "  'life': 4,\n",
       "  '7': 4,\n",
       "  'build': 2,\n",
       "  'quality': 3,\n",
       "  '9': 15,\n",
       "  '5': 12,\n",
       "  'screen': 8,\n",
       "  'processor': 3,\n",
       "  'meaningless': 1,\n",
       "  'comparison': 1,\n",
       "  'but': 21,\n",
       "  'see': 4,\n",
       "  'speed': 2,\n",
       "  'efficiency': 3,\n",
       "  'os': 11,\n",
       "  'reliability': 1,\n",
       "  'fluency': 1,\n",
       "  '0': 2,\n",
       "  'feature': 3,\n",
       "  'depth': 2,\n",
       "  '6': 1,\n",
       "  'flexibility': 2,\n",
       "  'customization': 2,\n",
       "  'app': 3,\n",
       "  'ecosystem': 1,\n",
       "  'form': 1,\n",
       "  'factor': 1,\n",
       "  'appearance': 1,\n",
       "  'completely': 1,\n",
       "  'subjectiveentertainment': 1,\n",
       "  'yes': 1,\n",
       "  'retina': 2,\n",
       "  'has': 5,\n",
       "  'higher': 1,\n",
       "  'resolution': 1,\n",
       "  'looks': 1,\n",
       "  'slightly': 1,\n",
       "  'better': 5,\n",
       "  'under': 2,\n",
       "  'sunlight': 2,\n",
       "  'practice': 3,\n",
       "  \"it's\": 3,\n",
       "  'wash': 1,\n",
       "  'you': 16,\n",
       "  'can': 10,\n",
       "  'above': 1,\n",
       "  'even': 14,\n",
       "  'manages': 1,\n",
       "  'beat': 1,\n",
       "  'few': 5,\n",
       "  'important': 1,\n",
       "  'areas': 1,\n",
       "  'at': 7,\n",
       "  'end': 4,\n",
       "  'day': 7,\n",
       "  'just': 11,\n",
       "  'such': 5,\n",
       "  'miserable': 2,\n",
       "  'device': 4,\n",
       "  'otherwise': 2,\n",
       "  'capable': 2,\n",
       "  'which': 9,\n",
       "  'includes': 1,\n",
       "  'so': 9,\n",
       "  'really': 4,\n",
       "  'deserves': 1,\n",
       "  'failing': 2,\n",
       "  'grade': 1,\n",
       "  'pity': 1,\n",
       "  'because': 7,\n",
       "  'this': 14,\n",
       "  'would': 3,\n",
       "  'have': 6,\n",
       "  'been': 1,\n",
       "  'quite': 4,\n",
       "  'attractive': 1,\n",
       "  'deserving': 1,\n",
       "  'flunking': 1,\n",
       "  'gradefirst': 1,\n",
       "  'runs': 1,\n",
       "  'warm': 1,\n",
       "  'your': 2,\n",
       "  'hands': 2,\n",
       "  'uncomfortable': 2,\n",
       "  'after': 2,\n",
       "  'minutes': 1,\n",
       "  'running': 1,\n",
       "  'apps': 1,\n",
       "  'some': 3,\n",
       "  'email': 1,\n",
       "  'checking': 1,\n",
       "  'or': 9,\n",
       "  'web': 1,\n",
       "  'browsing': 2,\n",
       "  'not': 20,\n",
       "  'only': 4,\n",
       "  'could': 3,\n",
       "  'actually': 6,\n",
       "  'be': 8,\n",
       "  'health': 1,\n",
       "  'concern': 1,\n",
       "  'know': 1,\n",
       "  'what': 3,\n",
       "  'heat': 2,\n",
       "  'radiation': 1,\n",
       "  'do': 5,\n",
       "  'human': 2,\n",
       "  'body': 4,\n",
       "  'got': 2,\n",
       "  'slowly': 1,\n",
       "  'burnt': 1,\n",
       "  'condition': 1,\n",
       "  'result': 2,\n",
       "  'longtime': 1,\n",
       "  'use': 10,\n",
       "  'tablet': 1,\n",
       "  'computers': 2,\n",
       "  'problem': 6,\n",
       "  'seriously': 1,\n",
       "  'bad': 2,\n",
       "  'that': 19,\n",
       "  'issue': 7,\n",
       "  'alone': 1,\n",
       "  'an': 7,\n",
       "  'extremely': 4,\n",
       "  'undesirable': 1,\n",
       "  'second': 1,\n",
       "  'surprising': 1,\n",
       "  'given': 2,\n",
       "  'will': 2,\n",
       "  'last': 3,\n",
       "  'unless': 1,\n",
       "  \"don't\": 5,\n",
       "  'all': 6,\n",
       "  'support': 1,\n",
       "  'about': 2,\n",
       "  'hours': 2,\n",
       "  'continuous': 1,\n",
       "  'many': 5,\n",
       "  'times': 2,\n",
       "  'whorse': 1,\n",
       "  'than': 7,\n",
       "  'laptop': 3,\n",
       "  'bit': 3,\n",
       "  'over': 2,\n",
       "  '24': 1,\n",
       "  'standby': 1,\n",
       "  'time': 2,\n",
       "  'if': 10,\n",
       "  'again': 1,\n",
       "  'worse': 1,\n",
       "  'any': 4,\n",
       "  'modestly': 1,\n",
       "  'example': 2,\n",
       "  'modest': 1,\n",
       "  'amount': 1,\n",
       "  'e': 1,\n",
       "  'mailing': 1,\n",
       "  'plus': 1,\n",
       "  'short': 1,\n",
       "  'calls': 5,\n",
       "  'exhaust': 1,\n",
       "  'before': 1,\n",
       "  'heavy': 1,\n",
       "  'user': 7,\n",
       "  'still': 4,\n",
       "  'usually': 2,\n",
       "  'up': 1,\n",
       "  'having': 1,\n",
       "  'charge': 1,\n",
       "  'least': 1,\n",
       "  'once': 1,\n",
       "  'when': 3,\n",
       "  \"i'm\": 1,\n",
       "  'out': 3,\n",
       "  'trip': 2,\n",
       "  'twice': 1,\n",
       "  'more': 4,\n",
       "  'making': 1,\n",
       "  'among': 4,\n",
       "  'biggest': 1,\n",
       "  'mental': 1,\n",
       "  'burdens': 1,\n",
       "  'while': 2,\n",
       "  \"you're\": 1,\n",
       "  'traveling': 1,\n",
       "  'simply': 7,\n",
       "  'worth': 1,\n",
       "  'absolutely': 1,\n",
       "  'worst': 1,\n",
       "  'phones': 3,\n",
       "  'used': 1,\n",
       "  'handled': 1,\n",
       "  'unacceptable': 1,\n",
       "  'may': 2,\n",
       "  'too': 5,\n",
       "  'ask': 1,\n",
       "  'week': 1,\n",
       "  'especially': 1,\n",
       "  'smart': 2,\n",
       "  'becoming': 1,\n",
       "  'almost': 4,\n",
       "  'full': 1,\n",
       "  'featured': 1,\n",
       "  'single': 2,\n",
       "  'shortcoming': 1,\n",
       "  'mixed': 3,\n",
       "  'bag': 3,\n",
       "  'wp': 3,\n",
       "  'osi': 1,\n",
       "  'love': 1,\n",
       "  'hate': 2,\n",
       "  'interface': 2,\n",
       "  'intuitive': 1,\n",
       "  'glitch': 1,\n",
       "  'free': 2,\n",
       "  'smooth': 1,\n",
       "  'every': 1,\n",
       "  'good': 4,\n",
       "  'ios': 2,\n",
       "  'android': 4,\n",
       "  'best': 1,\n",
       "  'most': 1,\n",
       "  'productive': 2,\n",
       "  'major': 1,\n",
       "  'platforms': 2,\n",
       "  'are': 10,\n",
       "  'content': 2,\n",
       "  'consumer': 1,\n",
       "  'three': 2,\n",
       "  'shoulder': 2,\n",
       "  'creator': 1,\n",
       "  'far': 2,\n",
       "  'ahead': 1,\n",
       "  'cannot': 3,\n",
       "  'match': 3,\n",
       "  \"microsoft's\": 2,\n",
       "  'office': 3,\n",
       "  '365': 3,\n",
       "  'integrates': 1,\n",
       "  'microsoft': 8,\n",
       "  'perfectly': 1,\n",
       "  'company': 2,\n",
       "  'subscribe': 1,\n",
       "  'cloud': 1,\n",
       "  'versions': 1,\n",
       "  'outlook': 1,\n",
       "  'teamsite': 1,\n",
       "  'sharepoint': 1,\n",
       "  'anything': 1,\n",
       "  'using': 2,\n",
       "  'although': 3,\n",
       "  'certain': 1,\n",
       "  'limitations': 1,\n",
       "  'small': 2,\n",
       "  'getting': 1,\n",
       "  'live': 2,\n",
       "  'skydive': 1,\n",
       "  'underrated': 1,\n",
       "  'already': 1,\n",
       "  'excellent': 2,\n",
       "  'experience': 2,\n",
       "  'another': 1,\n",
       "  'potentially': 2,\n",
       "  'killer': 1,\n",
       "  'released': 1,\n",
       "  'skype': 5,\n",
       "  'beta': 1,\n",
       "  'usable': 1,\n",
       "  'make': 9,\n",
       "  'anywhere': 1,\n",
       "  'world': 1,\n",
       "  'long': 1,\n",
       "  'wi': 1,\n",
       "  'fi': 1,\n",
       "  'connection': 2,\n",
       "  'no': 5,\n",
       "  'cellular': 1,\n",
       "  'required': 1,\n",
       "  'was': 2,\n",
       "  'international': 2,\n",
       "  'other': 2,\n",
       "  'came': 1,\n",
       "  'handy': 1,\n",
       "  'able': 1,\n",
       "  'back': 1,\n",
       "  'home': 3,\n",
       "  'work': 2,\n",
       "  'airports': 1,\n",
       "  'though': 2,\n",
       "  'did': 1,\n",
       "  'sim': 3,\n",
       "  'card': 4,\n",
       "  'local': 2,\n",
       "  'service': 2,\n",
       "  'temporary': 1,\n",
       "  'continued': 1,\n",
       "  '3g': 1,\n",
       "  'data': 1,\n",
       "  'costs': 1,\n",
       "  'two': 1,\n",
       "  'cents': 1,\n",
       "  'minute': 1,\n",
       "  'less': 1,\n",
       "  '1': 2,\n",
       "  '20': 1,\n",
       "  'cost': 1,\n",
       "  'made': 2,\n",
       "  'huge': 1,\n",
       "  'differentiator': 1,\n",
       "  'hope': 1,\n",
       "  'point': 2,\n",
       "  'clear': 1,\n",
       "  'users': 2,\n",
       "  'they': 7,\n",
       "  'own': 1,\n",
       "  'exclusive': 1,\n",
       "  'doing': 1,\n",
       "  'helps': 1,\n",
       "  'success': 1,\n",
       "  \"i'd\": 1,\n",
       "  'fail': 3,\n",
       "  'need': 1,\n",
       "  'something': 1,\n",
       "  'different': 2,\n",
       "  'from': 2,\n",
       "  'lifestyle': 1,\n",
       "  'wp7': 1,\n",
       "  'fully': 1,\n",
       "  'cooked': 1,\n",
       "  'yet': 1,\n",
       "  'latest': 1,\n",
       "  'mango': 1,\n",
       "  'update': 1,\n",
       "  'there': 3,\n",
       "  'name': 1,\n",
       "  \"can't\": 6,\n",
       "  'turn': 2,\n",
       "  'off': 3,\n",
       "  'stupid': 3,\n",
       "  'auto': 3,\n",
       "  'rotation': 3,\n",
       "  'settings': 1,\n",
       "  'does': 4,\n",
       "  'unlocked': 1,\n",
       "  'forgive': 1,\n",
       "  'call': 1,\n",
       "  'one': 3,\n",
       "  'those': 2,\n",
       "  'tech': 1,\n",
       "  'things': 2,\n",
       "  'sense': 1,\n",
       "  'these': 2,\n",
       "  'designers': 1,\n",
       "  'understand': 2,\n",
       "  'proper': 1,\n",
       "  'desired': 2,\n",
       "  'orientation': 3,\n",
       "  'determined': 1,\n",
       "  'by': 1,\n",
       "  'sensor': 4,\n",
       "  'mechanical': 1,\n",
       "  'were': 2,\n",
       "  'sure': 1,\n",
       "  'decide': 1,\n",
       "  'desires': 1,\n",
       "  'based': 1,\n",
       "  'reading': 1,\n",
       "  'relative': 3,\n",
       "  'positions': 3,\n",
       "  \"user's\": 3,\n",
       "  'posture': 2,\n",
       "  'relevant': 1,\n",
       "  'position': 3,\n",
       "  'itself': 4,\n",
       "  'relation': 1,\n",
       "  'determine': 1,\n",
       "  'current': 1,\n",
       "  'detect': 1,\n",
       "  \"phone's\": 1,\n",
       "  'autorotation': 1,\n",
       "  'causes': 1,\n",
       "  'annoyance': 2,\n",
       "  'utility': 1,\n",
       "  'ok': 2,\n",
       "  'gimmick': 1,\n",
       "  'attract': 1,\n",
       "  'shallow': 1,\n",
       "  'counters': 1,\n",
       "  'permanently': 1,\n",
       "  'implemented': 1,\n",
       "  'turned': 2,\n",
       "  'reason': 1,\n",
       "  'why': 2,\n",
       "  'big': 1,\n",
       "  'deal': 1,\n",
       "  'thing': 1,\n",
       "  'threatening': 1,\n",
       "  'failure': 1,\n",
       "  'overlook': 1,\n",
       "  'issues': 1,\n",
       "  'telling': 1,\n",
       "  'level': 4,\n",
       "  'ergonomics': 1,\n",
       "  'maker': 1,\n",
       "  'understands': 1,\n",
       "  'willing': 1,\n",
       "  'effort': 1,\n",
       "  'edit': 1,\n",
       "  'individual': 1,\n",
       "  'fields': 1,\n",
       "  'contact': 1,\n",
       "  'profile': 1,\n",
       "  'choices': 1,\n",
       "  'rigid': 1,\n",
       "  'satisfy': 1,\n",
       "  'ordinary': 1,\n",
       "  'needs': 1,\n",
       "  '3': 1,\n",
       "  'copy': 1,\n",
       "  'paste': 1,\n",
       "  'works': 2,\n",
       "  'well': 2,\n",
       "  'within': 1,\n",
       "  'same': 1,\n",
       "  'application': 1,\n",
       "  'reliably': 1,\n",
       "  'across': 2,\n",
       "  'applications': 2,\n",
       "  'others': 1,\n",
       "  'several': 1,\n",
       "  'details': 4,\n",
       "  'done': 1,\n",
       "  'hard': 4,\n",
       "  'part': 1,\n",
       "  'building': 1,\n",
       "  'promising': 1,\n",
       "  'difficult': 1,\n",
       "  'them': 1,\n",
       "  'basic': 1,\n",
       "  'simple': 1,\n",
       "  'right': 1,\n",
       "  'obviously': 1,\n",
       "  'engineering': 1,\n",
       "  'product': 1,\n",
       "  'culture': 1,\n",
       "  'paying': 2,\n",
       "  'attention': 3,\n",
       "  'geekness': 1,\n",
       "  'loses': 1,\n",
       "  'sides': 1,\n",
       "  'perhaps': 1,\n",
       "  'meant': 1,\n",
       "  'low': 1,\n",
       "  'entry': 1,\n",
       "  'we': 1,\n",
       "  'talking': 1,\n",
       "  'business': 1,\n",
       "  'here': 1,\n",
       "  'hardwarethe': 1,\n",
       "  'hardware': 2,\n",
       "  'addition': 1,\n",
       "  'downright': 1,\n",
       "  'little': 1,\n",
       "  'bulky': 1,\n",
       "  'particularly': 1,\n",
       "  'thick': 1,\n",
       "  'thickness': 1,\n",
       "  'partly': 1,\n",
       "  'including': 1,\n",
       "  'button': 1,\n",
       "  'keyboard': 5,\n",
       "  'thought': 1,\n",
       "  'necessity': 1,\n",
       "  'someone': 1,\n",
       "  'like': 1,\n",
       "  'superfluous': 1,\n",
       "  'ironically': 1,\n",
       "  'soft': 2,\n",
       "  'significant': 1,\n",
       "  'advantages': 1,\n",
       "  'programed': 1,\n",
       "  'extensions': 1,\n",
       "  'personally': 1,\n",
       "  'think': 1,\n",
       "  'failed': 1,\n",
       "  'pay': 1,\n",
       "  'keys': 1,\n",
       "  'illuminated': 1,\n",
       "  'illumination': 1,\n",
       "  'sometimes': 1,\n",
       "  'needed': 1,\n",
       "  'idea': 1,\n",
       "  'thinking': 1,\n",
       "  'clearly': 1,\n",
       "  'neither': 1,\n",
       "  'nor': 1,\n",
       "  'apple': 1,\n",
       "  'comes': 1,\n",
       "  'fine': 1,\n",
       "  'beautiful': 1,\n",
       "  \"4's\": 1,\n",
       "  'sound': 1,\n",
       "  'specification': 1,\n",
       "  'difference': 1,\n",
       "  'high': 1,\n",
       "  'uses': 2,\n",
       "  'chipset': 1,\n",
       "  'years': 1,\n",
       "  'ago': 1,\n",
       "  'slow': 1,\n",
       "  'measure': 1,\n",
       "  \"isn't\": 1,\n",
       "  'aspect': 1,\n",
       "  \"haven't\": 1,\n",
       "  'compared': 1,\n",
       "  '4s': 1,\n",
       "  'faster': 1,\n",
       "  'latter': 1,\n",
       "  'powerful': 1,\n",
       "  'testament': 1,\n",
       "  'summary': 1,\n",
       "  'recommend': 1,\n",
       "  'interested': 1,\n",
       "  'platform': 1,\n",
       "  'go': 1,\n",
       "  'should': 1,\n",
       "  'wait': 1,\n",
       "  'until': 1,\n",
       "  'apollo': 1,\n",
       "  'put': 1,\n",
       "  'top': 1,\n",
       "  'line': 1,\n",
       "  'nokia': 1,\n",
       "  'lumia': 1},\n",
       " {'of': 3,\n",
       "  'all': 1,\n",
       "  'the': 16,\n",
       "  'protectors': 1,\n",
       "  'i': 6,\n",
       "  'had': 2,\n",
       "  'seen': 1,\n",
       "  'for': 3,\n",
       "  'nexus': 1,\n",
       "  '4': 1,\n",
       "  'this': 4,\n",
       "  'was': 1,\n",
       "  'only': 2,\n",
       "  'nice': 1,\n",
       "  'looking': 1,\n",
       "  'one': 2,\n",
       "  'could': 1,\n",
       "  'find': 1,\n",
       "  'that': 1,\n",
       "  'a': 5,\n",
       "  'protector': 2,\n",
       "  'back': 2,\n",
       "  'phone': 4,\n",
       "  'which': 1,\n",
       "  'is': 3,\n",
       "  'glass': 2,\n",
       "  'cover': 1,\n",
       "  'snaps': 1,\n",
       "  'on': 2,\n",
       "  'securely': 1,\n",
       "  'yet': 1,\n",
       "  'still': 1,\n",
       "  'easy': 2,\n",
       "  'to': 3,\n",
       "  'remove': 1,\n",
       "  'if': 1,\n",
       "  'pried': 1,\n",
       "  'correct': 1,\n",
       "  'way': 2,\n",
       "  'have': 2,\n",
       "  'fumbled': 1,\n",
       "  'couple': 1,\n",
       "  'times': 1,\n",
       "  'and': 2,\n",
       "  'bumper': 2,\n",
       "  'has': 1,\n",
       "  'kept': 2,\n",
       "  'it': 3,\n",
       "  'completely': 1,\n",
       "  'safe': 2,\n",
       "  'while': 1,\n",
       "  'hate': 1,\n",
       "  'covering': 1,\n",
       "  'up': 1,\n",
       "  '34': 2,\n",
       "  'prism': 1,\n",
       "  'would': 1,\n",
       "  'rather': 1,\n",
       "  'be': 1,\n",
       "  'with': 2,\n",
       "  'halo': 1,\n",
       "  'screen': 1,\n",
       "  'fairly': 1,\n",
       "  'well': 1,\n",
       "  'covered': 1,\n",
       "  'in': 1,\n",
       "  'protection': 2,\n",
       "  'drawback': 1,\n",
       "  'due': 1,\n",
       "  'plastic': 1,\n",
       "  'protrudes': 1,\n",
       "  'over': 1,\n",
       "  'buttons': 1,\n",
       "  'sticking': 1,\n",
       "  'out': 1,\n",
       "  'somewhat': 1,\n",
       "  'from': 1,\n",
       "  'they': 1,\n",
       "  'are': 1,\n",
       "  'often': 1,\n",
       "  'pressed': 1,\n",
       "  'accidentally': 1,\n",
       "  'minor': 1,\n",
       "  'complaint': 1,\n",
       "  'but': 1,\n",
       "  'little': 1,\n",
       "  'too': 1,\n",
       "  'press': 1,\n",
       "  'them': 1,\n",
       "  'given': 1,\n",
       "  \"i'll\": 1,\n",
       "  'live': 1,\n",
       "  'trade': 1,\n",
       "  'off': 1},\n",
       " {'this': 1,\n",
       "  'screen': 5,\n",
       "  'protector': 3,\n",
       "  'is': 4,\n",
       "  'great': 1,\n",
       "  'for': 2,\n",
       "  'protection': 1,\n",
       "  'ex': 1,\n",
       "  'dropping': 1,\n",
       "  'it': 3,\n",
       "  'on': 2,\n",
       "  'gravel': 1,\n",
       "  'the': 6,\n",
       "  'only': 1,\n",
       "  'cons': 1,\n",
       "  'i': 2,\n",
       "  \"don't\": 2,\n",
       "  'care': 1,\n",
       "  'sticky': 1,\n",
       "  'feeling': 1,\n",
       "  'of': 1,\n",
       "  'due': 1,\n",
       "  'to': 2,\n",
       "  'being': 1,\n",
       "  'a': 1,\n",
       "  'soft': 1,\n",
       "  'also': 1,\n",
       "  'like': 1,\n",
       "  'fact': 1,\n",
       "  'that': 1,\n",
       "  'seems': 1,\n",
       "  'have': 1,\n",
       "  'ripples': 1,\n",
       "  'all': 1,\n",
       "  'over': 1,\n",
       "  'which': 1,\n",
       "  'unnoticeable': 1,\n",
       "  'while': 1},\n",
       " {'i': 1,\n",
       "  'bought': 1,\n",
       "  'this': 3,\n",
       "  'for': 1,\n",
       "  'my': 1,\n",
       "  'boyfriends': 1,\n",
       "  'birthday': 1,\n",
       "  'present': 1,\n",
       "  'after': 1,\n",
       "  'seeing': 1,\n",
       "  'his': 1,\n",
       "  'best': 1,\n",
       "  'friend': 1,\n",
       "  'with': 1,\n",
       "  'one': 1,\n",
       "  'and': 2,\n",
       "  'little': 1,\n",
       "  'jambox': 1,\n",
       "  'has': 2,\n",
       "  'been': 1,\n",
       "  'the': 3,\n",
       "  'answer': 1,\n",
       "  'to': 4,\n",
       "  'all': 1,\n",
       "  'of': 2,\n",
       "  'our': 1,\n",
       "  'portable': 1,\n",
       "  'music': 1,\n",
       "  'problems': 1,\n",
       "  \"it's\": 1,\n",
       "  'tiny': 1,\n",
       "  'but': 2,\n",
       "  'man': 1,\n",
       "  'does': 1,\n",
       "  'it': 7,\n",
       "  'pack': 1,\n",
       "  'a': 3,\n",
       "  'punch': 1,\n",
       "  'good': 1,\n",
       "  'bass': 1,\n",
       "  'too': 1,\n",
       "  'that': 1,\n",
       "  'is': 2,\n",
       "  'impressive': 2,\n",
       "  'takes': 1,\n",
       "  'few': 1,\n",
       "  'times': 1,\n",
       "  'get': 1,\n",
       "  'hang': 1,\n",
       "  'how': 1,\n",
       "  'works': 1,\n",
       "  'especially': 1,\n",
       "  'if': 1,\n",
       "  'you': 1,\n",
       "  'have': 1,\n",
       "  'two': 1,\n",
       "  'people': 1,\n",
       "  'connected': 1,\n",
       "  'we': 2,\n",
       "  'got': 1,\n",
       "  'down': 1,\n",
       "  'battery': 1,\n",
       "  'even': 1,\n",
       "  'lasts': 1,\n",
       "  'about': 1,\n",
       "  '4': 1,\n",
       "  '6': 1,\n",
       "  'hours': 1,\n",
       "  'on': 1,\n",
       "  'constant': 1,\n",
       "  'play': 1,\n",
       "  'which': 1,\n",
       "  'again': 1,\n",
       "  'love': 3,\n",
       "  'thing': 1},\n",
       " {'this': 1,\n",
       "  'iphone': 1,\n",
       "  'case': 1,\n",
       "  'is': 1,\n",
       "  'truly': 1,\n",
       "  'awesome': 1,\n",
       "  'besides': 1,\n",
       "  'actually': 1,\n",
       "  'protecting': 1,\n",
       "  'the': 4,\n",
       "  'back': 2,\n",
       "  'and': 1,\n",
       "  'screen': 1,\n",
       "  'of': 1,\n",
       "  'phone': 1,\n",
       "  'it': 1,\n",
       "  'has': 1,\n",
       "  'a': 1,\n",
       "  'metal': 1,\n",
       "  'kick': 1,\n",
       "  'stand': 1,\n",
       "  'attached': 1,\n",
       "  'to': 2,\n",
       "  'that': 1,\n",
       "  'i': 1,\n",
       "  'have': 1,\n",
       "  'not': 1,\n",
       "  'yet': 1,\n",
       "  'figured': 1,\n",
       "  'out': 1,\n",
       "  'how': 1,\n",
       "  'use': 1,\n",
       "  'properly': 1},\n",
       " {'my': 1,\n",
       "  'wife': 1,\n",
       "  'uses': 1,\n",
       "  'her': 1,\n",
       "  'phone': 1,\n",
       "  'quit': 1,\n",
       "  'a': 1,\n",
       "  'bit': 1,\n",
       "  'all': 1,\n",
       "  'day': 1,\n",
       "  'and': 1,\n",
       "  'night': 1,\n",
       "  'she': 1,\n",
       "  'gets': 1,\n",
       "  'almost': 1,\n",
       "  'two': 1,\n",
       "  'days': 1,\n",
       "  'from': 1,\n",
       "  'this': 1,\n",
       "  'battery': 1,\n",
       "  'would': 1,\n",
       "  'buy': 1,\n",
       "  'again': 1,\n",
       "  'if': 1,\n",
       "  'needed': 1},\n",
       " {'very': 1,\n",
       "  'nice': 1,\n",
       "  'little': 4,\n",
       "  'item': 2,\n",
       "  'stands': 1,\n",
       "  'out': 1,\n",
       "  'in': 2,\n",
       "  'your': 2,\n",
       "  'purse': 1,\n",
       "  'it': 2,\n",
       "  'really': 2,\n",
       "  'is': 2,\n",
       "  'a': 4,\n",
       "  'generic': 1,\n",
       "  'clamshell': 1,\n",
       "  'with': 2,\n",
       "  'net': 2,\n",
       "  'luggage': 1,\n",
       "  'holder': 1,\n",
       "  'like': 1,\n",
       "  'the': 4,\n",
       "  'on': 1,\n",
       "  'back': 1,\n",
       "  'of': 1,\n",
       "  'car': 1,\n",
       "  'trunk': 1,\n",
       "  'to': 1,\n",
       "  'hold': 1,\n",
       "  'place': 1,\n",
       "  'i': 1,\n",
       "  'use': 1,\n",
       "  'for': 1,\n",
       "  'ear': 1,\n",
       "  'bud': 1,\n",
       "  'earphones': 1,\n",
       "  'makes': 1,\n",
       "  'cute': 1,\n",
       "  'gift': 1,\n",
       "  'too': 1,\n",
       "  'cable': 1,\n",
       "  'tie': 1,\n",
       "  'worthless': 1,\n",
       "  'but': 1,\n",
       "  'no': 1,\n",
       "  'matter': 1,\n",
       "  'not': 1,\n",
       "  'needed': 1,\n",
       "  'round': 1,\n",
       "  'case': 1},\n",
       " {'gets': 1,\n",
       "  'the': 1,\n",
       "  'job': 1,\n",
       "  'done': 1,\n",
       "  'i': 1,\n",
       "  'dont': 1,\n",
       "  'use': 1,\n",
       "  'it': 1,\n",
       "  'everyday': 1,\n",
       "  'its': 1,\n",
       "  'more': 1,\n",
       "  'of': 1,\n",
       "  'an': 1,\n",
       "  'assessory': 1,\n",
       "  'to': 2,\n",
       "  'camofauge': 1,\n",
       "  'my': 1,\n",
       "  'mobile': 1,\n",
       "  'phone': 1,\n",
       "  'durible': 1,\n",
       "  'enough': 1,\n",
       "  'withstand': 1,\n",
       "  'multiple': 1,\n",
       "  'dropps': 1,\n",
       "  'or': 1,\n",
       "  'falls': 1,\n",
       "  'will': 1,\n",
       "  'purchase': 1,\n",
       "  'this': 1,\n",
       "  'brand': 1,\n",
       "  'again': 1,\n",
       "  'if': 1,\n",
       "  'needed': 1},\n",
       " {'i': 5,\n",
       "  'bought': 1,\n",
       "  'this': 2,\n",
       "  'cover': 3,\n",
       "  'after': 1,\n",
       "  'using': 1,\n",
       "  'the': 3,\n",
       "  'samsung': 1,\n",
       "  'flip': 1,\n",
       "  'have': 1,\n",
       "  'an': 2,\n",
       "  'stand': 1,\n",
       "  'inc': 1,\n",
       "  'sport': 1,\n",
       "  'model': 1,\n",
       "  'cell': 1,\n",
       "  'phone': 2,\n",
       "  'holder': 1,\n",
       "  'for': 1,\n",
       "  'car': 1,\n",
       "  'and': 2,\n",
       "  \"couldn't\": 1,\n",
       "  'see': 1,\n",
       "  'my': 2,\n",
       "  'screen': 2,\n",
       "  'now': 1,\n",
       "  'with': 1,\n",
       "  'is': 1,\n",
       "  'protected': 1,\n",
       "  'can': 1,\n",
       "  'view': 1,\n",
       "  \"it's\": 1,\n",
       "  'also': 1,\n",
       "  'not': 1,\n",
       "  'bulky': 1,\n",
       "  'like': 1,\n",
       "  'otter': 1,\n",
       "  'box': 1,\n",
       "  'so': 1,\n",
       "  'love': 1,\n",
       "  'it': 1},\n",
       " {'really': 1,\n",
       "  'awesome': 1,\n",
       "  'product': 1,\n",
       "  'not': 1,\n",
       "  'bulky': 1,\n",
       "  'like': 1,\n",
       "  'some': 1,\n",
       "  'other': 1,\n",
       "  'screen': 2,\n",
       "  'protectors': 1,\n",
       "  \"doesn't\": 1,\n",
       "  'affect': 1,\n",
       "  'typing': 1,\n",
       "  'or': 1,\n",
       "  'the': 2,\n",
       "  'viewing': 1,\n",
       "  'of': 1,\n",
       "  'things': 1,\n",
       "  'on': 1},\n",
       " {'i': 2,\n",
       "  'have': 1,\n",
       "  'tried': 1,\n",
       "  'the': 5,\n",
       "  'various': 1,\n",
       "  '3m': 1,\n",
       "  'screen': 2,\n",
       "  'protectors': 1,\n",
       "  'and': 1,\n",
       "  'this': 3,\n",
       "  'one': 1,\n",
       "  'is': 3,\n",
       "  'great': 1,\n",
       "  'hides': 1,\n",
       "  'your': 2,\n",
       "  'from': 1,\n",
       "  'others': 1,\n",
       "  'so': 1,\n",
       "  'you': 4,\n",
       "  'can': 1,\n",
       "  'view': 1,\n",
       "  'data': 1,\n",
       "  'securely': 1,\n",
       "  'am': 1,\n",
       "  'not': 2,\n",
       "  'enamored': 1,\n",
       "  'with': 1,\n",
       "  'cardboard': 1,\n",
       "  'applicator': 1,\n",
       "  'but': 1,\n",
       "  'it': 2,\n",
       "  'seems': 1,\n",
       "  'to': 1,\n",
       "  'do': 1,\n",
       "  'trick': 1,\n",
       "  'if': 2,\n",
       "  'want': 2,\n",
       "  'feel': 1,\n",
       "  'of': 1,\n",
       "  'glass': 1,\n",
       "  'protector': 1,\n",
       "  'for': 1,\n",
       "  'however': 1,\n",
       "  'privacy': 1},\n",
       " {'i': 19,\n",
       "  'received': 1,\n",
       "  'my': 4,\n",
       "  'case': 7,\n",
       "  'today': 1,\n",
       "  'am': 4,\n",
       "  'skeptical': 1,\n",
       "  'of': 3,\n",
       "  'its': 2,\n",
       "  'performance': 2,\n",
       "  'have': 1,\n",
       "  'otter': 4,\n",
       "  'box': 4,\n",
       "  'cases': 1,\n",
       "  'for': 2,\n",
       "  'other': 1,\n",
       "  'electronics': 1,\n",
       "  'so': 2,\n",
       "  'when': 1,\n",
       "  'ordered': 1,\n",
       "  'this': 3,\n",
       "  'was': 3,\n",
       "  'expecting': 3,\n",
       "  'the': 14,\n",
       "  'same': 2,\n",
       "  'quality': 1,\n",
       "  'material': 1,\n",
       "  'as': 2,\n",
       "  'but': 1,\n",
       "  'less': 1,\n",
       "  'weight': 1,\n",
       "  'price': 1,\n",
       "  'something': 1,\n",
       "  'more': 1,\n",
       "  'durable': 1,\n",
       "  'once': 1,\n",
       "  'put': 2,\n",
       "  'on': 2,\n",
       "  'instructed': 1,\n",
       "  'tried': 2,\n",
       "  'to': 9,\n",
       "  'make': 1,\n",
       "  'a': 3,\n",
       "  'phone': 4,\n",
       "  'call': 1,\n",
       "  'and': 3,\n",
       "  'realized': 2,\n",
       "  'that': 1,\n",
       "  'sound': 1,\n",
       "  'low': 1,\n",
       "  'too': 1,\n",
       "  'none': 1,\n",
       "  'had': 1,\n",
       "  'mess': 1,\n",
       "  'around': 1,\n",
       "  'with': 2,\n",
       "  'buttons': 1,\n",
       "  'get': 1,\n",
       "  'it': 4,\n",
       "  'adjust': 1,\n",
       "  'can': 2,\n",
       "  'hear': 1,\n",
       "  'then': 1,\n",
       "  'plug': 1,\n",
       "  'in': 2,\n",
       "  'head': 1,\n",
       "  'phones': 1,\n",
       "  'not': 1,\n",
       "  'use': 1,\n",
       "  'them': 1,\n",
       "  'without': 2,\n",
       "  'headphone': 1,\n",
       "  'adapter': 1,\n",
       "  'kind': 1,\n",
       "  'scared': 2,\n",
       "  'even': 2,\n",
       "  'water': 2,\n",
       "  'though': 1,\n",
       "  'passed': 1,\n",
       "  'test': 1,\n",
       "  'still': 1,\n",
       "  'spoiled': 1,\n",
       "  'me': 2,\n",
       "  'lifeproof': 1,\n",
       "  'give': 2,\n",
       "  'tank': 1,\n",
       "  'like': 1,\n",
       "  'thought': 1,\n",
       "  'would': 1,\n",
       "  'try': 1,\n",
       "  'see': 1,\n",
       "  'if': 1,\n",
       "  'stands': 1,\n",
       "  'up': 1,\n",
       "  'claims': 1},\n",
       " {'the': 2,\n",
       "  'price': 2,\n",
       "  'can': 1,\n",
       "  'not': 1,\n",
       "  'be': 1,\n",
       "  'beat': 1,\n",
       "  'on': 1,\n",
       "  'this': 1,\n",
       "  'product': 1,\n",
       "  'for': 1,\n",
       "  'anyone': 1,\n",
       "  'with': 1,\n",
       "  'a': 1,\n",
       "  'tablet': 1,\n",
       "  'these': 1,\n",
       "  'work': 1,\n",
       "  'perfectly': 1,\n",
       "  'and': 1,\n",
       "  'best': 1,\n",
       "  'part': 1,\n",
       "  'is': 1,\n",
       "  'because': 1,\n",
       "  'of': 1,\n",
       "  'you': 1,\n",
       "  'are': 1,\n",
       "  'never': 1,\n",
       "  'worried': 1,\n",
       "  'about': 1,\n",
       "  'losing': 1,\n",
       "  'one': 1},\n",
       " {'i': 6,\n",
       "  'had': 1,\n",
       "  'this': 4,\n",
       "  'smartphone': 3,\n",
       "  'for': 3,\n",
       "  'about': 1,\n",
       "  '2': 5,\n",
       "  'months': 1,\n",
       "  'and': 8,\n",
       "  'it': 7,\n",
       "  'is': 10,\n",
       "  'a': 8,\n",
       "  'great': 2,\n",
       "  'android': 1,\n",
       "  'os': 3,\n",
       "  'the': 15,\n",
       "  'latest': 1,\n",
       "  'so': 3,\n",
       "  'far': 1,\n",
       "  'at': 1,\n",
       "  'as': 3,\n",
       "  'matter': 2,\n",
       "  'of': 4,\n",
       "  'fact': 1,\n",
       "  'some': 1,\n",
       "  'smartphones': 1,\n",
       "  'in': 2,\n",
       "  'verizon': 1,\n",
       "  'att': 1,\n",
       "  'networks': 1,\n",
       "  'are': 1,\n",
       "  'still': 1,\n",
       "  'running': 2,\n",
       "  '1': 2,\n",
       "  \"you'd\": 1,\n",
       "  'be': 2,\n",
       "  'top': 1,\n",
       "  'line': 1,\n",
       "  'battery': 3,\n",
       "  'life': 1,\n",
       "  'lasting': 1,\n",
       "  'all': 1,\n",
       "  'day': 1,\n",
       "  'long': 1,\n",
       "  'if': 1,\n",
       "  'you': 2,\n",
       "  'install': 1,\n",
       "  'one': 2,\n",
       "  'management': 1,\n",
       "  'apps': 3,\n",
       "  'such': 1,\n",
       "  'saver': 1,\n",
       "  'set': 2,\n",
       "  'to': 3,\n",
       "  'turn': 1,\n",
       "  'off': 1,\n",
       "  'unused': 1,\n",
       "  'every': 1,\n",
       "  'or': 1,\n",
       "  'minutes': 1,\n",
       "  'my': 1,\n",
       "  'opinion': 1,\n",
       "  'best': 1,\n",
       "  'app': 1,\n",
       "  'task': 1,\n",
       "  'just': 1,\n",
       "  'make': 1,\n",
       "  'sure': 1,\n",
       "  'right': 1,\n",
       "  'stopped': 1,\n",
       "  'nothing': 1,\n",
       "  'worthwhile': 1,\n",
       "  'easy': 2,\n",
       "  'screen': 2,\n",
       "  'bright': 1,\n",
       "  'websites': 1,\n",
       "  'look': 1,\n",
       "  'beautiful': 1,\n",
       "  'only': 1,\n",
       "  'problem': 1,\n",
       "  'have': 2,\n",
       "  'with': 3,\n",
       "  'well': 1,\n",
       "  'any': 1,\n",
       "  'other': 1,\n",
       "  'touchscreen': 4,\n",
       "  'including': 2,\n",
       "  'er': 1,\n",
       "  'iphone': 1,\n",
       "  'that': 3,\n",
       "  'keyboard': 3,\n",
       "  'sucks': 1,\n",
       "  \"doesn't\": 2,\n",
       "  'what': 1,\n",
       "  'people': 1,\n",
       "  'say': 1,\n",
       "  'not': 2,\n",
       "  'use': 1,\n",
       "  'takes': 1,\n",
       "  'big': 1,\n",
       "  'chunk': 1,\n",
       "  \"i'm\": 1,\n",
       "  'letting': 1,\n",
       "  'go': 1,\n",
       "  'will': 1,\n",
       "  'get': 1,\n",
       "  'samsung': 1,\n",
       "  'intercept': 1,\n",
       "  'which': 1,\n",
       "  'has': 1,\n",
       "  'slide': 1,\n",
       "  'out': 1,\n",
       "  'way': 2,\n",
       "  'nature': 2,\n",
       "  'intended': 2,\n",
       "  'real': 1,\n",
       "  'keys': 1,\n",
       "  'even': 1,\n",
       "  'though': 1,\n",
       "  'dedicated': 1,\n",
       "  'graphics': 1,\n",
       "  'processor': 1,\n",
       "  \"don't\": 2,\n",
       "  'care': 1,\n",
       "  'since': 1,\n",
       "  'play': 1,\n",
       "  'video': 2,\n",
       "  'games': 3,\n",
       "  'much': 1,\n",
       "  'thing': 1,\n",
       "  'forgot': 1,\n",
       "  'optimus': 1,\n",
       "  'v': 1,\n",
       "  'plays': 1,\n",
       "  'videogames': 1,\n",
       "  'fluidly': 1,\n",
       "  'angry': 1,\n",
       "  'birds': 1,\n",
       "  'racing': 1,\n",
       "  'but': 1,\n",
       "  'playing': 1,\n",
       "  'on': 1,\n",
       "  'buy': 1,\n",
       "  'psp': 1,\n",
       "  'later': 1},\n",
       " {'good': 3,\n",
       "  'sound': 1,\n",
       "  'a': 1,\n",
       "  'very': 1,\n",
       "  'price': 1,\n",
       "  'looks': 1,\n",
       "  'to': 1,\n",
       "  'and': 1,\n",
       "  'is': 1,\n",
       "  'comfortable': 1,\n",
       "  'in': 1,\n",
       "  'the': 1,\n",
       "  'ear': 1,\n",
       "  'so': 1,\n",
       "  'what': 1,\n",
       "  'more': 1,\n",
       "  'can': 1,\n",
       "  'i': 1,\n",
       "  'say': 1},\n",
       " {'work': 1,\n",
       "  'well': 1,\n",
       "  'with': 1,\n",
       "  'seniors': 1,\n",
       "  'who': 1,\n",
       "  'do': 1,\n",
       "  'not': 1,\n",
       "  'have': 1,\n",
       "  'good': 1,\n",
       "  'tactile': 1,\n",
       "  'fingers': 2,\n",
       "  'or': 1,\n",
       "  'large': 1,\n",
       "  'on': 1,\n",
       "  'a': 1,\n",
       "  'ipad': 1,\n",
       "  'mini': 1,\n",
       "  'price': 1,\n",
       "  'is': 1,\n",
       "  'great': 1},\n",
       " {'i': 5,\n",
       "  'bought': 2,\n",
       "  'red': 1,\n",
       "  'blue': 1,\n",
       "  'and': 6,\n",
       "  'black': 2,\n",
       "  'the': 5,\n",
       "  'is': 1,\n",
       "  'my': 1,\n",
       "  'favorite': 1,\n",
       "  'they': 3,\n",
       "  'do': 1,\n",
       "  'help': 1,\n",
       "  'protect': 1,\n",
       "  'phone': 1,\n",
       "  'for': 1,\n",
       "  'price': 1,\n",
       "  'these': 2,\n",
       "  'are': 3,\n",
       "  'really': 1,\n",
       "  'good': 1,\n",
       "  'cases': 3,\n",
       "  'all': 1,\n",
       "  'ports': 1,\n",
       "  'holes': 1,\n",
       "  'were': 1,\n",
       "  'should': 1,\n",
       "  'be': 1,\n",
       "  'also': 3,\n",
       "  'like': 4,\n",
       "  'that': 1,\n",
       "  'tpu': 1,\n",
       "  'not': 1,\n",
       "  'silicon': 2,\n",
       "  'because': 2,\n",
       "  'starts': 1,\n",
       "  'to': 2,\n",
       "  'stretch': 1,\n",
       "  'after': 1,\n",
       "  'awhile': 1,\n",
       "  'having': 1,\n",
       "  'with': 1,\n",
       "  'stands': 1,\n",
       "  'watch': 1,\n",
       "  'a': 1,\n",
       "  'lot': 1,\n",
       "  'of': 1,\n",
       "  'media': 1,\n",
       "  'fit': 1,\n",
       "  'bill': 1,\n",
       "  'some': 1,\n",
       "  'more': 1,\n",
       "  'higher': 1,\n",
       "  'end': 1,\n",
       "  'incipio': 1,\n",
       "  'uag': 1,\n",
       "  'seidio': 1,\n",
       "  'active': 1,\n",
       "  'etc': 1},\n",
       " {'this': 1,\n",
       "  'is': 1,\n",
       "  'the': 3,\n",
       "  'same': 1,\n",
       "  'charger': 1,\n",
       "  'that': 1,\n",
       "  'came': 1,\n",
       "  'with': 1,\n",
       "  'razr': 1,\n",
       "  'v3': 1,\n",
       "  'phone': 2,\n",
       "  'light': 1,\n",
       "  'compact': 1,\n",
       "  'charges': 1},\n",
       " {'i': 2,\n",
       "  'purchased': 1,\n",
       "  'this': 1,\n",
       "  'in': 1,\n",
       "  'hope': 1,\n",
       "  'the': 1,\n",
       "  'reviews': 1,\n",
       "  'that': 1,\n",
       "  'said': 1,\n",
       "  'it': 1,\n",
       "  'glowed': 1,\n",
       "  'were': 2,\n",
       "  'correct': 1,\n",
       "  'unfortunately': 1,\n",
       "  'they': 1,\n",
       "  'not': 1,\n",
       "  'tried': 1,\n",
       "  'editing': 1,\n",
       "  'my': 1,\n",
       "  'settings': 1,\n",
       "  'but': 1,\n",
       "  'nothing': 1,\n",
       "  'worked': 1},\n",
       " {'this': 1,\n",
       "  'is': 1,\n",
       "  'a': 2,\n",
       "  'great': 1,\n",
       "  'item': 1,\n",
       "  'for': 2,\n",
       "  'the': 1,\n",
       "  'price': 1,\n",
       "  'i': 1,\n",
       "  'paid': 1,\n",
       "  'it': 4,\n",
       "  'highly': 1,\n",
       "  'recommend': 1,\n",
       "  'but': 1,\n",
       "  'if': 1,\n",
       "  'gets': 1,\n",
       "  'just': 1,\n",
       "  'lil': 1,\n",
       "  'wet': 1,\n",
       "  'u': 1,\n",
       "  'can': 1,\n",
       "  'wrap': 1,\n",
       "  'up': 1,\n",
       "  'because': 1,\n",
       "  \"it's\": 1,\n",
       "  'over': 1,\n",
       "  'with': 1},\n",
       " {'tips': 1,\n",
       "  'appear': 2,\n",
       "  'on': 1,\n",
       "  'stylus': 1,\n",
       "  'too': 1,\n",
       "  'large': 1,\n",
       "  'for': 1,\n",
       "  'galaxys3': 1,\n",
       "  'disappointed': 1,\n",
       "  'items': 1,\n",
       "  'also': 1,\n",
       "  'to': 1,\n",
       "  'be': 1,\n",
       "  'much': 1,\n",
       "  'more': 1,\n",
       "  'costly': 1,\n",
       "  'then': 1,\n",
       "  'they': 1,\n",
       "  'were': 1,\n",
       "  'last': 1},\n",
       " {'i': 5,\n",
       "  'have': 3,\n",
       "  'tried': 1,\n",
       "  'several': 1,\n",
       "  'and': 6,\n",
       "  'failed': 1,\n",
       "  'other': 1,\n",
       "  'bt': 1,\n",
       "  'head': 1,\n",
       "  'phones': 1,\n",
       "  'both': 1,\n",
       "  'more': 1,\n",
       "  'less': 1,\n",
       "  'expensive': 1,\n",
       "  'until': 1,\n",
       "  'now': 1,\n",
       "  \"it's\": 1,\n",
       "  'been': 1,\n",
       "  'bad': 1,\n",
       "  'these': 1,\n",
       "  'sound': 2,\n",
       "  'great': 1,\n",
       "  'an': 2,\n",
       "  'incredible': 1,\n",
       "  'range': 1,\n",
       "  'use': 1,\n",
       "  'iphone': 1,\n",
       "  'they': 3,\n",
       "  'are': 1,\n",
       "  'very': 1,\n",
       "  'comfortable': 1,\n",
       "  'come': 1,\n",
       "  'with': 1,\n",
       "  'additional': 1,\n",
       "  'ear': 1,\n",
       "  'pieces': 1,\n",
       "  'to': 2,\n",
       "  'fit': 1,\n",
       "  'any': 1,\n",
       "  'ears': 1,\n",
       "  'the': 3,\n",
       "  'block': 1,\n",
       "  'outside': 2,\n",
       "  'noise': 2,\n",
       "  'wonderfully': 1,\n",
       "  'also': 1,\n",
       "  'work': 1,\n",
       "  'well': 2,\n",
       "  'when': 1,\n",
       "  'talking': 1,\n",
       "  'on': 1,\n",
       "  'phone': 1,\n",
       "  'people': 1,\n",
       "  'talked': 1,\n",
       "  'say': 1,\n",
       "  'normal': 1,\n",
       "  \"haven't\": 1,\n",
       "  'used': 1,\n",
       "  'them': 1,\n",
       "  'so': 1,\n",
       "  \"don't\": 1,\n",
       "  'know': 1,\n",
       "  'how': 1,\n",
       "  'cancel': 1,\n",
       "  'wind': 1,\n",
       "  'such': 1},\n",
       " {'came': 1,\n",
       "  'with': 2,\n",
       "  'charger': 1,\n",
       "  'extra': 1,\n",
       "  'battery': 3,\n",
       "  'and': 2,\n",
       "  'i': 1,\n",
       "  'holster': 1,\n",
       "  'that': 1,\n",
       "  'you': 1,\n",
       "  'can': 1,\n",
       "  'charge': 2,\n",
       "  'a': 2,\n",
       "  'in': 1,\n",
       "  'was': 1,\n",
       "  'able': 1,\n",
       "  'to': 2,\n",
       "  'two': 2,\n",
       "  'batteries': 1,\n",
       "  'at': 1,\n",
       "  'once': 1,\n",
       "  'due': 1,\n",
       "  'having': 1,\n",
       "  'shargers': 1,\n",
       "  'this': 1,\n",
       "  'geeat': 1,\n",
       "  'for': 1,\n",
       "  'phone': 1,\n",
       "  'little': 1,\n",
       "  'life': 1},\n",
       " {\"i've\": 1,\n",
       "  'tried': 1,\n",
       "  'this': 1,\n",
       "  'with': 1,\n",
       "  'many': 1,\n",
       "  'galsxy': 1,\n",
       "  's3': 1,\n",
       "  'phones': 1,\n",
       "  'and': 3,\n",
       "  'nothing': 1,\n",
       "  'it': 3,\n",
       "  \"won't\": 1,\n",
       "  'even': 1,\n",
       "  'charge': 1,\n",
       "  'my': 1,\n",
       "  'phone': 1,\n",
       "  'when': 1,\n",
       "  'i': 2,\n",
       "  'have': 1,\n",
       "  'the': 3,\n",
       "  'chargerin': 1,\n",
       "  'day': 1,\n",
       "  'opened': 1,\n",
       "  'there': 1,\n",
       "  'was': 1,\n",
       "  'something': 1,\n",
       "  'shaking': 1,\n",
       "  'inside': 1,\n",
       "  'little': 1,\n",
       "  'white': 1,\n",
       "  'square': 1,\n",
       "  'very': 2,\n",
       "  'bad': 1,\n",
       "  'buy': 1,\n",
       "  'disappointed': 1}]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final = []\n",
    "for re in reviews:\n",
    "    \n",
    "    words = re.split()\n",
    "    uniquewords = pd.Series(words).unique()\n",
    "    freq = dict.fromkeys(uniquewords, 0)\n",
    "    for word in words:\n",
    "        freq[word] += 1\n",
    "    final.append(freq)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numOfWordsA = dict.fromkeys(uniquewords, 0)\n",
    "# bagOfWordsA = review.split()\n",
    "# for word in bagOfWordsA:\n",
    "    # numOfWordsA[word] += 1\n",
    "# numOfWordsA\n",
    "# numOfWordsB = dict.fromkeys(uniqueWords, 0)\n",
    "# for word in bagOfWordsB:\n",
    "    # numOfWordsB[word] += 1\n",
    "# pd.Series(numOfWordsA.values).equals(df['cnt'].values)\n",
    "# np.count_nonzero(pd.Series(numOfWordsA) - (df['cnt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = computeTF(numOfWordsA, bagOfWordsA)\n",
    "# pd.Series(result).equals(df['tf'])\n",
    "# np.count_nonzero(pd.Series(result).values  - df['tf'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 0.3136575588550416,\n",
       " 'is': 0.4855078157817008,\n",
       " 'a': 0.4248831939652659,\n",
       " 'great': 1.3121863889661687,\n",
       " 'new': 3.258096538021482,\n",
       " 'case': 1.8718021769015913,\n",
       " 'design': 3.258096538021482,\n",
       " 'that': 0.7731898882334817,\n",
       " 'i': 0.26236426446749106,\n",
       " 'have': 0.6931471805599453,\n",
       " 'not': 0.6190392084062235,\n",
       " 'seen': 2.5649493574615367,\n",
       " 'before': 2.5649493574615367,\n",
       " 'it': 0.4855078157817008,\n",
       " 'has': 1.3121863889661687,\n",
       " 'slim': 3.258096538021482,\n",
       " 'silicone': 3.258096538021482,\n",
       " 'skin': 3.258096538021482,\n",
       " 'really': 1.6486586255873816,\n",
       " 'locks': 3.258096538021482,\n",
       " 'in': 1.0608719606852626,\n",
       " 'the': 0.16705408466316624,\n",
       " 'phone': 0.6190392084062235,\n",
       " 'to': 0.4248831939652659,\n",
       " 'cover': 2.159484249353372,\n",
       " 'and': 0.4855078157817008,\n",
       " 'protect': 2.5649493574615367,\n",
       " 'your': 1.8718021769015913,\n",
       " 'from': 1.6486586255873816,\n",
       " 'spills': 3.258096538021482,\n",
       " 'such': 1.8718021769015913,\n",
       " 'also': 1.466337068793427,\n",
       " 'hard': 2.5649493574615367,\n",
       " 'polycarbonate': 3.258096538021482,\n",
       " 'outside': 2.5649493574615367,\n",
       " 'shell': 3.258096538021482,\n",
       " 'guard': 3.258096538021482,\n",
       " 'against': 3.258096538021482,\n",
       " 'damage': 3.258096538021482,\n",
       " 'comes': 2.5649493574615367,\n",
       " 'with': 0.4248831939652659,\n",
       " 'different': 2.5649493574615367,\n",
       " 'interchangeable': 3.258096538021482,\n",
       " 'skins': 3.258096538021482,\n",
       " 'covers': 3.258096538021482,\n",
       " 'create': 3.258096538021482,\n",
       " 'multiple': 2.5649493574615367,\n",
       " 'color': 3.258096538021482,\n",
       " 'combinations': 3.258096538021482,\n",
       " 'kind': 2.5649493574615367,\n",
       " 'of': 0.6190392084062235,\n",
       " 'than': 2.5649493574615367,\n",
       " 'usual': 3.258096538021482,\n",
       " 'chunk': 2.5649493574615367,\n",
       " 'plastic': 2.5649493574615367,\n",
       " 'innovative': 3.258096538021482,\n",
       " 'suits': 3.258096538021482,\n",
       " 'iphone': 1.6486586255873816,\n",
       " '5': 2.5649493574615367,\n",
       " 'perfectly': 2.159484249353372}"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computeIDF(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the base of the log()? Question! (This is the asnwer)\n",
    "- How are we going to find the word that \"best summarizes the review\" described by tfidf_data? The highes tfidf?????\n",
    "- What is the difference between using series.str.split() and str.split()? later seems to not return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnt</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>this</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0352941</td>\n",
       "      <td>0.313658</td>\n",
       "      <td>0.0110703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0352941</td>\n",
       "      <td>0.485508</td>\n",
       "      <td>0.0171356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0470588</td>\n",
       "      <td>0.424883</td>\n",
       "      <td>0.0199945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>great</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>1.31219</td>\n",
       "      <td>0.0154375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0352941</td>\n",
       "      <td>1.8718</td>\n",
       "      <td>0.0660636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>design</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>that</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0235294</td>\n",
       "      <td>0.77319</td>\n",
       "      <td>0.0181927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>0.262364</td>\n",
       "      <td>0.00308664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>have</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.00815467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>not</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>0.619039</td>\n",
       "      <td>0.00728281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seen</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0301759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>before</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0301759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0352941</td>\n",
       "      <td>0.424883</td>\n",
       "      <td>0.0149959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>has</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>1.31219</td>\n",
       "      <td>0.0154375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slim</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>silicone</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skin</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>really</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>1.64866</td>\n",
       "      <td>0.019396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>locks</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>1.06087</td>\n",
       "      <td>0.0124808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0352941</td>\n",
       "      <td>0.167054</td>\n",
       "      <td>0.00589603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>phone</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0235294</td>\n",
       "      <td>0.619039</td>\n",
       "      <td>0.0145656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0352941</td>\n",
       "      <td>0.424883</td>\n",
       "      <td>0.0149959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cover</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0235294</td>\n",
       "      <td>2.15948</td>\n",
       "      <td>0.0508114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0588235</td>\n",
       "      <td>0.485508</td>\n",
       "      <td>0.0285593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>protect</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0301759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>your</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>1.8718</td>\n",
       "      <td>0.0220212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>1.64866</td>\n",
       "      <td>0.019396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spills</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>such</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>1.8718</td>\n",
       "      <td>0.0220212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0235294</td>\n",
       "      <td>1.46634</td>\n",
       "      <td>0.034502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0301759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polycarbonate</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outside</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0301759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shell</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guard</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>against</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>damage</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comes</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0301759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>0.424883</td>\n",
       "      <td>0.00499863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>different</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0235294</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0603517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>interchangeable</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skins</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>covers</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>create</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>multiple</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0301759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>color</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>combinations</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kind</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0301759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0235294</td>\n",
       "      <td>0.619039</td>\n",
       "      <td>0.0145656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>than</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0301759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>usual</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chunk</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0301759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>plastic</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0301759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>innovative</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suits</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>3.2581</td>\n",
       "      <td>0.0383305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>iphone</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>1.64866</td>\n",
       "      <td>0.019396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.56495</td>\n",
       "      <td>0.0301759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perfectly</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0117647</td>\n",
       "      <td>2.15948</td>\n",
       "      <td>0.0254057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                cnt         tf       idf       tfidf\n",
       "this              3  0.0352941  0.313658   0.0110703\n",
       "is                3  0.0352941  0.485508   0.0171356\n",
       "a                 4  0.0470588  0.424883   0.0199945\n",
       "great             1  0.0117647   1.31219   0.0154375\n",
       "new               1  0.0117647    3.2581   0.0383305\n",
       "case              3  0.0352941    1.8718   0.0660636\n",
       "design            1  0.0117647    3.2581   0.0383305\n",
       "that              2  0.0235294   0.77319   0.0181927\n",
       "i                 1  0.0117647  0.262364  0.00308664\n",
       "have              1  0.0117647  0.693147  0.00815467\n",
       "not               1  0.0117647  0.619039  0.00728281\n",
       "seen              1  0.0117647   2.56495   0.0301759\n",
       "before            1  0.0117647   2.56495   0.0301759\n",
       "it                3  0.0352941  0.424883   0.0149959\n",
       "has               1  0.0117647   1.31219   0.0154375\n",
       "slim              1  0.0117647    3.2581   0.0383305\n",
       "silicone          1  0.0117647    3.2581   0.0383305\n",
       "skin              1  0.0117647    3.2581   0.0383305\n",
       "really            1  0.0117647   1.64866    0.019396\n",
       "locks             1  0.0117647    3.2581   0.0383305\n",
       "in                1  0.0117647   1.06087   0.0124808\n",
       "the               3  0.0352941  0.167054  0.00589603\n",
       "phone             2  0.0235294  0.619039   0.0145656\n",
       "to                3  0.0352941  0.424883   0.0149959\n",
       "cover             2  0.0235294   2.15948   0.0508114\n",
       "and               5  0.0588235  0.485508   0.0285593\n",
       "protect           1  0.0117647   2.56495   0.0301759\n",
       "your              1  0.0117647    1.8718   0.0220212\n",
       "from              1  0.0117647   1.64866    0.019396\n",
       "spills            1  0.0117647    3.2581   0.0383305\n",
       "such              1  0.0117647    1.8718   0.0220212\n",
       "also              2  0.0235294   1.46634    0.034502\n",
       "hard              1  0.0117647   2.56495   0.0301759\n",
       "polycarbonate     1  0.0117647    3.2581   0.0383305\n",
       "outside           1  0.0117647   2.56495   0.0301759\n",
       "shell             1  0.0117647    3.2581   0.0383305\n",
       "guard             1  0.0117647    3.2581   0.0383305\n",
       "against           1  0.0117647    3.2581   0.0383305\n",
       "damage            1  0.0117647    3.2581   0.0383305\n",
       "comes             1  0.0117647   2.56495   0.0301759\n",
       "with              1  0.0117647  0.424883  0.00499863\n",
       "different         2  0.0235294   2.56495   0.0603517\n",
       "interchangeable   1  0.0117647    3.2581   0.0383305\n",
       "skins             1  0.0117647    3.2581   0.0383305\n",
       "covers            1  0.0117647    3.2581   0.0383305\n",
       "create            1  0.0117647    3.2581   0.0383305\n",
       "multiple          1  0.0117647   2.56495   0.0301759\n",
       "color             1  0.0117647    3.2581   0.0383305\n",
       "combinations      1  0.0117647    3.2581   0.0383305\n",
       "kind              1  0.0117647   2.56495   0.0301759\n",
       "of                2  0.0235294  0.619039   0.0145656\n",
       "than              1  0.0117647   2.56495   0.0301759\n",
       "usual             1  0.0117647    3.2581   0.0383305\n",
       "chunk             1  0.0117647   2.56495   0.0301759\n",
       "plastic           1  0.0117647   2.56495   0.0301759\n",
       "innovative        1  0.0117647    3.2581   0.0383305\n",
       "suits             1  0.0117647    3.2581   0.0383305\n",
       "iphone            1  0.0117647   1.64866    0.019396\n",
       "5                 1  0.0117647   2.56495   0.0301759\n",
       "perfectly         1  0.0117647   2.15948   0.0254057"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = review.split()\n",
    "uniquewords = pd.Series(words).unique()\n",
    "freq = dict.fromkeys(uniquewords, 0)\n",
    "for word in words:\n",
    "    freq[word] += 1\n",
    "length = len(words)\n",
    "check = pd.DataFrame(columns=['cnt', 'tf', 'idf', 'tfidf'], index=uniquewords)  # dataframe of documents\n",
    "\n",
    "for word in uniquewords:\n",
    "    # print(word)\n",
    "    re_pat = '\\\\b%s\\\\b' % word\n",
    "    cnt = freq.get(word)\n",
    "    tf = cnt / length#(pd.Series(review).str.count('\\\\b') + 1).values[0]\n",
    "    idf = np.log(len(reviews) / reviews.str.contains(re_pat).sum())#.values[0]\n",
    "    # print(tf.values[0])\n",
    "    # df['cnt'], df['tf'], df['idf'], df['tfidf'] = cnt, tf, idf, tf * idf\n",
    "    check.loc[word, 'cnt'], check.loc[word, 'tfidf'] = cnt, tf * idf\n",
    "    check.loc[word, 'tf'], check.loc[word, 'idf'] = tf, idf\n",
    "    # df.loc[word] = pd.Series([cnt, tf, idf, tf * idf])\n",
    "# df.loc['this', 'cnt'] = 1\n",
    "\n",
    "check#['cnt'].sum()#.sort_values('tfidf', ascending=False)\n",
    "# com2 = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [\n",
    "    'the fox and the moon',\n",
    "    'the cow and the moon',\n",
    "    'the cow and the spoon'\n",
    "]\n",
    "sentences = pd.Series(sent)\n",
    "sentence = 'the fox and the moon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'fox', 'and', 'the', 'moon', 'the', 'cow', 'and', 'the', 'moon', 'the', 'cow', 'and', 'the', 'spoon']\n",
      "['the' 'fox' 'and' 'moon' 'cow' 'spoon']\n",
      "{'the': 0, 'fox': 0, 'and': 0, 'moon': 0, 'cow': 0, 'spoon': 0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cnt</th>\n",
       "      <th>tf</th>\n",
       "      <th>idf</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0666667</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>0.0732408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>moon</th>\n",
       "      <td>2</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.405465</td>\n",
       "      <td>0.054062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cow</th>\n",
       "      <td>2</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.405465</td>\n",
       "      <td>0.054062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spoon</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0666667</td>\n",
       "      <td>1.09861</td>\n",
       "      <td>0.0732408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cnt         tf       idf      tfidf\n",
       "the     6        0.4         0          0\n",
       "fox     1  0.0666667   1.09861  0.0732408\n",
       "and     3        0.2         0          0\n",
       "moon    2   0.133333  0.405465   0.054062\n",
       "cow     2   0.133333  0.405465   0.054062\n",
       "spoon   1  0.0666667   1.09861  0.0732408"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sentences.str.split().sum()\n",
    "print(words)\n",
    "uniquewords = pd.Series(words).unique()\n",
    "print(uniquewords)\n",
    "freq = dict.fromkeys(uniquewords, 0)\n",
    "print(freq)\n",
    "for word in words:\n",
    "    freq[word] += 1\n",
    "length = len(words)\n",
    "check = pd.DataFrame(columns=['cnt', 'tf', 'idf', 'tfidf'], index=uniquewords)  # dataframe of documents\n",
    "\n",
    "for word in uniquewords:\n",
    "    # print(word)\n",
    "    re_pat = '\\\\b%s\\\\b' % word\n",
    "    cnt = freq.get(word)\n",
    "    tf = cnt / length#(pd.Series(review).str.count('\\\\b') + 1).values[0]\n",
    "    idf = np.log(len(sentences) / sentences.str.contains(re_pat).sum())#.values[0]\n",
    "    # print(tf.values[0])\n",
    "    # df['cnt'], df['tf'], df['idf'], df['tfidf'] = cnt, tf, idf, tf * idf\n",
    "    check.loc[word, 'cnt'], check.loc[word, 'tfidf'] = cnt, tf * idf\n",
    "    check.loc[word, 'tf'], check.loc[word, 'idf'] = tf, idf\n",
    "    # df.loc[word] = pd.Series([cnt, tf, idf, tf * idf])\n",
    "# df.loc['this', 'cnt'] = 1\n",
    "\n",
    "check#['cnt'].sum()#.sort_values('tfidf', ascending=False)\n",
    "# com2 = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whole dataset testing\n",
    "fp = os.path.join('data', 'reviews.txt')\n",
    "reviews = pd.read_csv(fp, header=None, squeeze=True)\n",
    "review = open(os.path.join('data', 'review.txt'), encoding='utf8').read().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29 s, sys: 18.1 ms, total: 29 s\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "words = review.split()\n",
    "uniquewords = pd.Series(words).unique()\n",
    "freq = dict.fromkeys(uniquewords, 0)\n",
    "for word in words:\n",
    "    freq[word] += 1\n",
    "length = len(words)\n",
    "check = pd.DataFrame(columns=['cnt', 'tf', 'idf', 'tfidf'], index=uniquewords)  # dataframe of documents\n",
    "\n",
    "for word in uniquewords:\n",
    "    # print(word)\n",
    "    re_pat = '\\\\b%s\\\\b' % word\n",
    "    cnt = freq.get(word)\n",
    "    tf = cnt / length#(pd.Series(review).str.count('\\\\b') + 1).values[0]\n",
    "    idf = np.log(len(reviews) / reviews.str.contains(re_pat).sum())#.values[0]\n",
    "    # print(tf.values[0])\n",
    "    # df['cnt'], df['tf'], df['idf'], df['tfidf'] = cnt, tf, idf, tf * idf\n",
    "    check.loc[word, 'cnt'], check.loc[word, 'tfidf'] = cnt, tf * idf\n",
    "    check.loc[word, 'tf'], check.loc[word, 'idf'] = tf, idf\n",
    "    # df.loc[word] = pd.Series([cnt, tf, idf, tf * idf])\n",
    "# df.loc['this', 'cnt'] = 1\n",
    "\n",
    "check#['cnt'].sum()#.sort_values('tfidf', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_data(review, reviews):\n",
    "    \"\"\"\n",
    "    :Example:\n",
    "    >>> fp = os.path.join('data', 'reviews.txt')\n",
    "    >>> reviews = pd.read_csv(fp, header=None, squeeze=True)\n",
    "    >>> review = open(os.path.join('data', 'review.txt'), encoding='utf8').read().strip()\n",
    "    >>> out = tfidf_data(review, reviews)\n",
    "    >>> out['cnt'].sum()\n",
    "    85\n",
    "    >>> 'before' in out.index\n",
    "    True\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevant_word(out):\n",
    "    \"\"\"\n",
    "    :Example:\n",
    "    >>> fp = os.path.join('data', 'reviews.txt')\n",
    "    >>> reviews = pd.read_csv(fp, header=None, squeeze=True)\n",
    "    >>> review = open(os.path.join('data', 'review.txt'), encoding='utf8').read().strip()\n",
    "    >>> out = tfidf_data(review, reviews)\n",
    "    >>> relevant_word(out) in out.index\n",
    "    True\n",
    "    \"\"\"\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweet Analysis: Internet Research Agency\n",
    "\n",
    "The dataset `data/ira.csv` contains tweets tagged by Twitter as likely being posted by the *Internet Research Angency* (the tweet factory facing allegations for attempting to influence US political elections).\n",
    "\n",
    "The questions in this section will focus on the following:\n",
    "1. We will look at the hashtags present in the text and trends in their makeup.\n",
    "2. We will prepare this dataset for modeling by creating features out of the text fields.\n",
    "\n",
    "**Question 4 (HashTags)**\n",
    "\n",
    "You may assume that a hashtag is any string without whitespace following a `#` (this is more permissive than Twitters rules for hashtags; you are encouraged to go down this rabbit-hole to better figure out how to clean your data!).\n",
    "\n",
    "* Create a function `hashtag_list` that takes in a column of tweet-text and returns a column containing the list of hashtags present in the tweet text. If a tweet doesn't contain a hashtag, the function should return an empty list.\n",
    "\n",
    "* Create a function `most_common_hashtag` that takes in a column of hashtag-lists (the output above) and returns a column consisting a single hashtag from the tweet-text. \n",
    "    - If the text has no hashtags, the entry should be `NaN`,\n",
    "    - If the text has one distinct hashtag, the entry should contain that hashtag,\n",
    "    - If the text has more than one hashtag, the entry should be the most common hashtag (among all hashtags in the column). If there is a tie for most common, any of the most common can be returned.\n",
    "        - E.g. if the input column was: `pd.Series([[1, 2, 2], [3, 2, 3]])`, the output would be: `pd.Series([2, 2])`. Even though `3` was more common in the second list, `2` is the most common among all hashtags in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3906258</td>\n",
       "      <td>ea85ac8be1e8ab479064ca4c0fe3ac6587f76b1ef97452...</td>\n",
       "      <td>2016-11-16 09:04</td>\n",
       "      <td>The Best Exercise To Lose Belly Fat In 2 weeks...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1051443</td>\n",
       "      <td>8e58ab0f46d273103d9e71aa92cdaffb6e330ec7d15ae5...</td>\n",
       "      <td>2016-12-24 04:31</td>\n",
       "      <td>RT @Philanthropy: Dozens of ‘hate groups’ have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2823399</td>\n",
       "      <td>Room Of Rumor</td>\n",
       "      <td>2016-08-18 20:26</td>\n",
       "      <td>Artificial intelligence can find, map poverty,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>272878</td>\n",
       "      <td>San Francisco Daily</td>\n",
       "      <td>2016-03-18 19:28</td>\n",
       "      <td>Uber balks at rules proposed by world’s busies...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7697802</td>\n",
       "      <td>41bb9ae5991f53996752a0ab8dd36b543821abca8d5aed...</td>\n",
       "      <td>2016-07-30 15:44</td>\n",
       "      <td>RT @dirtroaddiva1: #IHatePokemonGoBecause he  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1409274</td>\n",
       "      <td>New York City Today</td>\n",
       "      <td>2016-01-04 19:02</td>\n",
       "      <td>Chick-fil-A remains closed after health violat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2973541</td>\n",
       "      <td>ce7b9f8c86dfbf9b2bd03eda62f0d42ac1c2b1b593ba0b...</td>\n",
       "      <td>2016-05-20 14:56</td>\n",
       "      <td>RT @SenSanders: We cannot afford to wait to ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1042655</td>\n",
       "      <td>Andy Sparks</td>\n",
       "      <td>2016-04-13 14:52</td>\n",
       "      <td>RT @MatthewGellert: #IWouldPreferToForget that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7838616</td>\n",
       "      <td>40bd0ff013b85c7646ca07ad238bc4dc865ce2cc87034a...</td>\n",
       "      <td>2016-10-08 10:19</td>\n",
       "      <td>RT @rapstationradio: #NowPlaying: RJ (OMMIO) \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8005939</td>\n",
       "      <td>0512ea612cfe45a7d9c8c0fd42466e8a8068a6fb3efb34...</td>\n",
       "      <td>2016-08-15 09:57</td>\n",
       "      <td>Hill Street Vida Blues. #AthleticsTVShows @sus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6262477</td>\n",
       "      <td>ef983249ef6ed5de427c4dc19ad6d966c6cf572c2505e4...</td>\n",
       "      <td>2016-09-09 18:39</td>\n",
       "      <td>RT @c982f7295cf57508a8d39bae6310c9546492d4105c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>7358841</td>\n",
       "      <td>San Francisco Daily</td>\n",
       "      <td>2016-10-30 19:25</td>\n",
       "      <td>PHOTOS: Man driving ATV hit by semi-truck whil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>7387743</td>\n",
       "      <td>0a0af8893cdc8454338447004eeaf65ee2934977c71ec4...</td>\n",
       "      <td>2016-10-08 08:30</td>\n",
       "      <td>RT @shannoncoulter: You don't have to use your...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6472710</td>\n",
       "      <td>Seattle Post</td>\n",
       "      <td>2016-06-03 16:27</td>\n",
       "      <td>Celebrity biographer Wendy Leigh dies in Londo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>664831</td>\n",
       "      <td>Syria Today</td>\n",
       "      <td>2016-10-25 13:50</td>\n",
       "      <td>48 citizens escape horrible acts of terrorists...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3388766</td>\n",
       "      <td>Warfare Worldwide</td>\n",
       "      <td>2016-01-27 18:33</td>\n",
       "      <td>@WarfareWW Its remaining divisions will be ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8085368</td>\n",
       "      <td>a317054d08e59498204d8262cef56217bab7e9f33a0e32...</td>\n",
       "      <td>2016-11-29 14:10</td>\n",
       "      <td>RT @NiqueTatted_721: [VIDEO] 2fic – “The Mantl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3557764</td>\n",
       "      <td>e7c3a4b1e42a99361173456b74a01fe65033e637c69944...</td>\n",
       "      <td>2016-10-29 03:37</td>\n",
       "      <td>RT @Trentsickle: Hillary's campaign manager @R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8463444</td>\n",
       "      <td>cd08e4b204df337b04b4b0955d3479ca3d7de11f3dc271...</td>\n",
       "      <td>2016-03-10 17:52</td>\n",
       "      <td>NFL free agency 2016 live updates: Brandon Bro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5597149</td>\n",
       "      <td>Cleveland Online</td>\n",
       "      <td>2016-10-04 14:37</td>\n",
       "      <td>Ohio voters request absentee ballots for presi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4333298</td>\n",
       "      <td>1-800-WOKE-AF</td>\n",
       "      <td>2016-06-24 11:36</td>\n",
       "      <td>RT @srslyjaidyn: I don't think I have either.....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>550190</td>\n",
       "      <td>f8b72a3e67daa7adce161eed7ee9583f2bc80db18c39ee...</td>\n",
       "      <td>2016-08-17 00:00</td>\n",
       "      <td>Really, Facebook? People kissing is bad? Out o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2685251</td>\n",
       "      <td>aa80df35407d74929dc17b058bdc679ae7f995c237f1cf...</td>\n",
       "      <td>2016-03-22 19:04</td>\n",
       "      <td>#StopIslam #IslamKills Obama wants us to belie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>7286081</td>\n",
       "      <td>212274cc2985b28273a7531d4340aeaf2a6455dd1792fa...</td>\n",
       "      <td>2016-06-01 08:23</td>\n",
       "      <td>RT @GeffGefferson1: #SlowJamTV Back At One Day...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>3929593</td>\n",
       "      <td>1-800-WOKE-AF</td>\n",
       "      <td>2016-09-03 21:10</td>\n",
       "      <td>RT @BleepThePolice: SCprosecutor:No reason to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3916555</td>\n",
       "      <td>9654d48c86e21d57ccefa8fbb945ca3355f558bd47bca5...</td>\n",
       "      <td>2016-03-23 07:53</td>\n",
       "      <td>RT @DanScavino: .@realDonaldTrump was consider...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5910474</td>\n",
       "      <td>San Francisco Daily</td>\n",
       "      <td>2016-05-31 08:32</td>\n",
       "      <td>AP FACT CHECK: Some Clinton email misstatement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>235271</td>\n",
       "      <td>cfb0d237487ebe3520cb83bd82df903c9c7afd5a60acba...</td>\n",
       "      <td>2016-12-11 16:54</td>\n",
       "      <td>RT @Conservatexian: News post: \"Biden tamps do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>7203296</td>\n",
       "      <td>cd08e4b204df337b04b4b0955d3479ca3d7de11f3dc271...</td>\n",
       "      <td>2016-09-16 16:28</td>\n",
       "      <td>Denham Springs man dies after crashing car int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4689099</td>\n",
       "      <td>e35cc49080427faaf1f73e41752c5fea35a96f6c243863...</td>\n",
       "      <td>2016-02-29 02:34</td>\n",
       "      <td>#OscarsSoWhite ? Try #oscarssoboring and whate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89970</th>\n",
       "      <td>6353988</td>\n",
       "      <td>Phoenix Daily News</td>\n",
       "      <td>2016-04-12 07:27</td>\n",
       "      <td>#local #news Wings of Freedom tour brings WWII...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89971</th>\n",
       "      <td>2009707</td>\n",
       "      <td>0dea399346f2f1a94e11fbc84897a35f1ccd3fcde6ea5f...</td>\n",
       "      <td>2016-11-05 14:41</td>\n",
       "      <td>RT @BlackElleWoods: 🙄 just take your votes &amp;am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89972</th>\n",
       "      <td>3458042</td>\n",
       "      <td>4537439c2e1d6f7d6d3e290b17e511c919556891aad999...</td>\n",
       "      <td>2016-11-21 16:08</td>\n",
       "      <td>RT @yellowgerbil: #HateToEatAndRun but this is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89973</th>\n",
       "      <td>4374946</td>\n",
       "      <td>New York City Today</td>\n",
       "      <td>2016-11-19 08:40</td>\n",
       "      <td>At least four dead after airplane crashes into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89974</th>\n",
       "      <td>4253989</td>\n",
       "      <td>474bea05f1324e553322b796f91e998e20cb142bbb1dd1...</td>\n",
       "      <td>2016-02-24 16:51</td>\n",
       "      <td>RT @uraniisitnikov2: Peak 3 Payday Loans. 24x7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89975</th>\n",
       "      <td>6563623</td>\n",
       "      <td>Giselle Evans</td>\n",
       "      <td>2016-06-19 15:28</td>\n",
       "      <td>RT @hippycinephile: #ThingsIWontTellMyDad  I b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89976</th>\n",
       "      <td>7119454</td>\n",
       "      <td>7b9a8430f80fe5d371a1d2b3c18ee53cceb4d2fbeb974b...</td>\n",
       "      <td>2016-09-05 17:09</td>\n",
       "      <td>RT @blicqer: Dwyane Wade Speaks Out On Gun Vio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89977</th>\n",
       "      <td>5618478</td>\n",
       "      <td>a317054d08e59498204d8262cef56217bab7e9f33a0e32...</td>\n",
       "      <td>2016-11-30 00:15</td>\n",
       "      <td>RT @CicelyRenee: Don't.  Do.  Me. https://t.co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89978</th>\n",
       "      <td>8665093</td>\n",
       "      <td>83216c1bdaf0245f9ac5b98a8c4b3cf2a1634b74d8b38d...</td>\n",
       "      <td>2016-12-24 00:34</td>\n",
       "      <td>RT @MichaelSkolnik: I know Hillary is recoveri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89979</th>\n",
       "      <td>9028272</td>\n",
       "      <td>Kathie</td>\n",
       "      <td>2016-12-26 15:13</td>\n",
       "      <td>RT @Solely_Toya: #ChristmasAftermath Trying to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89980</th>\n",
       "      <td>5172124</td>\n",
       "      <td>Memphis Online</td>\n",
       "      <td>2016-10-31 09:17</td>\n",
       "      <td>Dry conditions blamed for wildfires in eastern...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89981</th>\n",
       "      <td>6452255</td>\n",
       "      <td>caf105a130b8a17444728eca4a6447acb063701a86385c...</td>\n",
       "      <td>2016-10-19 15:08</td>\n",
       "      <td>RT @ChrixMorgan: #RejectedDebateTopics support...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89982</th>\n",
       "      <td>5625767</td>\n",
       "      <td>3da8a7609e8e8bbc93fb2051c4d6568ad5436d629d0037...</td>\n",
       "      <td>2016-11-12 08:03</td>\n",
       "      <td>RT @JBBinLaden12: What turns you on?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89983</th>\n",
       "      <td>5774604</td>\n",
       "      <td>Newspeak Daily</td>\n",
       "      <td>2016-04-13 21:58</td>\n",
       "      <td>Sex is no fun when you think your partner is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89984</th>\n",
       "      <td>232417</td>\n",
       "      <td>Rita Hart</td>\n",
       "      <td>2016-09-28 13:45</td>\n",
       "      <td>RT @therichards5: #MyBestFriendIsntAllowed to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89985</th>\n",
       "      <td>2480207</td>\n",
       "      <td>2102d9a41e7ffa7e3f9d67b5b41a3ff77eed1d7d2a8c2c...</td>\n",
       "      <td>2016-01-12 21:57</td>\n",
       "      <td>A father mistook his 14-year-old son for an in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89986</th>\n",
       "      <td>1372677</td>\n",
       "      <td>Pigeon Today</td>\n",
       "      <td>2016-01-18 17:04</td>\n",
       "      <td>Of course, guys! You're welcome in the USA! #r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89987</th>\n",
       "      <td>5085769</td>\n",
       "      <td>Phoenix Daily News</td>\n",
       "      <td>2016-04-27 19:50</td>\n",
       "      <td>Can't-miss May events for Arizona kids  #events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89988</th>\n",
       "      <td>7272603</td>\n",
       "      <td>aa80df35407d74929dc17b058bdc679ae7f995c237f1cf...</td>\n",
       "      <td>2016-01-10 14:03</td>\n",
       "      <td>RT @QuikHit: Peaceful is Muslims staying in th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89989</th>\n",
       "      <td>68403</td>\n",
       "      <td>1fdac06ebb156f54ed03ca124d7faa8638dff2e7229910...</td>\n",
       "      <td>2016-09-18 12:27</td>\n",
       "      <td>RT @TheWelshTwitt: Heads up #Ohio Your Governo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89990</th>\n",
       "      <td>4030737</td>\n",
       "      <td>c465bceee4e65cc392661fafdecabc98450eb1f0b67e66...</td>\n",
       "      <td>2016-06-17 13:37</td>\n",
       "      <td>#BlackLivesMatter talking about going after ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89991</th>\n",
       "      <td>6926508</td>\n",
       "      <td>Dana Gold</td>\n",
       "      <td>2016-07-02 15:04</td>\n",
       "      <td>RT @SideOfHashTags: #IHate____Because  I hate ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89992</th>\n",
       "      <td>8815244</td>\n",
       "      <td>f46e654ff3f1f9697f2b94de5a2e42a6914e1f00da14a7...</td>\n",
       "      <td>2016-12-18 13:03</td>\n",
       "      <td>Sixpence Nun The Richer #MakeMusicReligious</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89993</th>\n",
       "      <td>8609135</td>\n",
       "      <td>Kathie</td>\n",
       "      <td>2016-12-28 16:14</td>\n",
       "      <td>RT @felicia014: Too Many BAD Surprises #2016In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89994</th>\n",
       "      <td>8771336</td>\n",
       "      <td>9ebdcf10ebedc9abf33a34e07792e18230ecd26cea77ab...</td>\n",
       "      <td>2016-04-26 00:48</td>\n",
       "      <td>11-year-old in Colonial Heights mauled by same...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89995</th>\n",
       "      <td>5635647</td>\n",
       "      <td>KansasCityDailyNews</td>\n",
       "      <td>2016-04-03 21:19</td>\n",
       "      <td>Trump: Kasich shouldn't be allowed to run http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89996</th>\n",
       "      <td>7012979</td>\n",
       "      <td>f46e654ff3f1f9697f2b94de5a2e42a6914e1f00da14a7...</td>\n",
       "      <td>2016-12-19 15:04</td>\n",
       "      <td>RT @JefLeeson: #ThingsYouCantIgnore The last s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89997</th>\n",
       "      <td>6955774</td>\n",
       "      <td>88669ad69e40d7c199af91e8107f1e0e7988d377d2e41f...</td>\n",
       "      <td>2016-08-27 14:15</td>\n",
       "      <td>RT @nealcarter: When someone said the first li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89998</th>\n",
       "      <td>8563509</td>\n",
       "      <td>ec2109adb67d2a24091026d5d9aab64dadca1fdb2f7355...</td>\n",
       "      <td>2016-10-20 15:19</td>\n",
       "      <td>RT @indigenous01: #rantfortoday I speak the Wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89999</th>\n",
       "      <td>8031863</td>\n",
       "      <td>Seattle Post</td>\n",
       "      <td>2016-04-04 01:54</td>\n",
       "      <td>10 Things to Know for Monday https://t.co/XoOg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               name  \\\n",
       "0      3906258  ea85ac8be1e8ab479064ca4c0fe3ac6587f76b1ef97452...   \n",
       "1      1051443  8e58ab0f46d273103d9e71aa92cdaffb6e330ec7d15ae5...   \n",
       "2      2823399                                      Room Of Rumor   \n",
       "3       272878                                San Francisco Daily   \n",
       "4      7697802  41bb9ae5991f53996752a0ab8dd36b543821abca8d5aed...   \n",
       "5      1409274                                New York City Today   \n",
       "6      2973541  ce7b9f8c86dfbf9b2bd03eda62f0d42ac1c2b1b593ba0b...   \n",
       "7      1042655                                        Andy Sparks   \n",
       "8      7838616  40bd0ff013b85c7646ca07ad238bc4dc865ce2cc87034a...   \n",
       "9      8005939  0512ea612cfe45a7d9c8c0fd42466e8a8068a6fb3efb34...   \n",
       "10     6262477  ef983249ef6ed5de427c4dc19ad6d966c6cf572c2505e4...   \n",
       "11     7358841                                San Francisco Daily   \n",
       "12     7387743  0a0af8893cdc8454338447004eeaf65ee2934977c71ec4...   \n",
       "13     6472710                                       Seattle Post   \n",
       "14      664831                                        Syria Today   \n",
       "15     3388766                                  Warfare Worldwide   \n",
       "16     8085368  a317054d08e59498204d8262cef56217bab7e9f33a0e32...   \n",
       "17     3557764  e7c3a4b1e42a99361173456b74a01fe65033e637c69944...   \n",
       "18     8463444  cd08e4b204df337b04b4b0955d3479ca3d7de11f3dc271...   \n",
       "19     5597149                                   Cleveland Online   \n",
       "20     4333298                                      1-800-WOKE-AF   \n",
       "21      550190  f8b72a3e67daa7adce161eed7ee9583f2bc80db18c39ee...   \n",
       "22     2685251  aa80df35407d74929dc17b058bdc679ae7f995c237f1cf...   \n",
       "23     7286081  212274cc2985b28273a7531d4340aeaf2a6455dd1792fa...   \n",
       "24     3929593                                      1-800-WOKE-AF   \n",
       "25     3916555  9654d48c86e21d57ccefa8fbb945ca3355f558bd47bca5...   \n",
       "26     5910474                                San Francisco Daily   \n",
       "27      235271  cfb0d237487ebe3520cb83bd82df903c9c7afd5a60acba...   \n",
       "28     7203296  cd08e4b204df337b04b4b0955d3479ca3d7de11f3dc271...   \n",
       "29     4689099  e35cc49080427faaf1f73e41752c5fea35a96f6c243863...   \n",
       "...        ...                                                ...   \n",
       "89970  6353988                                 Phoenix Daily News   \n",
       "89971  2009707  0dea399346f2f1a94e11fbc84897a35f1ccd3fcde6ea5f...   \n",
       "89972  3458042  4537439c2e1d6f7d6d3e290b17e511c919556891aad999...   \n",
       "89973  4374946                                New York City Today   \n",
       "89974  4253989  474bea05f1324e553322b796f91e998e20cb142bbb1dd1...   \n",
       "89975  6563623                                      Giselle Evans   \n",
       "89976  7119454  7b9a8430f80fe5d371a1d2b3c18ee53cceb4d2fbeb974b...   \n",
       "89977  5618478  a317054d08e59498204d8262cef56217bab7e9f33a0e32...   \n",
       "89978  8665093  83216c1bdaf0245f9ac5b98a8c4b3cf2a1634b74d8b38d...   \n",
       "89979  9028272                                             Kathie   \n",
       "89980  5172124                                     Memphis Online   \n",
       "89981  6452255  caf105a130b8a17444728eca4a6447acb063701a86385c...   \n",
       "89982  5625767  3da8a7609e8e8bbc93fb2051c4d6568ad5436d629d0037...   \n",
       "89983  5774604                                     Newspeak Daily   \n",
       "89984   232417                                          Rita Hart   \n",
       "89985  2480207  2102d9a41e7ffa7e3f9d67b5b41a3ff77eed1d7d2a8c2c...   \n",
       "89986  1372677                                       Pigeon Today   \n",
       "89987  5085769                                 Phoenix Daily News   \n",
       "89988  7272603  aa80df35407d74929dc17b058bdc679ae7f995c237f1cf...   \n",
       "89989    68403  1fdac06ebb156f54ed03ca124d7faa8638dff2e7229910...   \n",
       "89990  4030737  c465bceee4e65cc392661fafdecabc98450eb1f0b67e66...   \n",
       "89991  6926508                                          Dana Gold   \n",
       "89992  8815244  f46e654ff3f1f9697f2b94de5a2e42a6914e1f00da14a7...   \n",
       "89993  8609135                                             Kathie   \n",
       "89994  8771336  9ebdcf10ebedc9abf33a34e07792e18230ecd26cea77ab...   \n",
       "89995  5635647                                KansasCityDailyNews   \n",
       "89996  7012979  f46e654ff3f1f9697f2b94de5a2e42a6914e1f00da14a7...   \n",
       "89997  6955774  88669ad69e40d7c199af91e8107f1e0e7988d377d2e41f...   \n",
       "89998  8563509  ec2109adb67d2a24091026d5d9aab64dadca1fdb2f7355...   \n",
       "89999  8031863                                       Seattle Post   \n",
       "\n",
       "                   date                                               text  \n",
       "0      2016-11-16 09:04  The Best Exercise To Lose Belly Fat In 2 weeks...  \n",
       "1      2016-12-24 04:31  RT @Philanthropy: Dozens of ‘hate groups’ have...  \n",
       "2      2016-08-18 20:26  Artificial intelligence can find, map poverty,...  \n",
       "3      2016-03-18 19:28  Uber balks at rules proposed by world’s busies...  \n",
       "4      2016-07-30 15:44  RT @dirtroaddiva1: #IHatePokemonGoBecause he  ...  \n",
       "5      2016-01-04 19:02  Chick-fil-A remains closed after health violat...  \n",
       "6      2016-05-20 14:56  RT @SenSanders: We cannot afford to wait to ad...  \n",
       "7      2016-04-13 14:52  RT @MatthewGellert: #IWouldPreferToForget that...  \n",
       "8      2016-10-08 10:19  RT @rapstationradio: #NowPlaying: RJ (OMMIO) \"...  \n",
       "9      2016-08-15 09:57  Hill Street Vida Blues. #AthleticsTVShows @sus...  \n",
       "10     2016-09-09 18:39  RT @c982f7295cf57508a8d39bae6310c9546492d4105c...  \n",
       "11     2016-10-30 19:25  PHOTOS: Man driving ATV hit by semi-truck whil...  \n",
       "12     2016-10-08 08:30  RT @shannoncoulter: You don't have to use your...  \n",
       "13     2016-06-03 16:27  Celebrity biographer Wendy Leigh dies in Londo...  \n",
       "14     2016-10-25 13:50  48 citizens escape horrible acts of terrorists...  \n",
       "15     2016-01-27 18:33  @WarfareWW Its remaining divisions will be ass...  \n",
       "16     2016-11-29 14:10  RT @NiqueTatted_721: [VIDEO] 2fic – “The Mantl...  \n",
       "17     2016-10-29 03:37  RT @Trentsickle: Hillary's campaign manager @R...  \n",
       "18     2016-03-10 17:52  NFL free agency 2016 live updates: Brandon Bro...  \n",
       "19     2016-10-04 14:37  Ohio voters request absentee ballots for presi...  \n",
       "20     2016-06-24 11:36  RT @srslyjaidyn: I don't think I have either.....  \n",
       "21     2016-08-17 00:00  Really, Facebook? People kissing is bad? Out o...  \n",
       "22     2016-03-22 19:04  #StopIslam #IslamKills Obama wants us to belie...  \n",
       "23     2016-06-01 08:23  RT @GeffGefferson1: #SlowJamTV Back At One Day...  \n",
       "24     2016-09-03 21:10  RT @BleepThePolice: SCprosecutor:No reason to ...  \n",
       "25     2016-03-23 07:53  RT @DanScavino: .@realDonaldTrump was consider...  \n",
       "26     2016-05-31 08:32  AP FACT CHECK: Some Clinton email misstatement...  \n",
       "27     2016-12-11 16:54  RT @Conservatexian: News post: \"Biden tamps do...  \n",
       "28     2016-09-16 16:28  Denham Springs man dies after crashing car int...  \n",
       "29     2016-02-29 02:34  #OscarsSoWhite ? Try #oscarssoboring and whate...  \n",
       "...                 ...                                                ...  \n",
       "89970  2016-04-12 07:27  #local #news Wings of Freedom tour brings WWII...  \n",
       "89971  2016-11-05 14:41  RT @BlackElleWoods: 🙄 just take your votes &am...  \n",
       "89972  2016-11-21 16:08  RT @yellowgerbil: #HateToEatAndRun but this is...  \n",
       "89973  2016-11-19 08:40  At least four dead after airplane crashes into...  \n",
       "89974  2016-02-24 16:51  RT @uraniisitnikov2: Peak 3 Payday Loans. 24x7...  \n",
       "89975  2016-06-19 15:28  RT @hippycinephile: #ThingsIWontTellMyDad  I b...  \n",
       "89976  2016-09-05 17:09  RT @blicqer: Dwyane Wade Speaks Out On Gun Vio...  \n",
       "89977  2016-11-30 00:15  RT @CicelyRenee: Don't.  Do.  Me. https://t.co...  \n",
       "89978  2016-12-24 00:34  RT @MichaelSkolnik: I know Hillary is recoveri...  \n",
       "89979  2016-12-26 15:13  RT @Solely_Toya: #ChristmasAftermath Trying to...  \n",
       "89980  2016-10-31 09:17  Dry conditions blamed for wildfires in eastern...  \n",
       "89981  2016-10-19 15:08  RT @ChrixMorgan: #RejectedDebateTopics support...  \n",
       "89982  2016-11-12 08:03               RT @JBBinLaden12: What turns you on?  \n",
       "89983  2016-04-13 21:58  Sex is no fun when you think your partner is a...  \n",
       "89984  2016-09-28 13:45  RT @therichards5: #MyBestFriendIsntAllowed to ...  \n",
       "89985  2016-01-12 21:57  A father mistook his 14-year-old son for an in...  \n",
       "89986  2016-01-18 17:04  Of course, guys! You're welcome in the USA! #r...  \n",
       "89987  2016-04-27 19:50    Can't-miss May events for Arizona kids  #events  \n",
       "89988  2016-01-10 14:03  RT @QuikHit: Peaceful is Muslims staying in th...  \n",
       "89989  2016-09-18 12:27  RT @TheWelshTwitt: Heads up #Ohio Your Governo...  \n",
       "89990  2016-06-17 13:37  #BlackLivesMatter talking about going after ch...  \n",
       "89991  2016-07-02 15:04  RT @SideOfHashTags: #IHate____Because  I hate ...  \n",
       "89992  2016-12-18 13:03        Sixpence Nun The Richer #MakeMusicReligious  \n",
       "89993  2016-12-28 16:14  RT @felicia014: Too Many BAD Surprises #2016In...  \n",
       "89994  2016-04-26 00:48  11-year-old in Colonial Heights mauled by same...  \n",
       "89995  2016-04-03 21:19  Trump: Kasich shouldn't be allowed to run http...  \n",
       "89996  2016-12-19 15:04  RT @JefLeeson: #ThingsYouCantIgnore The last s...  \n",
       "89997  2016-08-27 14:15  RT @nealcarter: When someone said the first li...  \n",
       "89998  2016-10-20 15:19  RT @indigenous01: #rantfortoday I speak the Wo...  \n",
       "89999  2016-04-04 01:54  10 Things to Know for Monday https://t.co/XoOg...  \n",
       "\n",
       "[90000 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp = os.path.join('data', 'ira.csv')\n",
    "ira = pd.read_csv(fp, names=['id', 'name', 'date', 'text'])\n",
    "ira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               [Exercise, LoseBellyFat, CatTV, TeenWolf]\n",
       "1                                                      []\n",
       "2                                                  [tech]\n",
       "3                                                  [news]\n",
       "4                  [IHatePokemonGoBecause, PokesAreJokes]\n",
       "5                                                [health]\n",
       "6                                                      []\n",
       "7                                  [IWouldPreferToForget]\n",
       "8                        [NowPlaying, rap, hiphop, music]\n",
       "9                                      [AthleticsTVShows]\n",
       "10                                 [HillaryRottenClinton]\n",
       "11                                                     []\n",
       "12                                           [TrumpTapes]\n",
       "13                                        [entertainment]\n",
       "14                                                     []\n",
       "15                                                     []\n",
       "16                                                     []\n",
       "17                                                     []\n",
       "18                                               [sports]\n",
       "19                                             [politics]\n",
       "20                                                     []\n",
       "21                                                     []\n",
       "22                  [StopIslam, IslamKills, ParisAttacks]\n",
       "23                                            [SlowJamTV]\n",
       "24                            [assaultatspringvalleyhigh]\n",
       "25                              [Brussels, WakeUpAmerica]\n",
       "26                                                     []\n",
       "27                                                     []\n",
       "28                                                     []\n",
       "29       [OscarsSoWhite, oscarssoboring, OscarHasNoColor]\n",
       "                               ...                       \n",
       "89970                                       [local, news]\n",
       "89971                                                  []\n",
       "89972                                   [HateToEatAndRun]\n",
       "89973                                                  []\n",
       "89974                                                  []\n",
       "89975                              [ThingsIWontTellMyDad]\n",
       "89976                                                  []\n",
       "89977                                                  []\n",
       "89978                                                  []\n",
       "89979                                [ChristmasAftermath]\n",
       "89980                                                  []\n",
       "89981                              [RejectedDebateTopics]\n",
       "89982                                                  []\n",
       "89983                                            [health]\n",
       "89984                           [MyBestFriendIsntAllowed]\n",
       "89985                                                  []\n",
       "89986                                   [refugeeswelcome]\n",
       "89987                                            [events]\n",
       "89988                                                  []\n",
       "89989             [Ohio, ImpeachObama, NCpol, PA, NV, NH]\n",
       "89990                       [BlackLivesMatter, Christmas]\n",
       "89991                                  [IHate____Because]\n",
       "89992                                [MakeMusicReligious]\n",
       "89993                                      [2016In4Words]\n",
       "89994                                                  []\n",
       "89995                                          [politics]\n",
       "89996                               [ThingsYouCantIgnore]\n",
       "89997                                                  []\n",
       "89998                                      [rantfortoday]\n",
       "89999                                              [news]\n",
       "Name: text, Length: 90000, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_text = ira['text']\n",
    "hashtag_pat = '#(\\w+)'\n",
    "prog = re.compile(hashtag_pat)\n",
    "tweet_text.apply(lambda x: prog.findall(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_list(tweet_text):\n",
    "    \"\"\"\n",
    "    :Example:\n",
    "    >>> testdata = [['RT @DSC80: Text-cleaning is cool! #NLP https://t.co/xsfdw88d #NLP1 #NLP1']]\n",
    "    >>> test = pd.DataFrame(testdata, columns=['text'])\n",
    "    >>> out = hashtag_list(test['text'])\n",
    "    >>> (out.iloc[0] == ['NLP', 'NLP1', 'NLP1'])\n",
    "    True\n",
    "    \"\"\"\n",
    "    hashtag_pat = '#(\\w+)' # Hash Tag pattern\n",
    "    prog = re.compile(hashtag_pat) # Compile\n",
    "    tags = tweet_text.apply(lambda x: prog.findall(x)) # Find all\n",
    "    return tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata = [['RT @DSC80: Text-cleaning is cool! #NLP https://t.co/xsfdw88d #NLP1 #NLP1']]\n",
    "test = pd.DataFrame(testdata, columns=['text'])\n",
    "out = hashtag_list(test['text'])\n",
    "(out.iloc[0] == ['NLP', 'NLP1', 'NLP1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_lists = hashtag_list(ira['text'])\n",
    "freq = pd.Series(tweet_lists.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = freq.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(tweet_list):\n",
    "    \"\"\"Helper function to compute one single list\"\"\"\n",
    "    if len(tweet_list) == 0: # Empty list\n",
    "        return np.nan\n",
    "    \n",
    "    if len(tweet_list) == 1: # One elem\n",
    "        return tweet_list[0]\n",
    "    \n",
    "    for com in counts: # Highest to lowest freq\n",
    "        if com in tweet_list: # Check if in tweet\n",
    "            return com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_hashtag(tweet_lists):\n",
    "    \"\"\"\n",
    "    :Example:\n",
    "    >>> testdata = [['RT @DSC80: Text-cleaning is cool! #NLP https://t.co/xsfdw88d #NLP1 #NLP1']]\n",
    "    >>> test = hashtag_list(pd.DataFrame(testdata, columns=['text'])['text'])\n",
    "    >>> most_common_hashtag(test).iloc[0]\n",
    "    'NLP1'\n",
    "    \"\"\"\n",
    "    freq = pd.Series(tweet_lists.sum()) # Total Frequency Series\n",
    "    counts = freq.value_counts() # Count occurrences\n",
    "\n",
    "    def most_common(tweet_list):\n",
    "        \"\"\"Helper function to compute one single list\"\"\"\n",
    "        if len(tweet_list) == 0: # Empty list\n",
    "            return np.nan\n",
    "\n",
    "        if len(tweet_list) == 1: # One elem\n",
    "            return tweet_list[0]\n",
    "\n",
    "        for com in counts.index: # Highest to lowest freq\n",
    "            if com in tweet_list: # Check if in tweet\n",
    "                return com\n",
    "    \n",
    "    return tweet_lists.apply(most_common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NLP1'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata = [['RT @DSC80: Text-cleaning is cool! #NLP https://t.co/xsfdw88d #NLP1 #NLP1']]\n",
    "test = hashtag_list(pd.DataFrame(testdata, columns=['text'])['text'])\n",
    "most_common_hashtag(test).iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5 (Features)**\n",
    "\n",
    "Now create a dataframe of features from the `ira` data.  That is create a function `create_features` that takes in the `ira` data and returns a dataframe with the same index as `ira` (i.e. the rows correspond to the same tweets) and the following columns:\n",
    "* `num_hashtags` gives the number of hashtags present in a tweet,\n",
    "* `mc_hashtags` gives the most common hashtag associated to a tweet (as given by the problem above),\n",
    "* `num_tags` gives the number of tags a given tweet has (look for the presence of `@`),\n",
    "* `num_links` gives the number of hyper-links present in a given tweet \n",
    "    - (a hyper-link is a string starting with `http(s)://` not followed by whitespaces),\n",
    "* A boolean column `is_retweet` that describes if the given tweet is a retweet (i.e. `RT`),\n",
    "* A 'clean' text field `text` that contains the tweet text with:\n",
    "    - The non-alphanumeric characters removed (except spaces),\n",
    "    - All words should be separated by exactly one space,\n",
    "    - The characters all lowercase,\n",
    "    - All the meta-information above (Retweet info, tags, hyperlinks, hashtags) removed.\n",
    "\n",
    "*Note:* You should make a helper function for each column.\n",
    "\n",
    "*Note:* This will take a while to run on the entire dataset -- test it on a small sample first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = os.path.join('data', 'ira.csv')\n",
    "ira = pd.read_csv(fp, names=['id', 'name', 'date', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_list2(tweet_text):# Not really needed??????????\n",
    "    \"\"\"Helper function to get the hashtag list\"\"\"\n",
    "    hashtag_pat = '(#(\\w+))' # Hash Tag pattern\n",
    "    prog = re.compile(hashtag_pat) # Compile\n",
    "    tup = tweet_text.apply(lambda x: prog.findall(x)) # Find all\n",
    "    tags = tup.apply(lambda x: [group[0] for group in x]) # Get full hashtags\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hashtag_list2(tweet_text) # Done "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_list(tweet_text): # As long as there is a @, it would be a tag right???????????? ASK!!!!!!!!!!!!!!!!!!\n",
    "    \"\"\"Helper function to get the tag list\"\"\"\n",
    "    tag_pat = '@\\w+' # Hash Tag pattern\n",
    "    prog = re.compile(tag_pat) # Compile\n",
    "    tags = tweet_text.apply(lambda x: prog.findall(x)) # Find all\n",
    "    # tags = tup.apply(lambda x: [group[0] for group in x]) # Get full tags\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                       []\n",
       "1                                          [@Philanthropy]\n",
       "2                                                       []\n",
       "3                                                       []\n",
       "4                                         [@dirtroaddiva1]\n",
       "5                                                       []\n",
       "6                                            [@SenSanders]\n",
       "7                                        [@MatthewGellert]\n",
       "8                                       [@rapstationradio]\n",
       "9                                          [@susanslusser]\n",
       "10       [@c982f7295cf57508a8d39bae6310c9546492d4105cac...\n",
       "11                                                      []\n",
       "12                                       [@shannoncoulter]\n",
       "13                                                      []\n",
       "14                                                      []\n",
       "15                                            [@WarfareWW]\n",
       "16                          [@NiqueTatted_721, @2ficmusic]\n",
       "17                              [@Trentsickle, @RobbyMook]\n",
       "18                                                      []\n",
       "19                                                      []\n",
       "20                                          [@srslyjaidyn]\n",
       "21                                                      []\n",
       "22                                                      []\n",
       "23       [@GeffGefferson1, @BRIANMCKNIGHT1, @SotallyTob...\n",
       "24                                       [@BleepThePolice]\n",
       "25                         [@DanScavino, @realDonaldTrump]\n",
       "26                                                      []\n",
       "27                                       [@Conservatexian]\n",
       "28                                                      []\n",
       "29                                                      []\n",
       "                               ...                        \n",
       "89970                                                   []\n",
       "89971    [@BlackElleWoods, @dougmillsnyt, @HillaryClinton]\n",
       "89972                                      [@yellowgerbil]\n",
       "89973                                                   []\n",
       "89974                                   [@uraniisitnikov2]\n",
       "89975                                    [@hippycinephile]\n",
       "89976                          [@blicqer, @YourBlackWorld]\n",
       "89977                                       [@CicelyRenee]\n",
       "89978                                    [@MichaelSkolnik]\n",
       "89979                                       [@Solely_Toya]\n",
       "89980                                                   []\n",
       "89981                                       [@ChrixMorgan]\n",
       "89982                                      [@JBBinLaden12]\n",
       "89983                                                   []\n",
       "89984                                      [@therichards5]\n",
       "89985                                                   []\n",
       "89986                                                   []\n",
       "89987                                                   []\n",
       "89988                                           [@QuikHit]\n",
       "89989                                     [@TheWelshTwitt]\n",
       "89990                                                   []\n",
       "89991                                    [@SideOfHashTags]\n",
       "89992                                                   []\n",
       "89993                                        [@felicia014]\n",
       "89994                                                   []\n",
       "89995                                                   []\n",
       "89996                                         [@JefLeeson]\n",
       "89997                             [@nealcarter, @thetrudz]\n",
       "89998                                      [@indigenous01]\n",
       "89999                                                   []\n",
       "Name: text, Length: 90000, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_list(tweet_text) # Done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does 'https://t.…\"' also count as hyperlink??????????????????????????????????????\n",
    "def link_list(tweet_text): # My search turns out that there is no link followed by white spaces???????????? ASK!!!!!!!!!!!!\n",
    "    \"\"\"Helper function to get the hyperlink list\"\"\"\n",
    "    link_pat = 'https?:\\/\\/(?! )*' # Hash Tag pattern\n",
    "    prog = re.compile(link_pat) # Compile\n",
    "    links = tweet_text.apply(lambda x: prog.findall(x)) # Find all\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_retweet(tweet_text):\n",
    "    \"\"\"Helper function to check if retweet\"\"\"\n",
    "    rt_pat = '^RT' # Hash Tag pattern\n",
    "    prog = re.compile(rt_pat) # Compile\n",
    "    rt = tweet_text.apply(lambda x: prog.findall(x)) # Find all\n",
    "    return rt.apply(lambda x: True if len(x) != 0 else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_text = ira['text']\n",
    "tweet_lists = hashtag_list(tweet_text) # List of hashtags\n",
    "num_hashtags = tweet_lists.apply(len) # Number of hashtags\n",
    "mc_hashtags = most_common_hashtag(tweet_lists) # Most common hashtags\n",
    "num_tags = tag_list(tweet_text).apply(len) # Number of tags\n",
    "num_links = link_list(tweet_text).apply(len) # Number of links\n",
    "is_retweet = is_retweet(tweet_text) # If tweet is retweeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(tweet_text):\n",
    "    \"\"\"Helper function to clean single text string\"\"\"\n",
    "    remove_rt = re.sub(r'^RT', '', tweet_text)\n",
    "    remove_hash = re.sub(r'#\\w+', '', remove_rt) # Remove hashtags\n",
    "    remove_tags = re.sub(r'@\\w+', '', remove_hash) # Remove tags\n",
    "    # remove_link = \n",
    "    substitute = re.sub(r'[^A-Za-z0-9 ]', ' ', tweet_text) # Remove non-alphanumeric\n",
    "    space = re.sub(r' +', ' ', substitute) # Fix space\n",
    "    lower = space.lower() # Lowercase\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://t.co/JESLKxfiu1 #AfricanArchitecture ']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Help!!!!!!!!!!!!! ASK!!!!!!!!!!!!!!!!!!! Trying to get the link, but not successful here\n",
    "string = 'http://t.co/JESLKxfiu1 #AfricanArchitecture #Pyramids'\n",
    "link_pat = 'https?:\\/\\/(?! )*.+ ' # Hash Tag pattern\n",
    "prog = re.compile(link_pat) # Compile\n",
    "links = prog.findall(string) # Find all\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(ira):\n",
    "    \"\"\"\n",
    "    :Example:\n",
    "    >>> testdata = [['RT @DSC80: Text-cleaning is cool! #NLP https://t.co/xsfdw88d #NLP1 #NLP1']]\n",
    "    >>> test = pd.DataFrame(testdata, columns=['text'])\n",
    "    >>> out = create_features(test)\n",
    "    >>> anscols = ['text', 'num_hashtags', 'mc_hashtags', 'num_tags', 'num_links', 'is_retweet']\n",
    "    >>> ansdata = [['text cleaning is cool', 3, 'NLP1', 1, 1, True]]\n",
    "    >>> ans = pd.DataFrame(ansdata, columns=anscols)\n",
    "    >>> (out == ans).all().all()\n",
    "    True\n",
    "    \"\"\"\n",
    "\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations! You're done!\n",
    "\n",
    "* Submit the lab on Gradescope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
